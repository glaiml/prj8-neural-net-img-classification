{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glaiml/prj8-neural-net-img-classification/blob/master/vgk-neuner-img-classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMhsX4iHBJvt",
        "colab_type": "text"
      },
      "source": [
        "Vinayak G Kudva Project submission for GLAIML \n",
        "https://github.com/glaiml/prj8-neural-net-img-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiQaKY_-gmj",
        "colab_type": "text"
      },
      "source": [
        "# The Real Problem\n",
        "\n",
        "Recognizing multi-digit numbers in photographs captured at street level is an important\n",
        "component of modern-day map making. A classic example of a corpus of such street\n",
        "level photographs is Google’s Street View imagery comprised of hundreds of millions of\n",
        "geo-located 360 degree panoramic images. The ability to automatically transcribe an\n",
        "address number from a geo-located patch of pixels and associate the transcribed\n",
        "number with a known street address helps pinpoint, with a high degree of accuracy, the\n",
        "location of the building it represents.\n",
        "More broadly, recognizing numbers in photographs is a problem of interest to the optical\n",
        "character recognition community. While OCR on constrained domains like document\n",
        "processing is well studied, arbitrary multi-character text recognition in photographs is\n",
        "still highly challenging. This difficulty arises due to the wide variability in the visual\n",
        "appearance of text in the wild on account of a large range of fonts, colors, styles,\n",
        "orientations, and character arrangements. The recognition problem is further\n",
        "complicated by environmental factors such as lighting, shadows, specularities, and\n",
        "occlusions as well as by image acquisition factors such as resolution, motion, and focus\n",
        "blurs.\n",
        "In this project we will use dataset with images centred around a single digit (many of the\n",
        "images do contain some distractors at the sides). Although we are taking a sample of\n",
        "the data which is simpler, it is more complex than MNIST because of the distractors.\n",
        "\n",
        "# Project Description\n",
        "In this hands-on project the goal is to build a python code for image classification from\n",
        "scratch to understand the nitty gritties of building and training a model and further to\n",
        "understand the advantages of neural networks. First we will implement a simple KNN\n",
        "classifier and later implement a Neural Network to classify the images in the SVHN\n",
        "dataset. We will compare the computational efficiency and accuracy between the\n",
        "traditional methods and neural networks.\n",
        "\n",
        "# The Street View House Numbers (SVHN) Dataset\n",
        "SVHN is a real-world image dataset for developing machine learning and object\n",
        "recognition algorithms with minimal requirement on data formatting but comes from a\n",
        "significantly harder, unsolved, real world problem (recognizing digits and numbers in\n",
        "natural scene images). SVHN is obtained from house numbers in Google Street View\n",
        "images.\n",
        "\n",
        "# Overview\n",
        "The images come in two formats as shown below.\n",
        "Format 1 : Original images with character level bounding boxes.\n",
        "Format 2 : MNIST-like 32-by-32 images centered around a single character (many\n",
        "of the images do contain some distractors at the sides).\n",
        "\n",
        "The goal of this project is to take an image from the SVHN dataset and determine what that digit is.\n",
        "This is a multi-class classification problem with 10 classes, one for each digit 0-9. Digit '1' has label 1,\n",
        "'9' has label 9 and '0' has label 10.\n",
        "Although, there are close to 6,00,000 images in this dataset, we have extracted 60,000 images\n",
        "(42000 training and 18000 test images) to do this project. The data comes in a MNIST-like format of\n",
        "32-by-32 RGB images centred around a single digit (many of the images do contain some distractors\n",
        "at the sides).\n",
        "\n",
        "# Reference\n",
        "Acknowledgement for the datasets.\n",
        "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng\n",
        "Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop\n",
        "on Deep Learning and Unsupervised Feature Learning 2011. (https://research.google/pubs/pub37648/)\n",
        "\n",
        "http://ufldl.stanford.edu/housenumbers as the URL for this site when necessary\n",
        "\n",
        "# Downloads\n",
        "\n",
        "Refer to Olympus for project related files and instructions.\n",
        "Data Set:\n",
        "● The name of the dataset is SVHN_single_grey1.h5\n",
        "● The data is a subset of the original dataset. Use this subset only for the\n",
        "project.\n",
        "● Keep a copy of your dataset in your own google drive.\n",
        "\n",
        "# Project Objectives\n",
        "The objective of the project is to learn how to implement a simple image classification\n",
        "pipeline based on the k-Nearest Neighbour and a deep neural network.\n",
        "\n",
        "Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjKg-g6_z9nS",
        "colab_type": "code",
        "outputId": "1fc4b735-c4c9-4e65-fa75-076fb373a668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcGnIhj9AAX2",
        "colab_type": "text"
      },
      "source": [
        "# Data fetching and understand the train/val/test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIPmFcLt65co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Open the file as readonly\n",
        "h5f = h5py.File('/content/drive/My Drive/Colab Notebooks/NuNetProject/SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training, test and validation set\n",
        "X_train = h5f['X_train'][:]\n",
        "y_train = h5f['y_train'][:]\n",
        "X_test = h5f['X_test'][:]\n",
        "y_test = h5f['y_test'][:]\n",
        "\n",
        "\n",
        "# Close this file\n",
        "h5f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1vwYC23Au-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9ca44726-fc21-4454-b9d5-1db3c5862ff1"
      },
      "source": [
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "print (X_test.shape)\n",
        "print (y_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 32, 32)\n",
            "(42000,)\n",
            "(18000, 32, 32)\n",
            "(18000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DJLL0YAE91C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "00560302-c07a-4461-cc81-2ac7580a8e89"
      },
      "source": [
        "# Flatten X Train and X Test\n",
        "X_train_flat = X_train.reshape(42000, 1024)\n",
        "print (X_train_flat.shape)\n",
        "\n",
        "X_test_flat = X_test.reshape(18000, 1024)\n",
        "print (X_test_flat.shape)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 1024)\n",
            "(18000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSQ2SI7ADvM",
        "colab_type": "text"
      },
      "source": [
        "# Implement and apply an optimal k-Nearest Neighbor (kNN) classifier (7.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-yLXPt87ICY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "335c914f-d29c-4c5b-b9e8-ad69fc582dae"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as pl\n",
        "Nc = range(20, 40)\n",
        "knnElbow = [KNeighborsClassifier(n_neighbors=i, weights='distance', metric = 'cosine', n_jobs = 50) for i in Nc]\n",
        "knnElbow\n",
        "score = [knnElbow[j].fit(X_train_flat, y_train).score(X_test_flat, y_test) for j in range(len(knnElbow))]\n",
        "print(score)\n",
        "pl.plot(Nc,score)\n",
        "pl.xlabel('Number of Neighbors')\n",
        "pl.ylabel('Score')\n",
        "pl.title('Elbow Curve')\n",
        "pl.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6438333333333334, 0.6447222222222222, 0.6450555555555556, 0.6444444444444445, 0.6449444444444444, 0.6467777777777778, 0.6475555555555556, 0.6478888888888888, 0.6468888888888888, 0.6478333333333334, 0.6474444444444445, 0.6479444444444444, 0.6471111111111111, 0.647, 0.6461666666666667, 0.6465, 0.6461111111111111, 0.6450555555555556, 0.6441111111111111, 0.6438888888888888]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcaYiPx/x1XzxIFK8JQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3juKxraDxp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KNC = KNeighborsClassifier(n_neighbors= 31, weights='distance', metric = 'cosine', n_jobs = 50)\n",
        "# Call Nearest Neighbour algorithm\n",
        "\n",
        "KNC.fit(X_train_flat, y_train)\n",
        "pred_train = KNC.predict(X_train_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SejeHwfibmY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef5f4d2d-950e-4208-f351-8f1fddd18813"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "mat_train = confusion_matrix(y_train,pred_train)\n",
        "print (\"Training Accuracy ::\")\n",
        "KNC.score(X_train_flat, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy ::\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01N4Yeu9CSRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_y = KNC.predict(X_test_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Al0AdTwCTPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "10f5aa5f-b959-4017-abc5-7be907ae2b94"
      },
      "source": [
        "print (\"Test Accuracy ::\")\n",
        "KNC.score(X_test_flat, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy ::\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6479444444444444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8BTKfTUAJaA",
        "colab_type": "text"
      },
      "source": [
        "# Print the classification metric report (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDd6MEoFCekO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a86a25b5-507b-49a5-cbcb-8e223b87f277"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"KNN Metrics = \\n\", metrics.classification_report(y_test, pred_y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN Metrics = \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.76      0.65      1814\n",
            "           1       0.56      0.79      0.66      1828\n",
            "           2       0.78      0.62      0.70      1803\n",
            "           3       0.60      0.55      0.57      1719\n",
            "           4       0.73      0.77      0.75      1812\n",
            "           5       0.71      0.51      0.59      1768\n",
            "           6       0.64      0.56      0.60      1832\n",
            "           7       0.73      0.74      0.74      1808\n",
            "           8       0.59      0.55      0.57      1812\n",
            "           9       0.66      0.61      0.63      1804\n",
            "\n",
            "    accuracy                           0.65     18000\n",
            "   macro avg       0.66      0.65      0.65     18000\n",
            "weighted avg       0.66      0.65      0.65     18000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFKvuLHuAKjR",
        "colab_type": "text"
      },
      "source": [
        "# Implement and apply a deep neural network classifier including (feedforward neural network, RELU activations) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7_MslqeAJx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "292867f1-e049-448a-dee5-d5329316d750"
      },
      "source": [
        "# visualizing the first 10 images in the dataset and their labels\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 1))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(X_train[i], cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "print('label for each of the above image: %s' % (y_train[0:10]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAA9CAYAAACpzLMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO19WY9c13Xuqnmeiz2ym02ySYoiRVOU\nRFmKHMpxpMiCE8d2DARQXhIDmZCXPOYtec0/MBAEyYuTOEjiAE4kJZBki5pMmoMoUZQ4dJPsbpLd\n1V1d8zzch8L6+O2j011Vuffi4hJnvahUrD5nj2uv/X1rcPX7fXHEEUccccQRRxx5lMX9/7oBjjji\niCOOOOKII/+3xTF4HHHEEUccccSRR14cg8cRRxxxxBFHHHnkxTF4HHHEEUccccSRR14cg8cRRxxx\nxBFHHHnkxTF4HHHEEUccccSRR168u/3j1NRU3+VyiYhIOByWqakpERGZm5uTeDwuIiKtVks2NjZE\nRGR5eVm2trbw95lMRkREpqen5dChQyIicuzYMYnFYiIiUiwW5e7duyIicu3aNfn8889FRKRQKIjb\nPbDFfD6feDwevPerX/2qiIg899xz8thjj6FtGl6/sbEhn332mYiI/OEf/qFr2AC8/fbb6KPb7cZ7\nPR6P9Ho9fFZpNpuiv3e5XBiTAwcOiNc7GM5er4f28O+73S6e2e/3jefqewOBAD53Oh1pt9v42y++\n+EJERCqVCt71wgsvDO3jD3/4w34wGBQRkXw+LysrKyIyGGd9vtfrxTNbrZa0Wi20X/vicrnE5/Ph\nudqXer0utVoN7ex2u19qQ7fblU6ng76GQiEREUkkElgn2WxWstmsiAzmVH/zgx/8YNc+/t3f/V3/\nwYMHIiISDAYlHA6LiEgymcTnYDBotJ3nR9fsxsaGbG9vi4hIu92WaDSKdmlbXC6X6FhGIhHx+/14\npo5lq9XC83ksXC4X/r/T6WD+v//97w+dwx//+Md9fWav18N493o9CQQCIiLi9/uNNdhsNtEunatO\np2Osa1772v5qtSr1eh190edsbm7KjRs3MFb6m06ng3GYnJyU+fl5ERHZs2cPxvCv//qvh/bx7t27\n/Xv37omIyD/8wz/IW2+9JSIi29vbUi6XRUTk2WeflT/6oz8SEZFTp05hPEOhkCSTSTxL+9jr9dDH\nTqeDeWm1Wvg+FAoJj60+s9/vy61bt0RE5I033pCPPvpIRES2trYkn8+LyGDt6zgUi8WhffyLv/gL\nzGOn05FGoyEigz1dqVRERKRcLuNzrVbDb+r1ujGn+hy/34917vP50K9Go4F10ul0oFf498FgEHNn\n3bu6Pj0eD/72/Pnzu/ZxY2Ojr/us3+9jz+t7RQZjrM9uNpvQNcFgEO/pdru2n6vVquzZswfPWVtb\nw7t4T6hOiUajUq1WMWb6fSgUkkKhICKDta9zuLCwMHQOz5w509e1lslkoFdYR9+7d082NzfxN3Nz\ncyIyOP8ef/xxERGZmZmBvnG73di7vC/7/b5xZug+2NjYkDt37oiIyPr6OtZIOByGDs1kMoYe0v7+\n9Kc/HdrH3/u93+uvrq5i3HS9NxoNzAWfbYlEAu3nOa3Vamh/KBTCWHk8HuF1otJut/G3vH4CgQCe\nMzs7K9/97ndFROTll1+GLcJtW1xctO2jg/A44ogjjjjiiCOPvOyK8LjdbliOCwsLcubMGRERefLJ\nJ2Fl1+t1OXfunIgMEBtFe7LZrDz11FMiIvL888/Lk08+KSKDW59a+m63Gxbr2bNn5e233xYRkcuX\nL+M5nU4HN7eTJ0/Kb/7mb4qIyJEjR9DOQqGAW+7Ro0dldnZ25AFgNIYRDJfLZViejOroLajZbGJ8\nqtUqbv6BQMB4plqsbNXq/4sMbs6KdHU6HeOmp7eE+/fvG9+rtf7CCy8M7WMoFMINJpfLid6iV1dX\ncWNkYcu62WwaKJNa0PpfEfOWG4/HDdSDEQd9jsvlwvf1eh1j5fP5gKSEw2FjrHYTRs74Rs9z63a7\n8dnj8RjvVMSRkYput4u2pFIpPLNSqWBsqtUqPnNb/X4/nl+v1zHPvV7PQBAZ4RsmbrfbQGl0LPm2\nHAgEsA9cLpdxS9R2ulwuPIfH2+PxYA7r9bqBROnfcvv1Gfq32n+3243f841rFOl2u2gzz1E4HEZ7\nGHnltgWDQXz2er3GftU9WqvV8Jx6vY41WKvVMO9+vx9j2Gq1JJfLiYjI559/DgS63++jX+1223YP\n7SRerxdt4Dni+W2328b4282Lx+NBOyORiEQiERExb8KNRsPoo+qher2Oz4lEAn3Xtmgf7do5imh7\n/X6/oS+0LY1Gw0CP+DOjHPp9p9PB+8PhMPRgPp+X+/fv4/mMtOj3PDa61vU3+v+8FkaRWCyGM2Zm\nZgbPabVa2JciYqyLVColIiITExOyd+9eERHZv38/0NB+v28gkbpm2+22oX8V0fL5fDgD8vk8ft/p\ndLAukskk0J5ms4kxGUVeeeUVoIArKytAN2/evInx57OwVCqhncFgEKhLOp2Gfg0EAvh9uVwGulUo\nFHB2BoNBzBezAoz8VKtVo78qvGZ3kl1nOR6PY8FOT0+DQtq/f78kEgkRGRyg+tJWq4WFs3//fhzG\nzz77LBbFrVu3AD0eOHBAJicnRUTkm9/8Jga40WhgMkVEvvKVr4iIyEsvvQTDqdfryYcffigiIu+8\n8w4m9uWXX5aDBw/u2mkWt9uNjcWQLivxVqsFw+PGjRuALbe2trC4Dhw4gLY988wzsn//fhEZbDId\nQz54+v2+lEolERH5+7//e/nXf/1XtIkPDJ3YRqNhGFHjKKBkMonF0uv18N5isWhA3mwc8AGuf8uL\ny+v1GoeZ/j4YDGINWGky/Xs+qLxeLzYKK/p+v28YnLsJ/x0rHTZ+RB4amDx+bJz0+33Mp/ZFZACL\nqxJ3uVwGNccKTsfA6/UaY6lipTGHbU6r2MHc3W4X7/J6vWiz2+3G8xuNBj5ze/1+PygnnqtarWaM\nlfbB5XLhezYg3W43Dk0+OJgeGkV8Ph90QDqdxjMrlQr0zfz8PH7TarWgTF0uF+YoFAoZh7uu9wcP\nHsj6+rqIDC4Z2rZoNAo9tLi4KAsLCxgrhfJLpRIOsGazibb1+33jIB0m1vHcSbRtvV7PoJf1vaFQ\nCGMSj8fxORQK4bmtVgt9L5VKuFxWKhU8kw8VviD8T4XXpogYFycdJ6/XaxhZvF+ZvmF9wS4OemFb\nW1vD30YiEUmn0/i90tTFYhGHbyKRwDNrtRrW6p49e3CIjyLJZBKGysGDB7Hn2CCt1Wo4M9jw570S\niUSM/cqi+5Lng/dToVCAYeDz+aB7EokEjLEDBw7IgQMH8HvWbaOIghrpdBpryuv1yrVr10RkQKXx\nmaC6IZvN4sw+ePAg9pbIQ/2zsrIiFy9eFBGRq1evGsaSzku32zX0ie6/Xq9nXGJ0HLxe79BLskNp\nOeKII4444ogjj7zsivD4fD6Znp4WkYG1qBZ0OBwGMvD555/LhQsXRETkzp07sLaOHj0qx48fF5HB\nrePTTz8VEZEf/ehHsL6/+93vyre+9S0RGTginTp1SkREvvjiC8DHLpdLnnjiCRERefzxx2HxbW5u\nAml5//33ZWZmRkQGFqWiK6OI9YbPULXC2RcvXpSzZ8+KyAChYuhZ5fXXX8ct6/Tp0/IHf/AHIiLy\n4osvGjdS67tFBrcv7Qt/3+12DfqPIVK+RQ0Tn89n0EyMIKlDX6fTgXWs7dXfsHXPN3x+pn4fDAYN\nWoIRDUa3GMlhhJCRCOt47STFYhFrKhwOAz4Oh8PoC98K2u025jkcDgPq3dzcBFIYCoVwIy6Xy5jr\nZrOJm1IsFsPtUeQhlN9oNGxpAo/Hg/6Ni+7wfDPK1G630Uev12tQWjr2/X4fa4cRiWg0ivazw3Ol\nUsG6YLqQHbbZOZad/fUd+t9xkMhoNIo5z2azuOXmcjnQ2nNzc0B1RB7SFN1uF+Pf7XaBbORyOaA6\n169fl+vXr4vIgM7VOY1Go7gVf+1rXzP6qGOSzWahYwqFgjG2vAaGCVOK1n3At3l2NuXf6Pyys386\nncb4RKNR/KbdbmMet7e3oc82NzelWCziXUy5cztVeN8Pk9XVVcORXPdQtVrFOM3OzhrIhp1zMq93\nXoMiAoQnl8shaCSZTAKR6Ha76He5XMZeYV2v/y5iUsSjSCwWw1k4OzuLM69arYoGT4TDYcMxW9dj\nuVyGrtra2oLu8Xq9WMuBQABrn53KWef6fD4j0EXHJ5lMYkwWFxfBymxsbBhozDD5p3/6J1lcXBSR\nwZ549tlnRWSwD1SXbGxsGOtE9e7Jkyfl6aefxvjovNfrdcM+UGovHo8DNapWq8ba5L7vhC4zDTqM\nQt/V4AkEAvAoP3XqFBoYDAaxSQqFArzFK5UKorGOHz8OPxte1OVyGcbP4cOHQXvt3bvX8GT/xS9+\nISKDTasQ8/T0tMEDK9x89+5dTHg+nx+LU6/Vagbfrwttc3NT/vu//1tERD788EMszGq1avD97Bei\nm/u//uu/ZGlpSUREXnvtNfn+978vIoMIFqYf9L2ZTAYKq9VqYeEzPBwIBLCQU6nUWAaP1ceFIWym\nqxg2VqXpdrtxoMbjcXzPSpDpEPa94PeyguHDvtvtwhBhOop5+2FSKBSMcWLFwcJjr7/nzcOUE9N6\nVn8GHTM2yKy/1/HgQ/9/hy7g6CorpaXi8/mgULh9HOnocrkwn6FQCMaD3+/Hs/igZ5iYKadAIGAc\ngnZ+Nf8TikTftXfvXly2VlZWsD8WFhZw2LAhzxFJd+/ehWFz48YNGDx37twRjTxhoyUQCOB71gFH\njhyBkfPkk0/ikC6VStgriURirL24k/BY8WemnDiyLxwOY0zS6bRxQOrcMa0ZCoWwNhqNhkEp8yFq\nZ8Syrhoma2trhk5UqogNHj64o9GooX/5sqTicrkwt4VCAQZPu93GXrf6cPE5ob4ik5OT+N7n86Gd\npVJpLB+eeDyO8U4mk0Z0oPaR6apIJII1cu3aNfinXr16FfuvXC6jPZFIBLr+9OnTcJUIBoOG/xJT\ne2rsHT58GOf07Ows3js9PW24iQyTDz74QG7evCkiA93w27/92yIyAD7UEPriiy9g4Ik8pMCOHj2K\nyz/vv3q9jradPHkStJff70c7L168iPObI5at/q+6rpnGt9KpduJQWo444ogjjjjiyCMvQxEeRWwO\nHToEq42difx+vxEpoRbc9PS0EfE0MTEhIoObkjpzVSoVwKz1eh2W8v79++F4XKvVYDkGAgGDXmFK\nhSMDxoEnG40G+uXz+WBBnzt3Tt555x0RGThnKVowOzsL6C4Wi+HGWyqVkN+mWq3K7du3RUTkhz/8\nIW4Yf/qnfwrKhB30zpw5g74HAgED7tU+BoNB3GYikQi89UcRa2SL3e2x3W4bUL7OVzqdBnQ+MzOD\ndjJd1e/3DSdOtdCr1SpQrwcPHhiRS4rqiIjhOK3tHMUBTWVzcxNtD4VCuH0xNdfv9w3HaoZ3+Z1M\nzekzu92uccPk52hfQ6GQgaLwLdvu1sH01ihizeekn9lhkdEtr9cLhCcUChmop10kjBUO1vHhNjJ8\nzDlTmH7kz+NSBdzHqakpODsyEjU5OWnr0Nlut7F3b9y4AQr68uXLQIIZze31euhLp9PBfrp48aKB\nECq6/MILL+BzvV43UCy9CY8iVvSGUR1G7RjNY2dp/X0gEABiE4lEDDSTEVOmcdlRX8fQmpNHx4QR\nnXGQusnJSUMXKJWztbWFz+vr65jbZDKJd1qDFhi1VXTi1q1b0PXZbBa6OxKJGI6vOjZutxvocqPR\nAALDUbhWtHKYNJtNtIHbrM/S99pFXVlpemUOisUi9E06nUb7mZZkp2WOmLRSjuwiwNGT40RMKooq\nMkBMFaVZWFiAw3YymcTZ1u12cTZwlPTHH38MuqpWq+E3/X5fvv71r4uIyBNPPAG0anl5GeudaXa/\n34/vOdcaR/+OIrsaPJFIBA/m8EgO3eSDKRKJYAGm02mDBtBDc25uDputXC4bC0cXYDqdhmFQKBSM\nCBD9fa/Xw3N4wXKo3CgSDAaxEYPBICb23Llz4Fp9Ph/e9fLLL4OGm56exmLM5XLy3nvvichgklUJ\nVqtVPMcaeqp9YX8nkYfGm3WBah9rtZr87Gc/G7mP0WjUOAgZNrbziA+Hw6AXT58+DeNzz549tv4c\n3W7XoD10wxUKBVB7zWYTBiH7MXAoOPfZmrRsN9na2jLoVp2rWCwGQ6XVahnREbpJOPpG+y4yoCo4\nFF3XyL1796CMgsGgEd5uF+3Hxk+r1TKMhHGEE+VZQ9FZqWkfOXyXfdNYEbPBw2uNE0+ygcyHICdd\n5L70+30jqmicyBCmGcLhMIzuZDKJd/GFgKlDr9eLQ+Xq1atQsg8ePMAedblcRlSXrkE2DO7fvy8f\nf/yxiAwuXseOHRORgbGvl79erwcK7NNPP8WlbRSx+qOwkcNG1E70JUc96brlKLx+v4/54HQRjUbD\niNTU+Q6FQrjARSIR45Kh76rX6yOv18OHDxvuC6rHmTasVqugCvlg5bFhqrnZbOIC+fnnn2Nd7N+/\n3/DXY18kXSNMl/D4dbtdw0hQPaG6YzcpFAowosvlMvrA+8/j8RjzqfPg8XhgJLB+q1QqaHMsFjN8\nDDlamC8W7MOjf9vr9bDemXplH5hRhNdXsViEwRkMBmG09Ho9GCrpdBrPj8fjmPdjx45h3vP5POi8\n69ev48ybn5+H+0sikcBv2Jhhg5uTjLZaLfQ3HA4blL6dOJSWI4444ogjjjjyyMuuCE+n04EFd/Pm\nTUQ/zc7O4rbHiZHK5TIs5X6/D2vL4/HghuzxePAba84TfSYnu2s0Gkb8vVp6TIkwfN9sNkemQkTM\nW2u1WkV02O3bt42U/eql/mu/9muA7BhJymaz8tprr4nIAP5+/fXXMYY/+MEP8Bumb9hRjhONsaOW\nXTTOvXv34FD953/+50P7aBf9IWLeSDgCJBKJwEI/ePAgnOYSiQToyG63i/ZwdAX3ye/3Y/10u13D\nIVmFHQz535jeGia8FrRf/Hz9jj37OT26/j0n4otGo/h9sVgE5XHr1i2MTSwWw7xZEwNyoj8V/jwO\nvCwyuNXwLV5vUz6fz0h4yZQErylO36/7MhwO20Yk8ZwwwsN7lPcir1nef7VabSyEh+eRqRa+dTMi\nFwwGjVsr59tR1IXRR7/fj/nlm3C73TbWie5RTnCWTCaBJrTbbayHjY0N6IxRhKOxOp2OEbXHgQV2\n1AVH59XrdUSzMF2rzxL5MqWl48bRkIyGRKNRAz3j5G6j7kWPxwP0Y3NzE8i+3+8Hvb25uYn8MNPT\n08Ycci4r/X5rawu3/jt37oBSmZiYMPrN+YcYybFzSGbdN06AhLZfqah8Pm/kmVHhc5GpJW2TvpeT\nRPI+1mdGIhG0v1wuYwy5ZAOfE6VSCTo6l8sZASfjIDy8Bhk1Z73FOeY6nQ6Qq83NTZyRHMXGVGCx\nWLR1X+AcZto3q7A7Czs2W9tnJ7saPLlcDvCu1+s14EmGOPUllUoF4dXnz583EmOpAlpdXYVi4jok\n1gRD+pu1tTXQTO1220gypBuSqRnr5h8mvV4Pg7e1tYXoDs4cmUgkkDU6FAqBKuh0Okb4ri6u5557\nTp5//nn0RaNN2Hel3+/j99YIJvZ9UvH7/Ti03n///bF8eCqVilH3iJUp+4JwtkuOfuAoDqY4OXye\nwz118xWLRYyVtQ120Rj6LJVRlRD7n3CmX6v/CYe8cmI6/T37Qvh8PszV5uYmIhGXl5dxIMZiMVBp\nsVjMMNiZ0lLhLLvjCoeh8ryxcWI1HnmeeRzYAGAq0u457GvEdBg/n6PYrAfrOKGwfHGxpmTQdXT9\n+nUcBjMzM4aBrAqX63yxIT85OYn5qtVqyG6+tbVl0JF6YOTzeTwznU5DH6yvr4MyO3/+PHTGKGKN\nuuJUDWzwcOSg6jy/34/xLxaLGHP2/wiHw+ivx+MxMmmr8CWSPzPNwzToOH4SPP+FQgEUYiQSwdx+\n9tlncvr0afyeaSDWNTr2y8vL8sknn4jIIEro8OHDeJ9dZKT2S2Sgd/Si7nK5MLfZbNao2TSOwVMs\nFnGeVSqVHaOC7fYf68pGowEDptVqGRFe+pmz5LPhx3UQmZ4tl8voVyQSwdkjYm887CQcLTwzMwPD\nlVOZ9Ho9Y371TDp79izGJJfLoZ2xWMwAMljf6PhzzUP2PbX6V3KkqV3KlZ3EobQcccQRRxxxxJFH\nXnZFeLa2tnCzmpubM6AmtV4DgYARuaOVyv/zP/8Tv43H47ghv//++4A8Dx06ZNBhaslypdmlpSWk\noH7qqafg3JROp2G9cj0Tn883lkMoQ5uNRgMUDCMhqVTKSC7G1IhauOx97/f7DUc2rhyrbeN2clJB\nax4bfqdWbv7ggw+M5EzDhMfDmgxQ3+X3+43bFTsM6hrg2mccPcJJ7nK5HNrNVaWLxaKRhEwtd3Yq\n5DFvt9sjowPsYFyv13EDsaJTnFZenfA4BwfXfeHkZYVCAbfNra0ttDESiRjQM0eG8O34/0SUlrWu\nG9OefFOyK59h/Y1dPa/dnmlXCoGpFu6vtfzEOH3k57daLYxbs9nEOjp//jzGPJVK4abXbDYxp5zT\nhBFfpmd1LYo8pMJEzKrSrP96vYcV6m/cuIE8YVeuXBmrLAGLld6yi9gSeUj9BwIB6BJGTMvlMtYt\nI8eRSATzwrrHmj+JaTV2Vtf+sqvCMOl0Ohg/puE5/1e5XMbeYiSBUcZWq4Vgj+vXr2NsksmkUQ7F\nuia1Hzz/TN9xkAZX4h61fyIDfaD6t1qtGnmHtC9McVuDGBR9ajabWId+vx8o3cTEBBCVcDiMecvl\nckAlc7kcxpmfv729jfW+vr5uoHfjlnlR5/B9+/bh/CsUCmhDvV7H9+FwGPP1zjvvIECl3++D/nO5\nXPh9LBbDuuYcWhyByhQuCydmtNZcHJZPaagPDytuLubG0SlMA+igXrlyBUYOH/oPHjwwathwA3UT\nrK6uGtlvlSa7dOkSFkI0GgVUeebMGQzA0aNHh3pqs/DmyOfzWERMjUxOTuK9XFCQIx/K5TIMP2s0\nCytuLtzIPi0M3zMHzz4Zavh98sknhsIeJhxizX4bfr8fm4/pina7bVCKDDHqomYImakupjFyuRwS\nU3F2Vw6RZR7bCuWP6jcQDofxdxz9xona9PkiA+WvbanVauhfMpk0IuF0E3KxxXa7DWM8FAohmm12\ndtaIkNLPnOCON7P2cVSp1+uGgaFiNVTsns2RYmzA7BSqai1qqWL9vNNz7OqIjSKsbzqdDpQ1p69o\nt9tIhnr8+HHs+1qthj3BzxF5mK11YWEBBk+pVAIEf/36dWNfqrDhFA6H0Z67d+8aUZjj9JMLajK1\ny/421sKsvLbtkl6yD1K32zUS3qku4UgrTq4Yi8WMYo1qWFarVejCXC5n0O67CadwaLVaGDOrvwr7\nJmq7eGwajQbm/NKlSwa9wgkMeZ1bffl0LNn1gSNLOfpQdcYowmtzpxQR3BerywWPFUdmaeTa/Pw8\nPkciEcxDo9HAmuVi0pxpmQ3zWq1m0KfjAAELCwtw4zh8+DDW0Y0bN+CzViwW0R9ej7VaDZFZrVYL\n7XG5XEbNOh2r1dVVw5VEhddMu902Umiwa4sK02E7iUNpOeKII4444ogjj7zsivBYayexlWp3++bU\n5P1+H9YoQ4/dbhfOz1NTU3Bs9ng8sGTv3LkDhKfb7eI5b7/9Nqz706dPGxVZ1XqdmJgYy5JlK7JW\nq8GSbTQa6HssFsPN5+OPP5YbN26IyMAa1ZsBV99Np9PI33Hq1ClYynv27DGgf064ZQc3u91u3ABu\n376N3DvFYnGsW2UikcDzk8kkxpxzVzC0zWU7lpaWDKpAEZtut2s4Nusz9+/fj+fkcjmgIcVi0XAS\n5HXFibL45jSqLCwsGNC53vRDoRBoz0gkYiQP1H50Oh05evSoiAyoV85hocjitWvX8LdWyFTfdevW\nLazx+fl53L6s0RnsFDqOE2G9XjeQNkb+mLpiWoTHkBN4MSKrbbBGBtnRK5zenR2bOSLMmnhw3Jpv\nnKBN9xajDY1GA46bzWbTyEujc+TxeAwEUffQ7OwskgdyBBGjXjw+1tu7tuHevXvQT1xrbhQJBoNG\nbhGmzHjMVRjFsNISnItJEZt4PG7QDDom7Hjs9XqxXycmJoBMcw2yRqOBz9vb2yMjytboIU76x3mw\n7Kglpvi4LtXt27flq1/9qogMzgxGk+2QjXA4bMwJoxz6Xh7vZrM5MoKlwvmC2DGc97RdJCUzJawP\nkskkdNXExAQoSo/Hgz3B+X+KxaKBXNmtEUa7OUp2FPnGN76BM2x2dhZzce7cOeRE4vYXCgXMSywW\nM1B2XV+RSAS1vQ4dOoT2f/rppwgCyOfzhruMtpnRMGtdLdapw/biUErLGsYs8mVvb058pv/WbrcN\n7pmzYKqimZ+fh69LvV5HRNiVK1eweP1+P/wnVlZWwB8ePnwYg9dsNvGbRCIxFlXASejY8GB48saN\nG6CTlpaWDL8HFv3+/v37cvXqVREZ8JmvvPKKiIj8/u//PsL1mIpwu914L2eA5U188+ZNuXTp0pfG\ndhTxer3YQOFw2OA/OTRXN3G9XgeczIbr+vo6uFmRh+HCqVQK86iKVMQ0eJgj5+ggay0rVvqjHiTH\njh1DP7iWWrlcBg/NPkdbW1vYkFy3JhQKAYpdW1sDzLq8vIy5mpqawoHOobZ3797FGBw4cMBQxDvJ\nOAclJy3ktWONzNoptJkPbk7kyfQK/61d+62QPRdItUtqV6/XoaRGEY6e44MhmUxiXrg+E1+8OAKS\no1Y4apAjOri2lDXKg/Uch2bb0ZHj+u+wT0av1zPoZYbs7dwHRB7qWk6+ls1mQa1OTU3h+06nY0SU\ncoJYpUymp6eNWlBq5HBttVKpNHIh31wuZ+wt7UepVIKODoVCRuJaTnWgY5PP56F3Op0OzoxEImEk\n9NPPVnqLfSLt/OnYcORoqVGk0WgYKVe0v3v27DEiV/V762WFaRpOnKgRhPPz81gXnFiPE/d5vV4j\nYtbOb87n88H4Zf+sUeSFF4bXdqwAACAASURBVF4w2qaG1sbGhnFmMCWrfWTfpEAggNpbJ0+ehBtK\nNpvFGbO0tASDyhrRxhcXNowZWOEo0mHiUFqOOOKII4444sgjL7siPHzLYuuYHVy5pg7fIti7mm9Q\nPp9P9u3bJyIDBzS9nX722Wfy5ptvisiAQlAL3efz4b3BYNCwWBVFuXLlCpCEZ555BingRxG+nTJy\nxTlBlpaW8Jl/HwwGjZuK3ga55lc+n5ef/OQnIjLIO/Ttb39bRAYwIY+RXR0jr9cL59pf/OIXGBOu\nID+KNBoNW9SLo6JEHt56OGIun88Dst3a2gKU7/V6cQPgpHiFQgHjc//+fSNfhQojBYxuMcpkHZPd\n5NChQ3Ck8/l8xs2K67bp7Wh5eRnvyWQyhsO4jvft27eBKpRKJay72dlZ3HYY9hd56HTPVctzuZyR\nM4dvmONQWiw7JTPk24410odRC0aKGFHjm7Md+sTOyYzSMaXV6/Xw/Th0lv6eSx7orT4WixkooJ2j\nb6PRwDqq1WpGf5mKsnNs7na72Fvs6Mu5aFhCoRCQzEwmM1auoWg0aqBYui84sICpXe2DiJmLKRwO\no8bgvn37cIvOZrNYz6VSCX3nZG2xWAxjOzExged4vV4jTT/r8lGRus3NTfRvenoa+v3u3bty+fJl\ntFF1CpcD8Pl82H+rq6uISs1kMkDGfT6fgVTY0UZMzXm9Xjyf55yjCWu1mrGPh4k17xevI0bpdhJO\nSqrIN9fPisViRuSw6tC1tTXb/HdMNTOdy0FE4+b/euONN1BC6fDhw0gU+dJLL6FtH330EdYvI2/N\nZtOgLxVBf/zxx4HwdLtdBDVxrj1GPWu1Gp7DeeiYnqvX60YtuGEy1IfHLnul/ps2hEMr1QDo9Xr4\n7HK50MB9+/bJqVOnRGQAv+pk/uxnP5OPPvpIRAaHhFIwXq8XmTVffPFFcLnBYFDOnTsnIiL//u//\njsErFovgQtVXaJhwciN9b61WM8IN2e9BPc2PHz8OysTv92Ozrq+vY7M+ePAA3//jP/4jxu173/ue\n0T47CoHr4liLUz7xxBMj9U37ogu+2WwaCpf9ZzhagjeILiTOcsu+DuFw2KAmtb+cuM1KaTEEa5fM\njmmbYZJMJqEsisWikVSN/TrUWLt9+7YRLaBz0mw2Aa3evHkT/eAEg9PT09ic6+vrRkQd89aq0K2U\n8DiHIwv72zB8b627ZMflW3luVhy8d+3WIM8BR0rwgcF+RCxsUI0iTOdms1mMOe8TjvqoVqvoI+9d\nfq/P54ORs7y8jPllKoKNpXQ6baxV9rfQQ2Xv3r3QYZlMZqzDJJ1OG9FEnHlWP3OmeTYCObUCU1oT\nExO4AOnhImKGLjMtxYnbuO4RR5Tu2bMHRlEoFBq5XlihUIBOzGQysry8LCKDIq7qjvDkk0/CyEok\nEjjU+IJy9+5d6PQTJ06AsmP3CKaH3G43+sTRtoFAwKDZORqP/UrH8eHhCxvTwiJie4m11nfkvcKh\n6JzKRPu4tbUFfbOxsWHobqamVdgY4+zKHPI/irz99tuG0aLjf/z4ccMoVT9H7p81sasauiKCfXPi\nxAm4pORyOTxzbW3NoKbZdYPpXAZixhGH0nLEEUccccQRRx552dXk4wgT9mrniC2v12tYYUzHcH0d\nvfFyZXCv1ysXLlwQkUE6arXu2fEqHA4j78bp06cBiW1vb8PKK5fLiJy6c+cOKIdRaJ9wOIwbIFvB\nXK2brenHHntMvvnNb4qIyJEjRwx4Va3NSCQCT/Yf/ehHcDZeXl6W//iP/xCRAUz43HPPob92yZY4\nadqrr74K5OratWu4/Y4i1WrVto4OQ7DRaNRI3sfIiP4tp7lPp9OgDufm5kD5eL1eICm3bt0C5Mm3\nC0ZvuGZPsVjE7TASiYxUuVj7obc7vgVvbW0ZXv6aQ2h9fR3viUajxu+VxmLn7Pn5efR1cnISY9Dr\n9Yy8RIw8cK4QFb6NjOOwrO3kpGx25QkY3mXHV55nRpnq9fqXypeIDMZeb7xch8uawJJrr3Fklgq3\nbRRhhMfj8QDB4GrpvHY4Ssfv9wM1iMVioC/591tbW8ifE41GsTYZrWI9l8lkgH5w/qonnngC47yy\nsjJW9EsymTTKy+ia4SjCdrtt3JgZ7eHbr84d1wiLx+PGWHFeHUZGGNnVz36/39jfOv5erxe6eZhs\nb29Dv6dSKXnvvfdEZIDY6JjNz88bCTvtokM3NjaADk9NTRk1uRjVYSpV1w6XP2DKzJq/iud8nHW6\nU0kWa4QWRzHyHOr68nq9GIfZ2VmMN9f3W19fhz4tl8sGassUDjvx8p6wKxc0ily/fh3jlkgk4OQ+\nOzuLXFZ3794F+pTP540ktjo+7XbbyMmjey6TyeA5IgKEp16vI/lvr/ewyjyvdy6fEgwGjXEeJrsa\nPAwN80ByeB/zh1YomTetGh/PPPMMDpuVlRV59913RWQAj9mFuUajUfDTBw4cMLzg+dDk6JFxsmY2\nm01DcXOEFCdf+8Y3viEiA6NLYWNryKgq/VgsJmfOnMHnv/zLvxSRwQJRiPfKlSvy9NNPf6k9nKGT\nk/tls1kUMJ2enh4rqkCfK2JCnuxrxBQI14LiQ7rdbhtQrm6gdDqNMeQQVqYKmG5h4QOJfb3GyQzK\niQ+txfb04OPD0ev1AqKdmJjAplpeXsaB+ODBA/gNzMzMGMad9vvevXtQygzjNhoNg2phmo6V4jhK\n1prYj/lyDuXm8VZhZWEtYMp7iH9vJ1ZYnP0VeK8w1TUOpcXGL1OmU1NTmAuPxwMjhP37fD4fLgF7\n9+6Fkq1Wq2gz6xuu/cP7uFarYS1NT08bNIP2kQsiplKpseiQeDxuHAZq1EejUSOEmA9F9lPiOkx6\neJRKJcNA5QSoOia8TprNJgwY9u1hqiuVSoEKcrlcBlW2mxw9ehRzVSgUcHG4c+cO3BEOHjxoJERU\nKrJUKsHgefDgAc6J48ePG5dbnYd4PI4x297eNnxJ2VDVd7H7BUfArq+vj6VPOSrKSjXbRdpZw6W5\nJpTO1dzcHIw6n88HnyL2m1SaTuTL0ZkcYs/1xbg949SYdLvdcuXKFREZGKg6d4uLi3Axeeyxx3CZ\ntyac1Pfy+r1//z505MLCAsLeDxw4IL/yK78iIgPDh41eLjTOLjJ2hhyfHzv2a+QRcMQRRxxxxBFH\nHPn/VIbm4VGxOl/apc22eq9zaYZnnnlGRAbWvd6K3333XViRlUoFFlqv1wONEo/HAaeFw2Hcalqt\nluEYxcnXxnFkYqdAq7WoN4PJyUkkD5yenjaQLo5s4rwVaqE//vjj8qu/+qsiIvIv//IvgO6WlpZg\n+TINY3XitfNMFxleM4SFb8Jer9c2gROjKZw6XcSEanWO0uk0orSY1tzc3ISFXqvVjPbzOLM1budE\nzennh0kymTTKQOitXOQhVNpoNAAfT05OAjXMZDJAdS5evIjIv42NDaCS09PT+LywsGAkOONaXYws\nccI3uygOa5mJYWK9nXGZAF2zjPBYHSX5VqnoRCgUwlro9/vGrUnns9lsGunjGfmzS1RorQ81zl5k\nZKzb7RqJPHW+mAbg9P1cGXpxcRHVtTmR2b1794x0/DpWlUrFoDf0Brtv3z7ML9eiYvQpFArtWC3b\nTmKxGOay1WoB4YlEIkZpCbtkklZknUs1aL9YF3IkV6lUAqrTbDahn/jmz7mVYrEY0J5MJjNylNbB\ngwcxlnfv3pUPP/xQRAbr9+tf/7qIDBxWtd+dTge0MFN2165dQ0JQpkhY53IUHZcvajabRlJGO2d/\nrhVWKpVGRrBETKTWSh0zOs9zyHW7+IzRz3v37kUb/H4/5ofzEW1ubhpUOVPczGrsFGE5zpkh8jCX\nUaVSAQLG9bDm5uagazc3N42gB0YrGWHTs//mzZugw+bn56Ffjx49Chq0VCoZOaL0+aznOGhjFER5\n1xHgP7YmI+OXcBI0Vu4qk5OTiCqKx+MovPfOO++AcmBu1prsjCfZLiKFaYNGozGWAtre3jZqQulh\nwAZVKpXChucQTTYEeGJrtRomPBaLgT7h4nLlchmTPzExYRsuWa/Xjc3EyRvt6sbsJKFQCO1hmoyp\nCKswJ6yfQ6EQoOJMJmMUVFXDIpfLGaG/XC/MTjFoO6yfx8kMGgwG0a5+v4+NxPWPWq0WNs+xY8cA\nl7daLfh/XbhwAVEH7K80Oztr+PxovzlShnnlYrGIwyUWixm0IRsG44Slc+01ppd53XHWWjac2Rjj\nrNiBQMDIAKtjxQZpIBAwDhgVa0I8Frs6XKMIRyUyNL+wsCCnT58WkcFeVz8+Dt/lg23//v34DadY\nYCrH7XZj7tiYDIVCiB45duyYQevonrPSreNEaUWjUWN/6xqLx+NGJnIVjthpNpugf/x+v+GvxYVE\ntZ2RSARrlYtubm9vw0DK5/O2dGcwGISxZI2W2U2CwSASdr755pug8F988UVQ+Nls1vCt0zXLdaMu\nXLiAiB6+WFgvZrquO50OxoALAgeDQaN/+rndbsNI2NrawqV6FGEjhy/2VtrKjv5lY5YTQM7Pz2O8\nt7a2DGNMjR/2t2L/JetFhxNq6v7mZI+jCK9NTofAPqtsvBcKBXwOBoNof6PRMFxGVMcUCgXsxYmJ\nCazNiYkJrFmP52HtM7/fb7ibaN9ZD41yXjiUliOOOOKII4448sjL0MSDikh4vV7jpsEWFlua7Cmv\nt+6TJ08i2WAul0O+nZWVFVh/DK1mMhnbhG4iYuTD0baFQiHcdlwu11gIDzubRqNRwIp8A+cbHSNa\nbNFz/Rav1wsL10rL6M2Ka28xvM40ANMS1qiCcar78i3dCgnzODBMyFEITIfojTeRSBhjzunP7fKY\ncMQOf3a73baV4rk9w4RRN4Z3e72ecXPQ5Fn79u1D/9bX1+HgWigUMIeZTAbUCTtlM/Jz8OBBjEG1\nWgWylcvljNpGdlEl4yYd9Hq9RtQgO1/a1WDiGybTmBz9xjfAVqtlS8/xc62QPaNV7PzMCM84MLrf\n7zdQFB3zffv2YcxFBMhMLBZD+7nu3+LiIurscd20er1uIH7a/lAohPbv378f0SNTU1NG9W6ORNO/\nLRQKiMgcRVhPcAQRU1pWKlDfy2VBuGZWKpUyInBYlzDKZ0fDMULBFefL5bJRz2lUtC6fz8NN4cKF\nC3D2/973voe9wlForGdbrRacnBuNBnRNNBrFfrWiw7zGtR+lUgloCedssdac0vEeN4JJxMyxw6Ux\n7NA+Rlv5nPP5fEZJEG0DU46FQsGogaZizXvDekXHwePxGCWFxukjBwKxY7s1UlOFkdFsNos9cePG\nDUM/cZQhB15w1Xh13p6cnDSCoHgt251P2u7dZFdtZB1Uu0R8XJOm3+9jYDweDxb7yZMn8beffvop\nKASRh5M4Pz8PRfPYY4/Br2Jtbc0IWeOwcT3Y2DeC2zmKcDbpTqdj1A9RyimXy4F6S6VSRqicKiNW\nCK1WC3Cdx+OBTwtHNgUCAeNv2BdIE24lk0lsXDa6xumf/p4PSzZsODyVn6vjHIvFjEzXvND4kOPF\ny9lgmabkcHu7Tcwc+zg+PPV6HXN1+fJlOX/+vIgMjBkdv2PHjgFSn5ychGJfW1tDG44ePQoqZO/e\nvYgiCAQC6Eez2YTB8JWvfAWHzoULF4x6MGpcWaOiVMZJrGgVVnBM87JRzIcjXwgikYgBE3NWZLsM\nzLzueA6tVJqK9XAcp49MsXU6HbQ5FApBcXMECB+W3N+ZmRmkstjY2ECbNzY2sP/cbrcRzaKU5enT\np+Ev1G63QbEwjcg19/L5PGibUYSjV7m//DkUChlGi/pJeDweGOF79uzB2gsGg5ivYDBorFU7SplT\nHOi7RUyjgaO6wuHwyAb67du3QSnPzs4i+mZxcdGIXNT312o1zMPKygoo5RMnTuD8YF250/piXczP\nZLqSjXTO0p3NZg1fkWEyMzODdcTRuXyxLBQKWI9bW1sGPah9/JM/+RP5jd/4DREZ6CRda9euXYPv\n09WrV3GZZBqzWq0ahrDuD32HyOByrWshmUyOBQR4vV7jQmBHJ9VqNZzB+/btg4/W/v37Ybi+++67\n8KdjQ5f3Qb/ft02nwXPa7XYN+prD0pleHKZvHErLEUccccQRRxx55GVoHh614AqFglGOnm/3agmy\nRRaPx3FDnp6ehvW6tLQExCYSiRj5LPRWtri4iPd+9tlnyENQKpVgWXc6HVh84XDY8NYfJ/qFo4d6\nvZ7hlKttvn//PsrXz83NwRpli5IpPx0LkQGCoJRJo9HALW5hYcGoC6ZW/E9/+lN56623RGRQt+Q7\n3/kOns/OxuM4SrIDaCAQMCgkFWulci4dohY0o3nc31qtZtSX4rw6jNgwgsTt59ubXUK0YVIoFOAo\nuba2hrXj9XoBi3Opgvn5eVlaWhKRwU1Zx0ZRGZHBelR6c2JiwohS0X6XSiVQlFx6IBqNYo1YS4Vw\nn8fNUcOIip3TsojY0kmMfnCEEe+V3fYNRwMxJcQUjJ0T7061qHYTbbPb7UY7o9Eo2s9JNKvVqhFA\noOt0YmIClBYnDdU1ou/R9RUMBuXkyZMiIvLyyy/L4cOH8UwVhtRLpRLW2MbGBpC9UYTRQo6KYkfx\nYDAIFJlrRCWTSSCQnAtIx0Lky1Q835B1bFOplKFLeK8resmoyjiRdr/85S+xJ1555RU4m7fbbaPs\nECNzOg/r6+tAtzmwwOpsa5evjRNkcnBIIBAwIrP0XeVyGfs4m80CgRlFGH22Bh9wkk7dH1wjMBgM\nArFLJpNG4IeeN6urqzjz2PWB0Qym0lhPWikzlXFZgWg0irXAiBbXgrt37x4+nzhxAnO9uLiIIKVm\nsym//OUvMTZKV2UyGehm1hG1Ws0YN50jK6XMkb36G6a6dpJdDR5rGB/7k7C3O3+vn7PZrBw8eBCf\ndcLn5ubk13/910VkYBTpYg4EAnLixAkRGQzwp59+KiKDTaA1WPL5PJJaxeNxTIjP5zPCksdJsMRG\nCnO5zHP3+30sxna7jTHhcFbrotY23Lp1CxBmuVzGAl9YWDCoNIWBf/7znyOZk9frRdbSY8eOQal5\nvV4j9HqUPrKvgPaRDVfetBzZ0Ol0MM4cis51dx48eAADol6vY3xYUTHUznQYUzvWqLdRfXi2t7dx\nKGQyGfDi2WwWm2phYQEREel0GvNQKBRAV+ZyOYwxJwXjLLtut9sohGpHGbBRac2uPA7fzMLGCNMr\nvBetlw+WYe/a7ZJgd+BZKUo72mtcg6fZbBqZ2tn4sYv+ZCXYbreNaEKmMnXtq4+Bij4zkUhArxw/\nfhxrw+oPqONQKBQMo4vXwzCx+lZxgjb2mVDhMOCZmRn4QnICzHa7DQqd/Sg5SpINnng8Dv3BFxqX\ny2X4CHFk1Kh7sdVqIaz/ySefRKoAzZ6rv2F/DB2/TqcDqjmRSBj+o7y+mNLkemic+JUjjNjHVH9f\nrVYNym5cYdcKLhrN4djaHp7bVCqFMZmfnzeiJJWWtyZvZbqSk4zqXrGePXwR1d9zf0eRXq8HPXr4\n8GFjvWjbbty4AV+jiYkJzIsCGta+F4tFrOVIJGLoVzb2dBw4waDV/YIvXvrecDg81GfQobQcccQR\nRxxxxJFHXoZSWnbOQSxWGJSTDSolkEgkYNlNTEzAAuU6Km63GxYlJ2Wr1+vy+eefi8jAIVURhnQ6\njRsA17xh634UYWcvj8eDNpw8eVI+/vhjtE1h67W1NfSX89gwAsaRD5999hnQG7/fj9vPwsKCcWvR\n53CejkuXLsmPf/xjERH53d/9XfTRGqkwTDiXB+ca4ugHTjDHUSKcqG5mZgZzmkqlQGPdu3cPybEY\n6Wg2m1/KqSQyWFeMULBD9U7OibtJIBAw6i6p9d/r9YzkaRxxo9Ltdo3buv4b3755DHiuOp0Ofm+N\ncNHvo9Gogbqwc/04kVpM2eyU7HOniBG+ETGKws7J1pxJds7sjFxysklrMIHdPI/aR75tM5qjN71o\nNIobINMhHOlRr9cx/olEAgnsjh8/bjyToyR1foPBoFGigNPZM82kc5FKpcZCW3ldWfO52KFsjLpE\nIhGMTyAQMJAcpvR1/DmxKEfqJZNJIFpM+ej7dBw4kGJUdODUqVNwZUilUlh3qVQKSA7nafH5fNAj\njBTynt6p3A6vWc6bxgk1GeHhZIP1eh2/LxaL0O/qFL6bRCIRgxZmZ3YdP3ZCT6fToOc4waD1DOAI\nJj3bgsGgkWyX3Q7YOZ33JbMOmtQxGo0CjVGUcDcJBAKghRcXF9HfQqEAh+oPPvjAQGMUpbly5Qqo\nyZs3bxr0vo4P59sJBAJgcdbX142AFs7/wzQo6z9G31Uf7MTy7GrwsHIUeaiArKFpzBnqIpqYmDAW\nLE8+J4JiyFW/r9VqxoLVELef//zn4ACffvppLKIzZ84gsmLv3r0jLVqV+fl528FZX1/HwtzY2EDU\n2KVLl2zrKvH4VCoVOXv2LNqsBk86nUbUwqFDh4zDQNv81FNPgfNcX1+XN954Q0QGtJHW81pcXByp\nboiK2+0GVcNcOnvf88FfqVSw0DKZjBHarYrS7XZjXra2tvC37F/EUCsrbqZerOGhdhFEwySdTgMy\nDwaDMIpFHoYwp9Npo8CoGie8kTiRFqcKKBaLaK/b7TZCRpVKWF9fN2ozqeE8MzNjJAy0q7E1itj5\n6ehnbbPH47HNWs2+dfl83vBdYeNUFSv76nCY+U7+QmzIWbN3j9PHVCplm+BM2yFiQuTW2nfaZq7h\nxheyeDxuGDNMxdsZkGyQWJO+6b6fm5sbi9LS91k/83dsrPIe0n8TMRP2WdMR8L5hHy2mq5Ty44Sv\n1vnS8SyXyyPXmnr22WdxkHGUmM/nM7Irs9HC1BlHS7HPGq8v7q+OPftRMR3XbrfRDz5v2Ei4c+fO\nWEn5pqamjAs2p1ZR/ZXNZqErOQv4gQMHYIDv3bsX7Wm324YPkj6TI105GouN/VqthrHlRKcejwf6\niZM6jiLf+c535MUXXxSRgd7XNiwvL+NsW1pawr5cX183qFEtCn7p0iXo46mpKRhRhw8fxly3Wi3o\n7/v37xvzruPJdRzZmOSLAtPPO50dDqXliCOOOOKII4488rIrwuP1emE5ZjIZw/pWyy6VSsFiPXLk\nCCKSpqengVpwHgd2bG6327BS2RmqXq/Dws1kMrg5X7hwAU6oIgJP8BMnTsiRI0dEZGD9Kcw2ivAt\nkRMGTkxMAJp98OABLNwrV64A4ZmcnAQU3ul0JJfLicgA1fnnf/5nERk4Yem4PfXUU/L888+LyABi\nZDhe5Wtf+xqotLfeegsw+kcffYQok8OHD4Ma+/a3vz20j+ywyDQR3ySYXnG5XJj3bDZr3GY4saSi\nRgx3Wx1qrc/dTTgxn8jo1cSXlpbgeJzP5zFmTMEobC4yWHeK0nCJj2KxiPdHIhGgifV63Uj0qO/i\nFP2lUgnwcTAYNBz3uP875eUZJlakZKdbOScDZMibq2nr7xnhazabtg6gOzldc+QXw+hWamac3B+c\nJM7v9xsOydxvppm0ffV63ahxp8LRXvzMRqOB33NCOh4HTmfPFIjH4zHK3Yw7j5yIknMlcckJdmDV\nNTwxMWGUn2Aag+sM6fi3Wi3o1EqlYiSn0xsyU4iNRgN93ymh5TCZm5vD2q9Wq0bdKNWtVgpG29Vs\nNqErp6amjGhYzh3Ga82aA0rErEjP/eDIRS5FkkqlxnJczmaz0I/cfqY6k8kkGIJMJgPac+/evUB+\nGA0vl8vQUUzPxuNxI2koO3tz1CD/hveLojrhcPhLqOlu8tprrwGZ6Xa7iBo7f/48zieRh3vz/Pnz\n8sILL4jIgH1RxK1UKuH8OH78OCK5jhw5gr7fu3cPbiuM8PC+ZHo5HA4b88sll4btxV0NnmQyadSV\nUZiekxLF43GESh4/ftyoE8LhrDrYrVbL8KXgEGZVjlwXJRAI4Jnr6+vy9ttvi8ggwdVLL70kIoNE\nR/qcer2OqCgd3GGi7SwWi3jX/Pw8KKStrS1MyObmpvzkJz8RkcFkq1G3srIily9fxu+Vk+x0Ogh3\nfvXVV0G9sZ8ER0FkMhn5nd/5HREZKAzNSl0ul7GItre35eLFiyIi8rd/+7cj9dGu5gxn0ORoqWg0\nisN+dnYWGzebzRoGhBoN1iSBrGDsorHYk56TELLvENeHGSavv/46oFuOBgkGg1BMHPVRqVSQGKtY\nLBqZofUgO3DgANa+teYYRxKpwZtMJjFmnPmW6TtrGoBxhDe/NcycLxNMx3BKAO0j+wG0Wi0j+d5O\nmX7Z50SF53YnJcNrbRSp1+tGQjduD1M/vGZVZ7AhYfWN4gy8egCwQcppBNhQFHkYacgFddl/KRqN\nGn6Aw4QveezTwJGLbJBvbW2hbRy9U6/XjSSv7Lul47a1tQXjnCNK2X3A7/cbEbcqrJPYX2SU/nHU\njOr9SqVirDUOhVep1+vGBZsvonw2cLg808X6+0Qigc/tdttYp+zHpLKwsDBWKpNwOIxDWfWOyGAO\ndX1xorxQKGToA23z6uoq9Mf29jaM3FarhfUVi8Xw+1gsZmtcsR4PhUJoQ71ex/xzlOwoEovF8JyN\njQ0kc33nnXeMPupa297elnPnzonIYGy/9a1vicigeLaugbm5OaPmofr5XLlyBc9fXl62TaHCfotc\nt5DTyljpXDtxKC1HHHHEEUccceSRl6GFbthiUmtxZWUFll0oFAKNVSqVYKV+8sknuJmk02ncTDg/\nATtRseWrfy9iJgvz+Xx4/tmzZ1GzhR2P2+02aIa/+qu/GjoADPF7PB6DjtFSBI1GAzeu1dVV3A7/\n7d/+DVZnPp/HmDAMOT8/j+SBTz/9tBHhwxXMVZrNJqi6P/uzPwM69NZbb2H8u93uWHVROKEfV3Jn\np3SOkIpGo7DEU6kUaER2lGMqqFQqGRQL38Y5eoeTVbKzqV3uhHFKhFy+fBmITT6fN0pb6A2KHZIb\njQYcyRnmjUajRj0bV4VvzwAAB7NJREFUTjypbcxkMnhmNpvFWG5vb6O909PTcKi3Rp6pjIvw+Hw+\no7QLIzyciI0pLXb+03lrNBrYH4yWWJ1Buc0cIWNXXoGdU7XPImI7r7sJP6PRaBgoDNMb3Ea+OTOK\nyTdPnaNOp4M5LRQKeFYymTSqojMioN+zc681MmecfnI0EX9uNBpGjhWeC21/oVAAbZpMJvG3LpcL\nc1qpVPB5c3MTqDDn02KnbnZKt86voiq1Ws2oZ7ibrK6uGvSKjiXnhLEivPz/ipax47H2kf9G+8EO\nyTpX7IDMFBhHqnHCP+3vqNLr9YyzS88Gj8dj5L3RZ7rdbswVVz/f3NxEf5l2drvdmCsuoeT1evF7\nRroY8eV3eTwenLtcm3IUefPNN6HfNzY25NatWyIyQNB1zDkqzev1ynvvvYe+/9Zv/ZaIDKL2eK3p\n2rx16xZKsiwtLcFthaurM4JnjQZnXchnv8pOTui77tRWq4WEcqVSSd58800RkS9FJegkc3G+7e1t\nRBuJmNlX2TOdo7Q4CSFvPE6Ux0nNePBYSY3DqTPUy7wuZ9B8+umnsaDefPNNUFdcS4QjD/x+vzz3\n3HMiIvLiiy/is8/nw1ixomHhbJFHjx4F93vo0CH54IMPRGRA7dn5iOwk1rBO9u1g/xsu4KYGTzqd\nNkKy9b35fB7Kt1wuG0qFDwbe9LpIOVmlNTSXfThGnceVlRW0JRAIYPyY7uH55AOOjRBWiJxpef/+\n/UZUC/t1qITDYSTaTKfTRgQQR6Tx53HEGorOY6bC1IM1+zEfKrznVPigt0YM2WWqtYbhMy3IRUjH\nEW5PvV7H8wOBgOFjoePu8XiMApfcNjWAv/jiCyjWQqGAg6pSqaBfHO7NSSbj8TiSVU5OThrZZvX3\nfCirX91uwhFYfPlrNptGOgU2OPX7fD5vpLjQvvf7fYxVsVjE4ZbP542wYR2fQqFg+DlyXSv9W2sB\ny1GjtDhJK9cOrFQqoH8mJyeNsHTV+4lEAvqOs7pbs7Lr+mW/Ks6qzmcMG1RWw1T3yrhRdpxOpVar\nGWHUqk99Ph90aLPZNN6ttDuHcrMu9ng8eCavcU48yBm42R2E/aA4RUGhUBgrEu1v/uZvjP3EdCvT\nyDoOa2tr0Enr6+vw+VlcXATVH4/H4ef68ccfA8yw+nRx6gD2GeSabyps8DQaDSOtgZ04lJYjjjji\niCOOOPLIy64Iz8TEBKzR1dVVw7mNLVB2UlNYrl6vG1Y2IwCc5Itvy4w8MDTPN092YmLonL8fB+Hh\nqLFGowFrkZNacV6VV199FZRTPp+HtRsIBGDRP/7448ZtX8eEb26cf4LfxVA2O0fu27cPXvO9Xu9/\ndCsRMdENTtAVDofRnlQqBcg3HA4bNBbf+rjNDIvz2NpVYLfetBjxYwRvVEfCRCJhIDY6h5zXhz97\nPB7kxQgGg0a0nN4uOGpwdXUVCBK3d3t7G5EVPLeBQMBwZOUxsEY9jSpMafl8PiP6ya5UgbVOFkPe\nfFPiiCd2rmaElaOldHxqtZpBo+j3nOuEk+ONIjx3XBGZ1ynfMEXEuOVq++v1OhCes2fP4iZZLBZx\nQ7ZG73CSSYXORQQIz9TUFPZfIpEwqs/rvGs9rt3E7/cb6J8KO11z/S+unJ7L5dDfZrOJPc0ID1Mm\njPb4fD4jEESfn0wmjRsy092qtzjycZgwLcwJWznajKu7M4KfyWSM+oIcLcfjxKiYSjgcNmgqpqoZ\naeQgGXYAHmcvBoNB4/kcaMHvZPSRhWtCqVjz8DCqxfXQGL3mSCVr//Qz67ZxnJa3t7e/RA2KmGe5\n1Vmef6/uJlevXsV3fN5XKhWjDAdHW+q4+f1+rMFmswmU3RohyolOh+Wn21UbzczMgFvjmi4cxcG8\nWSwWwws5AZKIGEaLfmbP8UQiYWwIFT5I/H6/sZk4IRMnXxtHyTKl0263jeR4nIiN28lJERWCnZiY\nGDoJjUbDSArFRoJdJAwXSePDht87rkSjUcwjhyVz7aJutwuo9d69e7Z+LblczoC/ub87hbMyvWRX\nJ8nK7Y9K+8zPz+OQ4tBTLpTJhyb7ZgQCASPjqr6TYX89PHVstK8cpdDtdmHwhkIhg4dW+Z8aOyKm\nwcNzxWuNa8HxumOFzv4/XCBS+yBiXmisvjraX2sdK1Ws1lDSUVMLaJt1jYRCIcN/htcXZ6dl+pRD\ny5X62djYALzOycs4nLjf7xtRQHzw8IVMdSErbjak//iP/3ikPjL9bpc5V9+t7eQCikopswHDBk+5\nXDZSJej3TO9ub2/DH7DVamE9WDNy6/PHybTs9/thiFWrVRg8lUoFRVmZdi6XyxiDaDSKOW80Gmi7\n7iuRwfzoIcjRTHxh63Q6tjXl+GBl/8JerzdWyDavU77Yc4JE7iO7MjA12Ov1sHb4/UytW8UuLYSI\nGGvKTueM6zPY6XRsa5nxXrHWr9M2s7HKiVGZAut0Ovi9y+Uy/NfsQvIrlYpxOee54wvWsOz1DqXl\niCOOOOKII4488uIa1/JzxBFHHHHEEUcc+f9NHITHEUccccQRRxx55MUxeBxxxBFHHHHEkUdeHIPH\nEUccccQRRxx55MUxeBxxxBFHHHHEkUdeHIPHEUccccQRRxx55MUxeBxxxBFHHHHEkUde/hfjAJy/\nhqM4BwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuRUBBxt5zXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "trainY = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9S8keQw6V9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "aa7d9587-2e15-4f76-ac73-77d1dd4cfd68"
      },
      "source": [
        "trainY[1:10]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWKs84zHAKqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "f9e5848e-2e4d-4c97-9a7e-7efd2f023b26"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape = (1024,)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = 'relu'))\n",
        "sgd = optimizers.Adam(lr = 0.01)\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_62 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 4,212,746\n",
            "Trainable params: 4,210,698\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxQLjdel4AHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c1f7ebc-457d-42bb-e081-34b0f0bebfbc"
      },
      "source": [
        "model.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 3s 77us/step - loss: 2.4034 - acc: 0.1000 - val_loss: 2.3032 - val_acc: 0.0982\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.2980 - acc: 0.1077 - val_loss: 2.2667 - val_acc: 0.1316\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 2.2096 - acc: 0.1523 - val_loss: 2.2193 - val_acc: 0.1381\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1016 - acc: 0.1858 - val_loss: 2.0741 - val_acc: 0.1929\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1044 - acc: 0.1710 - val_loss: 2.1456 - val_acc: 0.1418\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0670 - acc: 0.1855 - val_loss: 2.1453 - val_acc: 0.1888\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0418 - acc: 0.1970 - val_loss: 1.9975 - val_acc: 0.2046\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0140 - acc: 0.2091 - val_loss: 1.9520 - val_acc: 0.2254\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9973 - acc: 0.2192 - val_loss: 1.9718 - val_acc: 0.2218\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9745 - acc: 0.2323 - val_loss: 1.9995 - val_acc: 0.2341\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0066 - acc: 0.2235 - val_loss: 2.0605 - val_acc: 0.2279\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0451 - acc: 0.2126 - val_loss: 2.1547 - val_acc: 0.1882\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0555 - acc: 0.2050 - val_loss: 2.1845 - val_acc: 0.1374\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 2.0170 - acc: 0.2149 - val_loss: 2.0399 - val_acc: 0.2224\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.9720 - acc: 0.2414 - val_loss: 1.9755 - val_acc: 0.2411\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9518 - acc: 0.2491 - val_loss: 2.0097 - val_acc: 0.2206\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9561 - acc: 0.2613 - val_loss: 2.1945 - val_acc: 0.1656\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9083 - acc: 0.2787 - val_loss: 1.9207 - val_acc: 0.2896\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8893 - acc: 0.2935 - val_loss: 1.8376 - val_acc: 0.3019\n",
            "Epoch 20/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8733 - acc: 0.3104 - val_loss: 2.1626 - val_acc: 0.2263\n",
            "Epoch 21/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.9411 - acc: 0.2842 - val_loss: 2.1987 - val_acc: 0.1684\n",
            "Epoch 22/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9079 - acc: 0.2980 - val_loss: 2.0944 - val_acc: 0.2263\n",
            "Epoch 23/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8838 - acc: 0.3166 - val_loss: 1.8065 - val_acc: 0.3590\n",
            "Epoch 24/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7699 - acc: 0.3742 - val_loss: 1.9191 - val_acc: 0.3040\n",
            "Epoch 25/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7802 - acc: 0.3800 - val_loss: 1.7091 - val_acc: 0.4271\n",
            "Epoch 26/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8132 - acc: 0.3785 - val_loss: 2.3913 - val_acc: 0.1622\n",
            "Epoch 27/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.9133 - acc: 0.3390 - val_loss: 2.0061 - val_acc: 0.2981\n",
            "Epoch 28/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7418 - acc: 0.4116 - val_loss: 2.4028 - val_acc: 0.3258\n",
            "Epoch 29/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.6689 - acc: 0.4495 - val_loss: 2.2957 - val_acc: 0.3079\n",
            "Epoch 30/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7149 - acc: 0.4340 - val_loss: 1.5645 - val_acc: 0.4791\n",
            "Epoch 31/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6587 - acc: 0.4525 - val_loss: 1.5242 - val_acc: 0.4851\n",
            "Epoch 32/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.8090 - acc: 0.3812 - val_loss: 2.1177 - val_acc: 0.2405\n",
            "Epoch 33/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8681 - acc: 0.3548 - val_loss: 1.9132 - val_acc: 0.3332\n",
            "Epoch 34/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6996 - acc: 0.4425 - val_loss: 1.6718 - val_acc: 0.4516\n",
            "Epoch 35/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6676 - acc: 0.4549 - val_loss: 1.5015 - val_acc: 0.5023\n",
            "Epoch 36/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6178 - acc: 0.4681 - val_loss: 1.5887 - val_acc: 0.4466\n",
            "Epoch 37/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.7671 - acc: 0.4054 - val_loss: 1.6024 - val_acc: 0.4752\n",
            "Epoch 38/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1392 - acc: 0.2209 - val_loss: 2.2079 - val_acc: 0.2129\n",
            "Epoch 39/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1212 - acc: 0.2237 - val_loss: 2.0026 - val_acc: 0.3016\n",
            "Epoch 40/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9939 - acc: 0.2817 - val_loss: 1.8794 - val_acc: 0.3407\n",
            "Epoch 41/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8968 - acc: 0.3243 - val_loss: 1.7274 - val_acc: 0.4078\n",
            "Epoch 42/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7991 - acc: 0.3776 - val_loss: 1.6266 - val_acc: 0.4748\n",
            "Epoch 43/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8360 - acc: 0.3641 - val_loss: 1.7412 - val_acc: 0.4154\n",
            "Epoch 44/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9447 - acc: 0.3141 - val_loss: 2.0008 - val_acc: 0.2849\n",
            "Epoch 45/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9058 - acc: 0.3275 - val_loss: 1.7742 - val_acc: 0.3838\n",
            "Epoch 46/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7855 - acc: 0.3849 - val_loss: 1.6191 - val_acc: 0.4524\n",
            "Epoch 47/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7925 - acc: 0.3807 - val_loss: 2.0240 - val_acc: 0.3415\n",
            "Epoch 48/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8043 - acc: 0.3767 - val_loss: 1.7742 - val_acc: 0.4027\n",
            "Epoch 49/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7515 - acc: 0.3965 - val_loss: 1.5346 - val_acc: 0.4931\n",
            "Epoch 50/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6847 - acc: 0.4270 - val_loss: 1.4991 - val_acc: 0.4926\n",
            "Epoch 51/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6512 - acc: 0.4427 - val_loss: 1.5105 - val_acc: 0.4903\n",
            "Epoch 52/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6000 - acc: 0.4616 - val_loss: 1.4207 - val_acc: 0.5262\n",
            "Epoch 53/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5647 - acc: 0.4751 - val_loss: 1.4217 - val_acc: 0.5187\n",
            "Epoch 54/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5547 - acc: 0.4884 - val_loss: 1.3774 - val_acc: 0.5391\n",
            "Epoch 55/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.5883 - acc: 0.4754 - val_loss: 1.3874 - val_acc: 0.5484\n",
            "Epoch 56/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.5320 - acc: 0.4891 - val_loss: 1.3895 - val_acc: 0.5373\n",
            "Epoch 57/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5208 - acc: 0.4943 - val_loss: 4.8734 - val_acc: 0.1824\n",
            "Epoch 58/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6269 - acc: 0.4455 - val_loss: 1.5030 - val_acc: 0.4889\n",
            "Epoch 59/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5155 - acc: 0.4959 - val_loss: 1.3491 - val_acc: 0.5383\n",
            "Epoch 60/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5096 - acc: 0.4975 - val_loss: 1.3418 - val_acc: 0.5468\n",
            "Epoch 61/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.5287 - acc: 0.4871 - val_loss: 1.5345 - val_acc: 0.4858\n",
            "Epoch 62/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5087 - acc: 0.5037 - val_loss: 1.3127 - val_acc: 0.5584\n",
            "Epoch 63/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4750 - acc: 0.5208 - val_loss: 1.3645 - val_acc: 0.5481\n",
            "Epoch 64/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4509 - acc: 0.5220 - val_loss: 1.4402 - val_acc: 0.5261\n",
            "Epoch 65/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5502 - acc: 0.4833 - val_loss: 2.6655 - val_acc: 0.1349\n",
            "Epoch 66/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7425 - acc: 0.3994 - val_loss: 1.5980 - val_acc: 0.4458\n",
            "Epoch 67/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6011 - acc: 0.4602 - val_loss: 1.5365 - val_acc: 0.4948\n",
            "Epoch 68/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5096 - acc: 0.4935 - val_loss: 1.3624 - val_acc: 0.5389\n",
            "Epoch 69/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5118 - acc: 0.4937 - val_loss: 2.0765 - val_acc: 0.3489\n",
            "Epoch 70/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7032 - acc: 0.4157 - val_loss: 1.6101 - val_acc: 0.4565\n",
            "Epoch 71/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.6086 - acc: 0.4557 - val_loss: 1.4249 - val_acc: 0.5247\n",
            "Epoch 72/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5230 - acc: 0.4914 - val_loss: 1.3272 - val_acc: 0.5511\n",
            "Epoch 73/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5019 - acc: 0.4980 - val_loss: 1.3543 - val_acc: 0.5465\n",
            "Epoch 74/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.5165 - acc: 0.4940 - val_loss: 1.3129 - val_acc: 0.5517\n",
            "Epoch 75/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4590 - acc: 0.5113 - val_loss: 1.3184 - val_acc: 0.5537\n",
            "Epoch 76/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4268 - acc: 0.5333 - val_loss: 1.2600 - val_acc: 0.5788\n",
            "Epoch 77/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4068 - acc: 0.5389 - val_loss: 1.2899 - val_acc: 0.5723\n",
            "Epoch 78/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3938 - acc: 0.5393 - val_loss: 1.2591 - val_acc: 0.5843\n",
            "Epoch 79/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3734 - acc: 0.5529 - val_loss: 1.2303 - val_acc: 0.5842\n",
            "Epoch 80/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3572 - acc: 0.5582 - val_loss: 1.2358 - val_acc: 0.5899\n",
            "Epoch 81/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3617 - acc: 0.5532 - val_loss: 1.8275 - val_acc: 0.3625\n",
            "Epoch 82/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5391 - acc: 0.4929 - val_loss: 4.4740 - val_acc: 0.1834\n",
            "Epoch 83/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6340 - acc: 0.4570 - val_loss: 1.3590 - val_acc: 0.5555\n",
            "Epoch 84/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3927 - acc: 0.5411 - val_loss: 1.2463 - val_acc: 0.5836\n",
            "Epoch 85/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.6330 - acc: 0.4818 - val_loss: 2.3549 - val_acc: 0.4066\n",
            "Epoch 86/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4078 - acc: 0.5366 - val_loss: 1.5280 - val_acc: 0.4842\n",
            "Epoch 87/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3856 - acc: 0.5484 - val_loss: 1.2500 - val_acc: 0.5793\n",
            "Epoch 88/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3495 - acc: 0.5611 - val_loss: 1.1946 - val_acc: 0.6005\n",
            "Epoch 89/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3199 - acc: 0.5731 - val_loss: 1.2234 - val_acc: 0.5865\n",
            "Epoch 90/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2928 - acc: 0.5792 - val_loss: 1.1744 - val_acc: 0.6096\n",
            "Epoch 91/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3740 - acc: 0.5542 - val_loss: 1.7288 - val_acc: 0.4001\n",
            "Epoch 92/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3761 - acc: 0.5540 - val_loss: 1.1976 - val_acc: 0.6025\n",
            "Epoch 93/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3144 - acc: 0.5737 - val_loss: 1.4036 - val_acc: 0.5328\n",
            "Epoch 94/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3719 - acc: 0.5498 - val_loss: 1.2003 - val_acc: 0.5992\n",
            "Epoch 95/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3154 - acc: 0.5692 - val_loss: 1.1797 - val_acc: 0.6115\n",
            "Epoch 96/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6162 - acc: 0.4855 - val_loss: 2.3387 - val_acc: 0.3018\n",
            "Epoch 97/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4963 - acc: 0.5062 - val_loss: 1.4534 - val_acc: 0.5352\n",
            "Epoch 98/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3873 - acc: 0.5509 - val_loss: 1.2516 - val_acc: 0.5953\n",
            "Epoch 99/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3351 - acc: 0.5673 - val_loss: 1.1664 - val_acc: 0.6173\n",
            "Epoch 100/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3117 - acc: 0.5744 - val_loss: 1.2568 - val_acc: 0.5786\n",
            "Epoch 101/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6069 - acc: 0.4687 - val_loss: 1.4277 - val_acc: 0.5266\n",
            "Epoch 102/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6510 - acc: 0.4394 - val_loss: 1.6716 - val_acc: 0.4314\n",
            "Epoch 103/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4755 - acc: 0.5090 - val_loss: 1.5419 - val_acc: 0.4871\n",
            "Epoch 104/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4154 - acc: 0.5345 - val_loss: 1.3212 - val_acc: 0.5593\n",
            "Epoch 105/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3765 - acc: 0.5487 - val_loss: 1.2234 - val_acc: 0.6027\n",
            "Epoch 106/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3393 - acc: 0.5683 - val_loss: 1.3211 - val_acc: 0.5453\n",
            "Epoch 107/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3443 - acc: 0.5606 - val_loss: 1.1795 - val_acc: 0.6132\n",
            "Epoch 108/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3062 - acc: 0.5813 - val_loss: 1.1721 - val_acc: 0.6206\n",
            "Epoch 109/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4035 - acc: 0.5403 - val_loss: 2.0254 - val_acc: 0.3121\n",
            "Epoch 110/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4426 - acc: 0.5119 - val_loss: 1.3575 - val_acc: 0.5502\n",
            "Epoch 111/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3402 - acc: 0.5652 - val_loss: 1.2337 - val_acc: 0.5972\n",
            "Epoch 112/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3121 - acc: 0.5704 - val_loss: 1.3047 - val_acc: 0.5712\n",
            "Epoch 113/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4405 - acc: 0.5274 - val_loss: 1.3468 - val_acc: 0.5486\n",
            "Epoch 114/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3575 - acc: 0.5569 - val_loss: 1.2100 - val_acc: 0.6042\n",
            "Epoch 115/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4602 - acc: 0.5199 - val_loss: 2.3979 - val_acc: 0.2096\n",
            "Epoch 116/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5094 - acc: 0.5039 - val_loss: 1.4715 - val_acc: 0.5022\n",
            "Epoch 117/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3714 - acc: 0.5605 - val_loss: 1.3932 - val_acc: 0.5388\n",
            "Epoch 118/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3419 - acc: 0.5715 - val_loss: 1.2292 - val_acc: 0.6143\n",
            "Epoch 119/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3129 - acc: 0.5832 - val_loss: 1.2232 - val_acc: 0.5965\n",
            "Epoch 120/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2970 - acc: 0.5891 - val_loss: 1.1385 - val_acc: 0.6355\n",
            "Epoch 121/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2763 - acc: 0.5946 - val_loss: 1.1565 - val_acc: 0.6303\n",
            "Epoch 122/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2465 - acc: 0.6052 - val_loss: 1.1542 - val_acc: 0.6316\n",
            "Epoch 123/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2445 - acc: 0.6033 - val_loss: 1.1173 - val_acc: 0.6456\n",
            "Epoch 124/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2292 - acc: 0.6126 - val_loss: 1.0994 - val_acc: 0.6481\n",
            "Epoch 125/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2082 - acc: 0.6200 - val_loss: 1.1185 - val_acc: 0.6467\n",
            "Epoch 126/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4918 - acc: 0.5142 - val_loss: 2.3346 - val_acc: 0.2011\n",
            "Epoch 127/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5595 - acc: 0.4772 - val_loss: 1.4558 - val_acc: 0.5191\n",
            "Epoch 128/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3762 - acc: 0.5613 - val_loss: 1.2388 - val_acc: 0.6124\n",
            "Epoch 129/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3240 - acc: 0.5829 - val_loss: 1.1782 - val_acc: 0.6306\n",
            "Epoch 130/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2865 - acc: 0.5973 - val_loss: 1.1476 - val_acc: 0.6387\n",
            "Epoch 131/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5603 - acc: 0.4910 - val_loss: 1.5665 - val_acc: 0.4519\n",
            "Epoch 132/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4281 - acc: 0.5379 - val_loss: 1.4066 - val_acc: 0.5177\n",
            "Epoch 133/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3660 - acc: 0.5615 - val_loss: 1.2682 - val_acc: 0.5868\n",
            "Epoch 134/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3132 - acc: 0.5824 - val_loss: 1.1547 - val_acc: 0.6333\n",
            "Epoch 135/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2793 - acc: 0.5940 - val_loss: 1.1452 - val_acc: 0.6406\n",
            "Epoch 136/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2965 - acc: 0.5890 - val_loss: 2.2969 - val_acc: 0.2876\n",
            "Epoch 137/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.6261 - acc: 0.4671 - val_loss: 1.4465 - val_acc: 0.5089\n",
            "Epoch 138/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3930 - acc: 0.5551 - val_loss: 1.2333 - val_acc: 0.6083\n",
            "Epoch 139/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3401 - acc: 0.5785 - val_loss: 1.1709 - val_acc: 0.6315\n",
            "Epoch 140/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2848 - acc: 0.5944 - val_loss: 1.1306 - val_acc: 0.6508\n",
            "Epoch 141/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3107 - acc: 0.5845 - val_loss: 1.4856 - val_acc: 0.5251\n",
            "Epoch 142/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3148 - acc: 0.5852 - val_loss: 1.1311 - val_acc: 0.6437\n",
            "Epoch 143/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2784 - acc: 0.6018 - val_loss: 1.1198 - val_acc: 0.6472\n",
            "Epoch 144/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2459 - acc: 0.6094 - val_loss: 1.1010 - val_acc: 0.6564\n",
            "Epoch 145/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2577 - acc: 0.6038 - val_loss: 1.1896 - val_acc: 0.6263\n",
            "Epoch 146/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2388 - acc: 0.6160 - val_loss: 1.0822 - val_acc: 0.6622\n",
            "Epoch 147/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3849 - acc: 0.5687 - val_loss: 1.2632 - val_acc: 0.5849\n",
            "Epoch 148/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5584 - acc: 0.5073 - val_loss: 2.4128 - val_acc: 0.3621\n",
            "Epoch 149/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4321 - acc: 0.5363 - val_loss: 1.2709 - val_acc: 0.5991\n",
            "Epoch 150/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3269 - acc: 0.5838 - val_loss: 1.1653 - val_acc: 0.6308\n",
            "Epoch 151/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2709 - acc: 0.6035 - val_loss: 1.2023 - val_acc: 0.6155\n",
            "Epoch 152/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2393 - acc: 0.6125 - val_loss: 1.1097 - val_acc: 0.6519\n",
            "Epoch 153/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2823 - acc: 0.5966 - val_loss: 1.2119 - val_acc: 0.6142\n",
            "Epoch 154/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2580 - acc: 0.6085 - val_loss: 1.6465 - val_acc: 0.4504\n",
            "Epoch 155/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2758 - acc: 0.6047 - val_loss: 1.1392 - val_acc: 0.6401\n",
            "Epoch 156/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2282 - acc: 0.6204 - val_loss: 1.0834 - val_acc: 0.6629\n",
            "Epoch 157/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2313 - acc: 0.6137 - val_loss: 1.6253 - val_acc: 0.4742\n",
            "Epoch 158/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2613 - acc: 0.6125 - val_loss: 1.0802 - val_acc: 0.6653\n",
            "Epoch 159/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2274 - acc: 0.6212 - val_loss: 1.1086 - val_acc: 0.6499\n",
            "Epoch 160/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1984 - acc: 0.6318 - val_loss: 1.0761 - val_acc: 0.6624\n",
            "Epoch 161/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1742 - acc: 0.6390 - val_loss: 1.0553 - val_acc: 0.6712\n",
            "Epoch 162/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3071 - acc: 0.5962 - val_loss: 1.3240 - val_acc: 0.5878\n",
            "Epoch 163/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2288 - acc: 0.6235 - val_loss: 1.1320 - val_acc: 0.6489\n",
            "Epoch 164/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2645 - acc: 0.6128 - val_loss: 1.3215 - val_acc: 0.5828\n",
            "Epoch 165/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2627 - acc: 0.6193 - val_loss: 1.0859 - val_acc: 0.6678\n",
            "Epoch 166/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2129 - acc: 0.6315 - val_loss: 1.1556 - val_acc: 0.6407\n",
            "Epoch 167/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1930 - acc: 0.6435 - val_loss: 1.0870 - val_acc: 0.6616\n",
            "Epoch 168/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1590 - acc: 0.6469 - val_loss: 1.0681 - val_acc: 0.6664\n",
            "Epoch 169/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1560 - acc: 0.6499 - val_loss: 1.0659 - val_acc: 0.6677\n",
            "Epoch 170/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1633 - acc: 0.6445 - val_loss: 1.0659 - val_acc: 0.6712\n",
            "Epoch 171/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2230 - acc: 0.6223 - val_loss: 1.9208 - val_acc: 0.3591\n",
            "Epoch 172/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2102 - acc: 0.6275 - val_loss: 1.6355 - val_acc: 0.5318\n",
            "Epoch 173/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2339 - acc: 0.6256 - val_loss: 1.0674 - val_acc: 0.6720\n",
            "Epoch 174/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1570 - acc: 0.6514 - val_loss: 1.0301 - val_acc: 0.6844\n",
            "Epoch 175/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1766 - acc: 0.6391 - val_loss: 1.0788 - val_acc: 0.6561\n",
            "Epoch 176/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1244 - acc: 0.6597 - val_loss: 1.0129 - val_acc: 0.6879\n",
            "Epoch 177/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1107 - acc: 0.6670 - val_loss: 1.0323 - val_acc: 0.6861\n",
            "Epoch 178/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1059 - acc: 0.6672 - val_loss: 1.0407 - val_acc: 0.6798\n",
            "Epoch 179/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1080 - acc: 0.6709 - val_loss: 1.0153 - val_acc: 0.6960\n",
            "Epoch 180/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1034 - acc: 0.6672 - val_loss: 0.9910 - val_acc: 0.6996\n",
            "Epoch 181/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.8726 - acc: 0.4687 - val_loss: 2.1260 - val_acc: 0.2228\n",
            "Epoch 182/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5322 - acc: 0.5180 - val_loss: 1.5516 - val_acc: 0.5136\n",
            "Epoch 183/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3501 - acc: 0.5849 - val_loss: 1.2760 - val_acc: 0.6153\n",
            "Epoch 184/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3208 - acc: 0.5955 - val_loss: 1.2614 - val_acc: 0.6229\n",
            "Epoch 185/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3473 - acc: 0.5887 - val_loss: 3.7885 - val_acc: 0.2636\n",
            "Epoch 186/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4651 - acc: 0.5332 - val_loss: 1.4264 - val_acc: 0.5296\n",
            "Epoch 187/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3520 - acc: 0.5769 - val_loss: 1.2118 - val_acc: 0.6118\n",
            "Epoch 188/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3077 - acc: 0.5922 - val_loss: 1.3004 - val_acc: 0.5861\n",
            "Epoch 189/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3154 - acc: 0.5926 - val_loss: 1.1194 - val_acc: 0.6584\n",
            "Epoch 190/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3276 - acc: 0.5933 - val_loss: 3.7092 - val_acc: 0.1999\n",
            "Epoch 191/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5063 - acc: 0.5197 - val_loss: 1.4782 - val_acc: 0.5166\n",
            "Epoch 192/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3679 - acc: 0.5737 - val_loss: 1.1961 - val_acc: 0.6398\n",
            "Epoch 193/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3186 - acc: 0.5927 - val_loss: 1.1150 - val_acc: 0.6686\n",
            "Epoch 194/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3348 - acc: 0.5800 - val_loss: 1.1560 - val_acc: 0.6559\n",
            "Epoch 195/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2996 - acc: 0.5978 - val_loss: 1.0910 - val_acc: 0.6798\n",
            "Epoch 196/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2606 - acc: 0.6133 - val_loss: 1.0641 - val_acc: 0.6792\n",
            "Epoch 197/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2799 - acc: 0.6070 - val_loss: 1.0679 - val_acc: 0.6796\n",
            "Epoch 198/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2873 - acc: 0.6079 - val_loss: 1.0808 - val_acc: 0.6770\n",
            "Epoch 199/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2388 - acc: 0.6230 - val_loss: 1.0394 - val_acc: 0.6881\n",
            "Epoch 200/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2340 - acc: 0.6283 - val_loss: 1.3843 - val_acc: 0.5489\n",
            "Epoch 201/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2211 - acc: 0.6275 - val_loss: 1.0902 - val_acc: 0.6668\n",
            "Epoch 202/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3452 - acc: 0.5884 - val_loss: 1.7003 - val_acc: 0.4218\n",
            "Epoch 203/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4418 - acc: 0.5391 - val_loss: 1.2624 - val_acc: 0.6061\n",
            "Epoch 204/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3501 - acc: 0.5793 - val_loss: 1.1479 - val_acc: 0.6496\n",
            "Epoch 205/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2933 - acc: 0.5971 - val_loss: 1.1014 - val_acc: 0.6665\n",
            "Epoch 206/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2689 - acc: 0.6110 - val_loss: 1.0735 - val_acc: 0.6728\n",
            "Epoch 207/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2837 - acc: 0.6052 - val_loss: 1.1051 - val_acc: 0.6674\n",
            "Epoch 208/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2638 - acc: 0.6125 - val_loss: 1.0607 - val_acc: 0.6777\n",
            "Epoch 209/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2169 - acc: 0.6260 - val_loss: 1.0340 - val_acc: 0.6859\n",
            "Epoch 210/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1996 - acc: 0.6331 - val_loss: 1.0296 - val_acc: 0.6874\n",
            "Epoch 211/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4595 - acc: 0.5413 - val_loss: 2.5552 - val_acc: 0.2507\n",
            "Epoch 212/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5012 - acc: 0.5090 - val_loss: 1.3971 - val_acc: 0.5381\n",
            "Epoch 213/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3866 - acc: 0.5625 - val_loss: 1.1626 - val_acc: 0.6385\n",
            "Epoch 214/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3216 - acc: 0.5925 - val_loss: 1.1034 - val_acc: 0.6623\n",
            "Epoch 215/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2662 - acc: 0.6049 - val_loss: 1.0714 - val_acc: 0.6697\n",
            "Epoch 216/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2451 - acc: 0.6163 - val_loss: 1.0588 - val_acc: 0.6758\n",
            "Epoch 217/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2244 - acc: 0.6270 - val_loss: 1.0297 - val_acc: 0.6837\n",
            "Epoch 218/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2868 - acc: 0.5996 - val_loss: 1.3302 - val_acc: 0.5887\n",
            "Epoch 219/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2567 - acc: 0.6165 - val_loss: 1.0825 - val_acc: 0.6739\n",
            "Epoch 220/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2202 - acc: 0.6307 - val_loss: 1.0245 - val_acc: 0.6871\n",
            "Epoch 221/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2226 - acc: 0.6270 - val_loss: 1.0383 - val_acc: 0.6836\n",
            "Epoch 222/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3926 - acc: 0.5691 - val_loss: 2.0912 - val_acc: 0.3412\n",
            "Epoch 223/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.4167 - acc: 0.5478 - val_loss: 1.2833 - val_acc: 0.6008\n",
            "Epoch 224/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2987 - acc: 0.5977 - val_loss: 1.0968 - val_acc: 0.6673\n",
            "Epoch 225/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2435 - acc: 0.6161 - val_loss: 1.0637 - val_acc: 0.6787\n",
            "Epoch 226/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2234 - acc: 0.6246 - val_loss: 1.0269 - val_acc: 0.6861\n",
            "Epoch 227/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1967 - acc: 0.6326 - val_loss: 1.0268 - val_acc: 0.6831\n",
            "Epoch 228/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1836 - acc: 0.6419 - val_loss: 0.9989 - val_acc: 0.6914\n",
            "Epoch 229/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2432 - acc: 0.6213 - val_loss: 1.0951 - val_acc: 0.6698\n",
            "Epoch 230/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2302 - acc: 0.6209 - val_loss: 2.0320 - val_acc: 0.3261\n",
            "Epoch 231/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3101 - acc: 0.5889 - val_loss: 1.2049 - val_acc: 0.6281\n",
            "Epoch 232/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2164 - acc: 0.6311 - val_loss: 1.0373 - val_acc: 0.6916\n",
            "Epoch 233/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1897 - acc: 0.6429 - val_loss: 1.0256 - val_acc: 0.6908\n",
            "Epoch 234/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1767 - acc: 0.6453 - val_loss: 0.9987 - val_acc: 0.6973\n",
            "Epoch 235/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1514 - acc: 0.6567 - val_loss: 0.9826 - val_acc: 0.7018\n",
            "Epoch 236/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1520 - acc: 0.6511 - val_loss: 1.0506 - val_acc: 0.6837\n",
            "Epoch 237/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1396 - acc: 0.6519 - val_loss: 0.9884 - val_acc: 0.7021\n",
            "Epoch 238/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1251 - acc: 0.6589 - val_loss: 2.0656 - val_acc: 0.3878\n",
            "Epoch 239/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1833 - acc: 0.6367 - val_loss: 1.0137 - val_acc: 0.6927\n",
            "Epoch 240/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1114 - acc: 0.6626 - val_loss: 0.9751 - val_acc: 0.7012\n",
            "Epoch 241/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5902 - acc: 0.4837 - val_loss: 1.8617 - val_acc: 0.3501\n",
            "Epoch 242/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4267 - acc: 0.5270 - val_loss: 1.2975 - val_acc: 0.5581\n",
            "Epoch 243/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3527 - acc: 0.5674 - val_loss: 1.4488 - val_acc: 0.5377\n",
            "Epoch 244/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2809 - acc: 0.5941 - val_loss: 1.0922 - val_acc: 0.6642\n",
            "Epoch 245/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2357 - acc: 0.6161 - val_loss: 1.0643 - val_acc: 0.6689\n",
            "Epoch 246/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2004 - acc: 0.6261 - val_loss: 1.0347 - val_acc: 0.6792\n",
            "Epoch 247/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1877 - acc: 0.6350 - val_loss: 1.0094 - val_acc: 0.6888\n",
            "Epoch 248/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1585 - acc: 0.6435 - val_loss: 1.0093 - val_acc: 0.6893\n",
            "Epoch 249/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1490 - acc: 0.6453 - val_loss: 0.9912 - val_acc: 0.6972\n",
            "Epoch 250/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1341 - acc: 0.6515 - val_loss: 0.9907 - val_acc: 0.6983\n",
            "Epoch 251/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1138 - acc: 0.6600 - val_loss: 0.9849 - val_acc: 0.6984\n",
            "Epoch 252/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1164 - acc: 0.6605 - val_loss: 1.0497 - val_acc: 0.6839\n",
            "Epoch 253/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1455 - acc: 0.6485 - val_loss: 0.9928 - val_acc: 0.6971\n",
            "Epoch 254/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1119 - acc: 0.6597 - val_loss: 1.1020 - val_acc: 0.6434\n",
            "Epoch 255/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1186 - acc: 0.6558 - val_loss: 0.9673 - val_acc: 0.7026\n",
            "Epoch 256/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2002 - acc: 0.6276 - val_loss: 1.2861 - val_acc: 0.5903\n",
            "Epoch 257/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1727 - acc: 0.6410 - val_loss: 0.9986 - val_acc: 0.6938\n",
            "Epoch 258/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4924 - acc: 0.5390 - val_loss: 1.6290 - val_acc: 0.4979\n",
            "Epoch 259/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2786 - acc: 0.6122 - val_loss: 1.1157 - val_acc: 0.6715\n",
            "Epoch 260/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2081 - acc: 0.6379 - val_loss: 1.0634 - val_acc: 0.6818\n",
            "Epoch 261/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1604 - acc: 0.6532 - val_loss: 1.0997 - val_acc: 0.6553\n",
            "Epoch 262/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1343 - acc: 0.6554 - val_loss: 0.9923 - val_acc: 0.7006\n",
            "Epoch 263/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1313 - acc: 0.6605 - val_loss: 1.0499 - val_acc: 0.6811\n",
            "Epoch 264/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1212 - acc: 0.6600 - val_loss: 0.9814 - val_acc: 0.7031\n",
            "Epoch 265/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0921 - acc: 0.6703 - val_loss: 0.9705 - val_acc: 0.7017\n",
            "Epoch 266/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0908 - acc: 0.6726 - val_loss: 0.9641 - val_acc: 0.7068\n",
            "Epoch 267/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2352 - acc: 0.6205 - val_loss: 1.2185 - val_acc: 0.6123\n",
            "Epoch 268/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1179 - acc: 0.6595 - val_loss: 0.9788 - val_acc: 0.7043\n",
            "Epoch 269/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1013 - acc: 0.6689 - val_loss: 0.9982 - val_acc: 0.7005\n",
            "Epoch 270/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1251 - acc: 0.6615 - val_loss: 1.4831 - val_acc: 0.5926\n",
            "Epoch 271/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2997 - acc: 0.5942 - val_loss: 2.0341 - val_acc: 0.3293\n",
            "Epoch 272/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2566 - acc: 0.6053 - val_loss: 1.1348 - val_acc: 0.6474\n",
            "Epoch 273/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1642 - acc: 0.6446 - val_loss: 1.0294 - val_acc: 0.6902\n",
            "Epoch 274/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2001 - acc: 0.6362 - val_loss: 2.8395 - val_acc: 0.3544\n",
            "Epoch 275/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2736 - acc: 0.6070 - val_loss: 1.1068 - val_acc: 0.6686\n",
            "Epoch 276/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1843 - acc: 0.6455 - val_loss: 1.0319 - val_acc: 0.6889\n",
            "Epoch 277/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1269 - acc: 0.6591 - val_loss: 0.9887 - val_acc: 0.7004\n",
            "Epoch 278/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0950 - acc: 0.6679 - val_loss: 0.9719 - val_acc: 0.7049\n",
            "Epoch 279/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4475 - acc: 0.5486 - val_loss: 1.3882 - val_acc: 0.5459\n",
            "Epoch 280/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2160 - acc: 0.6251 - val_loss: 1.0501 - val_acc: 0.6791\n",
            "Epoch 281/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1530 - acc: 0.6538 - val_loss: 0.9970 - val_acc: 0.6952\n",
            "Epoch 282/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1109 - acc: 0.6655 - val_loss: 0.9729 - val_acc: 0.7055\n",
            "Epoch 283/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0951 - acc: 0.6690 - val_loss: 0.9734 - val_acc: 0.7039\n",
            "Epoch 284/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0705 - acc: 0.6747 - val_loss: 0.9449 - val_acc: 0.7101\n",
            "Epoch 285/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0713 - acc: 0.6789 - val_loss: 0.9597 - val_acc: 0.7057\n",
            "Epoch 286/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0457 - acc: 0.6859 - val_loss: 0.9574 - val_acc: 0.7088\n",
            "Epoch 287/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0641 - acc: 0.6830 - val_loss: 0.9236 - val_acc: 0.7186\n",
            "Epoch 288/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0283 - acc: 0.6944 - val_loss: 0.9111 - val_acc: 0.7227\n",
            "Epoch 289/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0125 - acc: 0.6973 - val_loss: 0.9201 - val_acc: 0.7193\n",
            "Epoch 290/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0051 - acc: 0.6977 - val_loss: 0.8997 - val_acc: 0.7285\n",
            "Epoch 291/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0074 - acc: 0.6990 - val_loss: 0.9967 - val_acc: 0.6997\n",
            "Epoch 292/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0041 - acc: 0.7033 - val_loss: 1.1396 - val_acc: 0.6777\n",
            "Epoch 293/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0644 - acc: 0.6833 - val_loss: 0.9291 - val_acc: 0.7221\n",
            "Epoch 294/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3809 - acc: 0.6079 - val_loss: 2.9729 - val_acc: 0.1700\n",
            "Epoch 295/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.7422 - acc: 0.4260 - val_loss: 1.6626 - val_acc: 0.4485\n",
            "Epoch 296/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.5060 - acc: 0.5130 - val_loss: 1.2838 - val_acc: 0.6018\n",
            "Epoch 297/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.3457 - acc: 0.5806 - val_loss: 1.2056 - val_acc: 0.6184\n",
            "Epoch 298/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2729 - acc: 0.6058 - val_loss: 1.0919 - val_acc: 0.6636\n",
            "Epoch 299/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2041 - acc: 0.6321 - val_loss: 1.0327 - val_acc: 0.6808\n",
            "Epoch 300/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1549 - acc: 0.6489 - val_loss: 0.9975 - val_acc: 0.6951\n",
            "Epoch 301/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1332 - acc: 0.6585 - val_loss: 0.9790 - val_acc: 0.7004\n",
            "Epoch 302/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1385 - acc: 0.6571 - val_loss: 1.0056 - val_acc: 0.6905\n",
            "Epoch 303/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0968 - acc: 0.6746 - val_loss: 0.9397 - val_acc: 0.7152\n",
            "Epoch 304/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0628 - acc: 0.6821 - val_loss: 0.9225 - val_acc: 0.7198\n",
            "Epoch 305/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0909 - acc: 0.6705 - val_loss: 1.5577 - val_acc: 0.5061\n",
            "Epoch 306/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0974 - acc: 0.6682 - val_loss: 0.9535 - val_acc: 0.7104\n",
            "Epoch 307/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1649 - acc: 0.6433 - val_loss: 1.0762 - val_acc: 0.6651\n",
            "Epoch 308/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0536 - acc: 0.6802 - val_loss: 0.9260 - val_acc: 0.7208\n",
            "Epoch 309/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0685 - acc: 0.6781 - val_loss: 0.9546 - val_acc: 0.7137\n",
            "Epoch 310/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0256 - acc: 0.6910 - val_loss: 0.8981 - val_acc: 0.7305\n",
            "Epoch 311/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0076 - acc: 0.7003 - val_loss: 0.8884 - val_acc: 0.7335\n",
            "Epoch 312/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9972 - acc: 0.7032 - val_loss: 0.9036 - val_acc: 0.7260\n",
            "Epoch 313/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2564 - acc: 0.6125 - val_loss: 1.0602 - val_acc: 0.6688\n",
            "Epoch 314/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1473 - acc: 0.6501 - val_loss: 0.9844 - val_acc: 0.6939\n",
            "Epoch 315/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0811 - acc: 0.6708 - val_loss: 0.9415 - val_acc: 0.7131\n",
            "Epoch 316/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0508 - acc: 0.6832 - val_loss: 0.9243 - val_acc: 0.7194\n",
            "Epoch 317/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0362 - acc: 0.6899 - val_loss: 0.9594 - val_acc: 0.7097\n",
            "Epoch 318/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0807 - acc: 0.6736 - val_loss: 0.9118 - val_acc: 0.7277\n",
            "Epoch 319/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0277 - acc: 0.6942 - val_loss: 0.8875 - val_acc: 0.7331\n",
            "Epoch 320/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0080 - acc: 0.6980 - val_loss: 1.2507 - val_acc: 0.6206\n",
            "Epoch 321/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0354 - acc: 0.6908 - val_loss: 0.8957 - val_acc: 0.7336\n",
            "Epoch 322/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0041 - acc: 0.6996 - val_loss: 0.8730 - val_acc: 0.7392\n",
            "Epoch 323/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9787 - acc: 0.7078 - val_loss: 0.8910 - val_acc: 0.7294\n",
            "Epoch 324/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9845 - acc: 0.7048 - val_loss: 0.8844 - val_acc: 0.7341\n",
            "Epoch 325/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9742 - acc: 0.7105 - val_loss: 0.8687 - val_acc: 0.7407\n",
            "Epoch 326/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9569 - acc: 0.7154 - val_loss: 1.0316 - val_acc: 0.6816\n",
            "Epoch 327/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9880 - acc: 0.7034 - val_loss: 0.9267 - val_acc: 0.7184\n",
            "Epoch 328/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9779 - acc: 0.7070 - val_loss: 0.8624 - val_acc: 0.7413\n",
            "Epoch 329/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9641 - acc: 0.7116 - val_loss: 0.8934 - val_acc: 0.7303\n",
            "Epoch 330/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9423 - acc: 0.7181 - val_loss: 0.8496 - val_acc: 0.7483\n",
            "Epoch 331/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0143 - acc: 0.6944 - val_loss: 0.8873 - val_acc: 0.7340\n",
            "Epoch 332/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9536 - acc: 0.7173 - val_loss: 0.8577 - val_acc: 0.7441\n",
            "Epoch 333/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9323 - acc: 0.7212 - val_loss: 0.8395 - val_acc: 0.7483\n",
            "Epoch 334/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9796 - acc: 0.7060 - val_loss: 0.9127 - val_acc: 0.7225\n",
            "Epoch 335/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9387 - acc: 0.7173 - val_loss: 0.8717 - val_acc: 0.7358\n",
            "Epoch 336/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9359 - acc: 0.7214 - val_loss: 0.8499 - val_acc: 0.7445\n",
            "Epoch 337/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9206 - acc: 0.7286 - val_loss: 0.8443 - val_acc: 0.7458\n",
            "Epoch 338/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9116 - acc: 0.7290 - val_loss: 0.8322 - val_acc: 0.7524\n",
            "Epoch 339/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0576 - acc: 0.6825 - val_loss: 1.2701 - val_acc: 0.6047\n",
            "Epoch 340/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0465 - acc: 0.6850 - val_loss: 0.9759 - val_acc: 0.6987\n",
            "Epoch 341/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9734 - acc: 0.7092 - val_loss: 0.8698 - val_acc: 0.7382\n",
            "Epoch 342/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9991 - acc: 0.6995 - val_loss: 0.8748 - val_acc: 0.7329\n",
            "Epoch 343/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9308 - acc: 0.7202 - val_loss: 0.8649 - val_acc: 0.7389\n",
            "Epoch 344/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9285 - acc: 0.7261 - val_loss: 0.8419 - val_acc: 0.7457\n",
            "Epoch 345/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9467 - acc: 0.7210 - val_loss: 0.9807 - val_acc: 0.6932\n",
            "Epoch 346/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9268 - acc: 0.7248 - val_loss: 0.8399 - val_acc: 0.7498\n",
            "Epoch 347/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9023 - acc: 0.7327 - val_loss: 0.8524 - val_acc: 0.7422\n",
            "Epoch 348/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9083 - acc: 0.7278 - val_loss: 0.8305 - val_acc: 0.7489\n",
            "Epoch 349/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9219 - acc: 0.7257 - val_loss: 0.8349 - val_acc: 0.7528\n",
            "Epoch 350/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9051 - acc: 0.7321 - val_loss: 0.8296 - val_acc: 0.7517\n",
            "Epoch 351/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8860 - acc: 0.7355 - val_loss: 0.8166 - val_acc: 0.7550\n",
            "Epoch 352/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8836 - acc: 0.7357 - val_loss: 0.8320 - val_acc: 0.7475\n",
            "Epoch 353/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8745 - acc: 0.7408 - val_loss: 0.8097 - val_acc: 0.7599\n",
            "Epoch 354/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8824 - acc: 0.7366 - val_loss: 0.8188 - val_acc: 0.7566\n",
            "Epoch 355/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8694 - acc: 0.7410 - val_loss: 0.8166 - val_acc: 0.7573\n",
            "Epoch 356/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8801 - acc: 0.7386 - val_loss: 0.8399 - val_acc: 0.7461\n",
            "Epoch 357/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8928 - acc: 0.7344 - val_loss: 0.8142 - val_acc: 0.7566\n",
            "Epoch 358/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8732 - acc: 0.7423 - val_loss: 0.8185 - val_acc: 0.7536\n",
            "Epoch 359/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8660 - acc: 0.7468 - val_loss: 0.8054 - val_acc: 0.7618\n",
            "Epoch 360/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8697 - acc: 0.7408 - val_loss: 0.8458 - val_acc: 0.7484\n",
            "Epoch 361/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8929 - acc: 0.7384 - val_loss: 0.8286 - val_acc: 0.7537\n",
            "Epoch 362/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9627 - acc: 0.7131 - val_loss: 1.0835 - val_acc: 0.6614\n",
            "Epoch 363/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9250 - acc: 0.7257 - val_loss: 0.8533 - val_acc: 0.7470\n",
            "Epoch 364/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8817 - acc: 0.7405 - val_loss: 0.9084 - val_acc: 0.7301\n",
            "Epoch 365/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9863 - acc: 0.7069 - val_loss: 1.2930 - val_acc: 0.5977\n",
            "Epoch 366/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9289 - acc: 0.7244 - val_loss: 0.8283 - val_acc: 0.7497\n",
            "Epoch 367/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8950 - acc: 0.7333 - val_loss: 1.6588 - val_acc: 0.4994\n",
            "Epoch 368/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9707 - acc: 0.7083 - val_loss: 0.9580 - val_acc: 0.7184\n",
            "Epoch 369/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8831 - acc: 0.7429 - val_loss: 1.1396 - val_acc: 0.6388\n",
            "Epoch 370/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8781 - acc: 0.7396 - val_loss: 0.7933 - val_acc: 0.7649\n",
            "Epoch 371/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8488 - acc: 0.7488 - val_loss: 0.7954 - val_acc: 0.7669\n",
            "Epoch 372/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8542 - acc: 0.7507 - val_loss: 0.8304 - val_acc: 0.7495\n",
            "Epoch 373/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8473 - acc: 0.7509 - val_loss: 1.1422 - val_acc: 0.6574\n",
            "Epoch 374/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9670 - acc: 0.7116 - val_loss: 0.8423 - val_acc: 0.7483\n",
            "Epoch 375/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9252 - acc: 0.7239 - val_loss: 0.8229 - val_acc: 0.7532\n",
            "Epoch 376/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0115 - acc: 0.6962 - val_loss: 0.9986 - val_acc: 0.6934\n",
            "Epoch 377/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9671 - acc: 0.7059 - val_loss: 1.0287 - val_acc: 0.6891\n",
            "Epoch 378/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8903 - acc: 0.7329 - val_loss: 0.8059 - val_acc: 0.7586\n",
            "Epoch 379/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8622 - acc: 0.7463 - val_loss: 1.1623 - val_acc: 0.6447\n",
            "Epoch 380/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8769 - acc: 0.7375 - val_loss: 2.3226 - val_acc: 0.4208\n",
            "Epoch 381/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2819 - acc: 0.6297 - val_loss: 2.2125 - val_acc: 0.2121\n",
            "Epoch 382/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4078 - acc: 0.5449 - val_loss: 1.3551 - val_acc: 0.5519\n",
            "Epoch 383/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1191 - acc: 0.6543 - val_loss: 0.9729 - val_acc: 0.6953\n",
            "Epoch 384/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0077 - acc: 0.6973 - val_loss: 0.9255 - val_acc: 0.7143\n",
            "Epoch 385/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0525 - acc: 0.6807 - val_loss: 1.0277 - val_acc: 0.6829\n",
            "Epoch 386/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9689 - acc: 0.7089 - val_loss: 0.9177 - val_acc: 0.7189\n",
            "Epoch 387/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9288 - acc: 0.7221 - val_loss: 0.8484 - val_acc: 0.7446\n",
            "Epoch 388/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1503 - acc: 0.6428 - val_loss: 1.7587 - val_acc: 0.4202\n",
            "Epoch 389/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.1179 - acc: 0.6542 - val_loss: 0.9627 - val_acc: 0.7052\n",
            "Epoch 390/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0217 - acc: 0.6956 - val_loss: 0.8739 - val_acc: 0.7382\n",
            "Epoch 391/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9743 - acc: 0.7113 - val_loss: 0.9033 - val_acc: 0.7314\n",
            "Epoch 392/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9399 - acc: 0.7178 - val_loss: 0.8238 - val_acc: 0.7558\n",
            "Epoch 393/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9260 - acc: 0.7273 - val_loss: 0.8884 - val_acc: 0.7371\n",
            "Epoch 394/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9299 - acc: 0.7207 - val_loss: 1.1180 - val_acc: 0.6582\n",
            "Epoch 395/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9633 - acc: 0.7143 - val_loss: 0.8372 - val_acc: 0.7502\n",
            "Epoch 396/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9092 - acc: 0.7298 - val_loss: 0.8273 - val_acc: 0.7535\n",
            "Epoch 397/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9322 - acc: 0.7181 - val_loss: 1.6416 - val_acc: 0.5029\n",
            "Epoch 398/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9377 - acc: 0.7184 - val_loss: 0.8104 - val_acc: 0.7603\n",
            "Epoch 399/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9451 - acc: 0.7192 - val_loss: 0.9554 - val_acc: 0.7043\n",
            "Epoch 400/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9247 - acc: 0.7249 - val_loss: 0.8549 - val_acc: 0.7447\n",
            "Epoch 401/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8984 - acc: 0.7308 - val_loss: 0.7957 - val_acc: 0.7636\n",
            "Epoch 402/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8766 - acc: 0.7405 - val_loss: 0.7759 - val_acc: 0.7674\n",
            "Epoch 403/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8660 - acc: 0.7447 - val_loss: 0.7860 - val_acc: 0.7647\n",
            "Epoch 404/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8990 - acc: 0.7339 - val_loss: 0.8209 - val_acc: 0.7577\n",
            "Epoch 405/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8727 - acc: 0.7413 - val_loss: 0.7734 - val_acc: 0.7698\n",
            "Epoch 406/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8669 - acc: 0.7433 - val_loss: 0.8042 - val_acc: 0.7567\n",
            "Epoch 407/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9007 - acc: 0.7295 - val_loss: 0.8141 - val_acc: 0.7534\n",
            "Epoch 408/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8479 - acc: 0.7474 - val_loss: 0.7747 - val_acc: 0.7678\n",
            "Epoch 409/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8538 - acc: 0.7465 - val_loss: 0.8265 - val_acc: 0.7529\n",
            "Epoch 410/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8472 - acc: 0.7498 - val_loss: 0.7694 - val_acc: 0.7675\n",
            "Epoch 411/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8240 - acc: 0.7550 - val_loss: 0.7758 - val_acc: 0.7704\n",
            "Epoch 412/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8215 - acc: 0.7571 - val_loss: 0.7760 - val_acc: 0.7643\n",
            "Epoch 413/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8528 - acc: 0.7459 - val_loss: 0.7978 - val_acc: 0.7622\n",
            "Epoch 414/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8262 - acc: 0.7550 - val_loss: 0.7753 - val_acc: 0.7727\n",
            "Epoch 415/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8601 - acc: 0.7476 - val_loss: 0.7679 - val_acc: 0.7738\n",
            "Epoch 416/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8248 - acc: 0.7552 - val_loss: 0.7633 - val_acc: 0.7720\n",
            "Epoch 417/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8271 - acc: 0.7517 - val_loss: 0.8383 - val_acc: 0.7451\n",
            "Epoch 418/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8344 - acc: 0.7540 - val_loss: 0.7896 - val_acc: 0.7632\n",
            "Epoch 419/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8041 - acc: 0.7583 - val_loss: 0.7624 - val_acc: 0.7764\n",
            "Epoch 420/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8352 - acc: 0.7519 - val_loss: 0.7543 - val_acc: 0.7772\n",
            "Epoch 421/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8043 - acc: 0.7594 - val_loss: 0.8031 - val_acc: 0.7608\n",
            "Epoch 422/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7991 - acc: 0.7626 - val_loss: 0.7547 - val_acc: 0.7789\n",
            "Epoch 423/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7964 - acc: 0.7665 - val_loss: 0.8216 - val_acc: 0.7559\n",
            "Epoch 424/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7925 - acc: 0.7666 - val_loss: 0.7593 - val_acc: 0.7783\n",
            "Epoch 425/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7872 - acc: 0.7680 - val_loss: 0.7540 - val_acc: 0.7823\n",
            "Epoch 426/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7787 - acc: 0.7705 - val_loss: 0.7458 - val_acc: 0.7824\n",
            "Epoch 427/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7802 - acc: 0.7685 - val_loss: 0.7414 - val_acc: 0.7816\n",
            "Epoch 428/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8020 - acc: 0.7650 - val_loss: 1.1152 - val_acc: 0.6733\n",
            "Epoch 429/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8029 - acc: 0.7603 - val_loss: 1.9502 - val_acc: 0.5046\n",
            "Epoch 430/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8038 - acc: 0.7587 - val_loss: 0.7554 - val_acc: 0.7734\n",
            "Epoch 431/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7938 - acc: 0.7647 - val_loss: 0.7420 - val_acc: 0.7839\n",
            "Epoch 432/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7792 - acc: 0.7700 - val_loss: 0.7663 - val_acc: 0.7724\n",
            "Epoch 433/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7775 - acc: 0.7718 - val_loss: 1.1512 - val_acc: 0.6529\n",
            "Epoch 434/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8201 - acc: 0.7560 - val_loss: 0.7513 - val_acc: 0.7793\n",
            "Epoch 435/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7793 - acc: 0.7688 - val_loss: 0.7636 - val_acc: 0.7757\n",
            "Epoch 436/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7732 - acc: 0.7708 - val_loss: 0.7392 - val_acc: 0.7828\n",
            "Epoch 437/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7770 - acc: 0.7683 - val_loss: 0.7806 - val_acc: 0.7675\n",
            "Epoch 438/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7747 - acc: 0.7705 - val_loss: 0.7375 - val_acc: 0.7849\n",
            "Epoch 439/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7922 - acc: 0.7654 - val_loss: 0.9366 - val_acc: 0.7188\n",
            "Epoch 440/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7698 - acc: 0.7722 - val_loss: 0.7329 - val_acc: 0.7852\n",
            "Epoch 441/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8246 - acc: 0.7571 - val_loss: 0.7228 - val_acc: 0.7863\n",
            "Epoch 442/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7692 - acc: 0.7747 - val_loss: 0.7294 - val_acc: 0.7886\n",
            "Epoch 443/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7930 - acc: 0.7623 - val_loss: 0.7453 - val_acc: 0.7799\n",
            "Epoch 444/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8879 - acc: 0.7401 - val_loss: 0.8362 - val_acc: 0.7482\n",
            "Epoch 445/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0019 - acc: 0.6968 - val_loss: 0.8333 - val_acc: 0.7446\n",
            "Epoch 446/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8379 - acc: 0.7480 - val_loss: 0.7804 - val_acc: 0.7651\n",
            "Epoch 447/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8059 - acc: 0.7603 - val_loss: 0.7503 - val_acc: 0.7761\n",
            "Epoch 448/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8310 - acc: 0.7537 - val_loss: 0.9435 - val_acc: 0.7294\n",
            "Epoch 449/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7940 - acc: 0.7658 - val_loss: 0.7287 - val_acc: 0.7859\n",
            "Epoch 450/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7721 - acc: 0.7711 - val_loss: 0.7307 - val_acc: 0.7874\n",
            "Epoch 451/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7716 - acc: 0.7736 - val_loss: 0.7389 - val_acc: 0.7782\n",
            "Epoch 452/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7514 - acc: 0.7733 - val_loss: 0.7245 - val_acc: 0.7881\n",
            "Epoch 453/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9208 - acc: 0.7307 - val_loss: 1.9052 - val_acc: 0.4227\n",
            "Epoch 454/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8745 - acc: 0.7353 - val_loss: 0.7925 - val_acc: 0.7641\n",
            "Epoch 455/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7880 - acc: 0.7674 - val_loss: 0.7957 - val_acc: 0.7663\n",
            "Epoch 456/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7735 - acc: 0.7705 - val_loss: 0.7523 - val_acc: 0.7788\n",
            "Epoch 457/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7710 - acc: 0.7733 - val_loss: 0.7289 - val_acc: 0.7868\n",
            "Epoch 458/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7781 - acc: 0.7673 - val_loss: 0.7768 - val_acc: 0.7741\n",
            "Epoch 459/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7674 - acc: 0.7748 - val_loss: 0.7216 - val_acc: 0.7893\n",
            "Epoch 460/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8550 - acc: 0.7469 - val_loss: 0.7541 - val_acc: 0.7755\n",
            "Epoch 461/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7797 - acc: 0.7688 - val_loss: 2.2472 - val_acc: 0.4483\n",
            "Epoch 462/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0858 - acc: 0.6784 - val_loss: 0.8903 - val_acc: 0.7383\n",
            "Epoch 463/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8158 - acc: 0.7613 - val_loss: 0.9014 - val_acc: 0.7387\n",
            "Epoch 464/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7941 - acc: 0.7681 - val_loss: 0.7498 - val_acc: 0.7850\n",
            "Epoch 465/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7694 - acc: 0.7756 - val_loss: 0.7168 - val_acc: 0.7923\n",
            "Epoch 466/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7664 - acc: 0.7740 - val_loss: 0.7474 - val_acc: 0.7802\n",
            "Epoch 467/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7483 - acc: 0.7796 - val_loss: 0.7877 - val_acc: 0.7651\n",
            "Epoch 468/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7448 - acc: 0.7816 - val_loss: 0.7063 - val_acc: 0.7927\n",
            "Epoch 469/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7961 - acc: 0.7675 - val_loss: 0.8063 - val_acc: 0.7603\n",
            "Epoch 470/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8335 - acc: 0.7505 - val_loss: 1.0144 - val_acc: 0.6772\n",
            "Epoch 471/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8253 - acc: 0.7510 - val_loss: 0.7588 - val_acc: 0.7731\n",
            "Epoch 472/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7792 - acc: 0.7710 - val_loss: 0.7313 - val_acc: 0.7855\n",
            "Epoch 473/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7659 - acc: 0.7742 - val_loss: 0.7265 - val_acc: 0.7886\n",
            "Epoch 474/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7597 - acc: 0.7755 - val_loss: 0.7151 - val_acc: 0.7924\n",
            "Epoch 475/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7491 - acc: 0.7802 - val_loss: 0.7047 - val_acc: 0.7929\n",
            "Epoch 476/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7546 - acc: 0.7745 - val_loss: 0.7735 - val_acc: 0.7667\n",
            "Epoch 477/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.2446 - acc: 0.6320 - val_loss: 1.5320 - val_acc: 0.5013\n",
            "Epoch 478/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 1.0996 - acc: 0.6618 - val_loss: 0.8967 - val_acc: 0.7286\n",
            "Epoch 479/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.9290 - acc: 0.7213 - val_loss: 0.8095 - val_acc: 0.7564\n",
            "Epoch 480/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8781 - acc: 0.7400 - val_loss: 0.7801 - val_acc: 0.7663\n",
            "Epoch 481/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8426 - acc: 0.7511 - val_loss: 0.7599 - val_acc: 0.7738\n",
            "Epoch 482/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8426 - acc: 0.7512 - val_loss: 0.7390 - val_acc: 0.7830\n",
            "Epoch 483/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8049 - acc: 0.7591 - val_loss: 0.7569 - val_acc: 0.7755\n",
            "Epoch 484/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7961 - acc: 0.7648 - val_loss: 0.7287 - val_acc: 0.7866\n",
            "Epoch 485/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7865 - acc: 0.7670 - val_loss: 0.7185 - val_acc: 0.7883\n",
            "Epoch 486/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8005 - acc: 0.7614 - val_loss: 0.7229 - val_acc: 0.7866\n",
            "Epoch 487/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7773 - acc: 0.7697 - val_loss: 0.7148 - val_acc: 0.7906\n",
            "Epoch 488/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7823 - acc: 0.7693 - val_loss: 0.8064 - val_acc: 0.7563\n",
            "Epoch 489/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7839 - acc: 0.7660 - val_loss: 0.7274 - val_acc: 0.7827\n",
            "Epoch 490/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7589 - acc: 0.7716 - val_loss: 0.7125 - val_acc: 0.7904\n",
            "Epoch 491/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7513 - acc: 0.7751 - val_loss: 0.7116 - val_acc: 0.7942\n",
            "Epoch 492/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7552 - acc: 0.7792 - val_loss: 0.6972 - val_acc: 0.7957\n",
            "Epoch 493/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7438 - acc: 0.7792 - val_loss: 0.6978 - val_acc: 0.7980\n",
            "Epoch 494/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7318 - acc: 0.7835 - val_loss: 0.6923 - val_acc: 0.7977\n",
            "Epoch 495/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7740 - acc: 0.7688 - val_loss: 0.8191 - val_acc: 0.7490\n",
            "Epoch 496/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7842 - acc: 0.7670 - val_loss: 0.7592 - val_acc: 0.7737\n",
            "Epoch 497/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7450 - acc: 0.7783 - val_loss: 0.6995 - val_acc: 0.7982\n",
            "Epoch 498/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7660 - acc: 0.7720 - val_loss: 0.7600 - val_acc: 0.7776\n",
            "Epoch 499/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7441 - acc: 0.7777 - val_loss: 0.6939 - val_acc: 0.7968\n",
            "Epoch 500/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.7245 - acc: 0.7846 - val_loss: 0.6934 - val_acc: 0.8005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e2056a4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_kacYrMAKxh",
        "colab_type": "text"
      },
      "source": [
        "# Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions) (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fgr8YBVAK4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IpfZhnAXwc",
        "colab_type": "text"
      },
      "source": [
        "# Implement batch normalization for training the neural network (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxo2EgG9ALFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zEsE_KeAehc",
        "colab_type": "text"
      },
      "source": [
        "# Understand the differences and trade-offs between traditional and NN classifiers with the help of classification metrics (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiBiFHMgAgUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}