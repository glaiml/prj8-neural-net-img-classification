{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glaiml/prj8-neural-net-img-classification/blob/master/vgk-neuner-img-classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMhsX4iHBJvt",
        "colab_type": "text"
      },
      "source": [
        "Vinayak G Kudva Project submission for GLAIML \n",
        "https://github.com/glaiml/prj8-neural-net-img-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiQaKY_-gmj",
        "colab_type": "text"
      },
      "source": [
        "# The Real Problem\n",
        "\n",
        "Recognizing multi-digit numbers in photographs captured at street level is an important\n",
        "component of modern-day map making. A classic example of a corpus of such street\n",
        "level photographs is Google’s Street View imagery comprised of hundreds of millions of\n",
        "geo-located 360 degree panoramic images. The ability to automatically transcribe an\n",
        "address number from a geo-located patch of pixels and associate the transcribed\n",
        "number with a known street address helps pinpoint, with a high degree of accuracy, the\n",
        "location of the building it represents.\n",
        "More broadly, recognizing numbers in photographs is a problem of interest to the optical\n",
        "character recognition community. While OCR on constrained domains like document\n",
        "processing is well studied, arbitrary multi-character text recognition in photographs is\n",
        "still highly challenging. This difficulty arises due to the wide variability in the visual\n",
        "appearance of text in the wild on account of a large range of fonts, colors, styles,\n",
        "orientations, and character arrangements. The recognition problem is further\n",
        "complicated by environmental factors such as lighting, shadows, specularities, and\n",
        "occlusions as well as by image acquisition factors such as resolution, motion, and focus\n",
        "blurs.\n",
        "In this project we will use dataset with images centred around a single digit (many of the\n",
        "images do contain some distractors at the sides). Although we are taking a sample of\n",
        "the data which is simpler, it is more complex than MNIST because of the distractors.\n",
        "\n",
        "# Project Description\n",
        "In this hands-on project the goal is to build a python code for image classification from\n",
        "scratch to understand the nitty gritties of building and training a model and further to\n",
        "understand the advantages of neural networks. First we will implement a simple KNN\n",
        "classifier and later implement a Neural Network to classify the images in the SVHN\n",
        "dataset. We will compare the computational efficiency and accuracy between the\n",
        "traditional methods and neural networks.\n",
        "\n",
        "# The Street View House Numbers (SVHN) Dataset\n",
        "SVHN is a real-world image dataset for developing machine learning and object\n",
        "recognition algorithms with minimal requirement on data formatting but comes from a\n",
        "significantly harder, unsolved, real world problem (recognizing digits and numbers in\n",
        "natural scene images). SVHN is obtained from house numbers in Google Street View\n",
        "images.\n",
        "\n",
        "# Overview\n",
        "The images come in two formats as shown below.\n",
        "Format 1 : Original images with character level bounding boxes.\n",
        "Format 2 : MNIST-like 32-by-32 images centered around a single character (many\n",
        "of the images do contain some distractors at the sides).\n",
        "\n",
        "The goal of this project is to take an image from the SVHN dataset and determine what that digit is.\n",
        "This is a multi-class classification problem with 10 classes, one for each digit 0-9. Digit '1' has label 1,\n",
        "'9' has label 9 and '0' has label 10.\n",
        "Although, there are close to 6,00,000 images in this dataset, we have extracted 60,000 images\n",
        "(42000 training and 18000 test images) to do this project. The data comes in a MNIST-like format of\n",
        "32-by-32 RGB images centred around a single digit (many of the images do contain some distractors\n",
        "at the sides).\n",
        "\n",
        "# Reference\n",
        "Acknowledgement for the datasets.\n",
        "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng\n",
        "Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop\n",
        "on Deep Learning and Unsupervised Feature Learning 2011. (https://research.google/pubs/pub37648/)\n",
        "\n",
        "http://ufldl.stanford.edu/housenumbers as the URL for this site when necessary\n",
        "\n",
        "# Downloads\n",
        "\n",
        "Refer to Olympus for project related files and instructions.\n",
        "Data Set:\n",
        "● The name of the dataset is SVHN_single_grey1.h5\n",
        "● The data is a subset of the original dataset. Use this subset only for the\n",
        "project.\n",
        "● Keep a copy of your dataset in your own google drive.\n",
        "\n",
        "# Project Objectives\n",
        "The objective of the project is to learn how to implement a simple image classification\n",
        "pipeline based on the k-Nearest Neighbour and a deep neural network.\n",
        "\n",
        "Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjKg-g6_z9nS",
        "colab_type": "code",
        "outputId": "1fc4b735-c4c9-4e65-fa75-076fb373a668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcGnIhj9AAX2",
        "colab_type": "text"
      },
      "source": [
        "# Data fetching and understand the train/val/test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIPmFcLt65co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Open the file as readonly\n",
        "h5f = h5py.File('/content/drive/My Drive/Colab Notebooks/NuNetProject/SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training, test and validation set\n",
        "X_train = h5f['X_train'][:]\n",
        "y_train = h5f['y_train'][:]\n",
        "X_test = h5f['X_test'][:]\n",
        "y_test = h5f['y_test'][:]\n",
        "\n",
        "\n",
        "# Close this file\n",
        "h5f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1vwYC23Au-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9ca44726-fc21-4454-b9d5-1db3c5862ff1"
      },
      "source": [
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "print (X_test.shape)\n",
        "print (y_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 32, 32)\n",
            "(42000,)\n",
            "(18000, 32, 32)\n",
            "(18000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DJLL0YAE91C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "00560302-c07a-4461-cc81-2ac7580a8e89"
      },
      "source": [
        "# Flatten X Train and X Test\n",
        "X_train_flat = X_train.reshape(42000, 1024)\n",
        "print (X_train_flat.shape)\n",
        "\n",
        "X_test_flat = X_test.reshape(18000, 1024)\n",
        "print (X_test_flat.shape)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 1024)\n",
            "(18000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSQ2SI7ADvM",
        "colab_type": "text"
      },
      "source": [
        "# Implement and apply an optimal k-Nearest Neighbor (kNN) classifier (7.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-yLXPt87ICY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "335c914f-d29c-4c5b-b9e8-ad69fc582dae"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as pl\n",
        "Nc = range(20, 40)\n",
        "knnElbow = [KNeighborsClassifier(n_neighbors=i, weights='distance', metric = 'cosine', n_jobs = 50) for i in Nc]\n",
        "knnElbow\n",
        "score = [knnElbow[j].fit(X_train_flat, y_train).score(X_test_flat, y_test) for j in range(len(knnElbow))]\n",
        "print(score)\n",
        "pl.plot(Nc,score)\n",
        "pl.xlabel('Number of Neighbors')\n",
        "pl.ylabel('Score')\n",
        "pl.title('Elbow Curve')\n",
        "pl.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6438333333333334, 0.6447222222222222, 0.6450555555555556, 0.6444444444444445, 0.6449444444444444, 0.6467777777777778, 0.6475555555555556, 0.6478888888888888, 0.6468888888888888, 0.6478333333333334, 0.6474444444444445, 0.6479444444444444, 0.6471111111111111, 0.647, 0.6461666666666667, 0.6465, 0.6461111111111111, 0.6450555555555556, 0.6441111111111111, 0.6438888888888888]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcaYiPx/x1XzxIFK8JQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3juKxraDxp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KNC = KNeighborsClassifier(n_neighbors= 31, weights='distance', metric = 'cosine', n_jobs = 50)\n",
        "# Call Nearest Neighbour algorithm\n",
        "\n",
        "KNC.fit(X_train_flat, y_train)\n",
        "pred_train = KNC.predict(X_train_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SejeHwfibmY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef5f4d2d-950e-4208-f351-8f1fddd18813"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "mat_train = confusion_matrix(y_train,pred_train)\n",
        "print (\"Training Accuracy ::\")\n",
        "KNC.score(X_train_flat, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy ::\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01N4Yeu9CSRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_y = KNC.predict(X_test_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Al0AdTwCTPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "10f5aa5f-b959-4017-abc5-7be907ae2b94"
      },
      "source": [
        "print (\"Test Accuracy ::\")\n",
        "KNC.score(X_test_flat, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy ::\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6479444444444444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8BTKfTUAJaA",
        "colab_type": "text"
      },
      "source": [
        "# Print the classification metric report (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDd6MEoFCekO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a86a25b5-507b-49a5-cbcb-8e223b87f277"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"KNN Metrics = \\n\", metrics.classification_report(y_test, pred_y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN Metrics = \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.76      0.65      1814\n",
            "           1       0.56      0.79      0.66      1828\n",
            "           2       0.78      0.62      0.70      1803\n",
            "           3       0.60      0.55      0.57      1719\n",
            "           4       0.73      0.77      0.75      1812\n",
            "           5       0.71      0.51      0.59      1768\n",
            "           6       0.64      0.56      0.60      1832\n",
            "           7       0.73      0.74      0.74      1808\n",
            "           8       0.59      0.55      0.57      1812\n",
            "           9       0.66      0.61      0.63      1804\n",
            "\n",
            "    accuracy                           0.65     18000\n",
            "   macro avg       0.66      0.65      0.65     18000\n",
            "weighted avg       0.66      0.65      0.65     18000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFKvuLHuAKjR",
        "colab_type": "text"
      },
      "source": [
        "# Implement and apply a deep neural network classifier including (feedforward neural network, RELU activations) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7_MslqeAJx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "292867f1-e049-448a-dee5-d5329316d750"
      },
      "source": [
        "# visualizing the first 10 images in the dataset and their labels\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 1))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(X_train[i], cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "print('label for each of the above image: %s' % (y_train[0:10]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAA9CAYAAACpzLMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhqM4BwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuRUBBxt5zXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "trainY = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9S8keQw6V9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "aa7d9587-2e15-4f76-ac73-77d1dd4cfd68"
      },
      "source": [
        "trainY[1:10]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWKs84zHAKqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "262a8af8-72b1-4da3-da74-12bd6fd6d3e4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape = (1024,), activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = 'relu'))\n",
        "sgd = optimizers.Adam(lr = 0.3)\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_102 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 4,212,746\n",
            "Trainable params: 4,210,698\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxQLjdel4AHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8af0b1bb-c0c8-4506-d145-fc9e37f25b51"
      },
      "source": [
        "model.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 2.4064 - acc: 0.0974 - val_loss: 2.3027 - val_acc: 0.0991\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.3045 - acc: 0.1022 - val_loss: 2.3041 - val_acc: 0.1008\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 2.2409 - acc: 0.1440 - val_loss: 2.3360 - val_acc: 0.1300\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.2248 - acc: 0.1402 - val_loss: 2.3206 - val_acc: 0.1007\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.3044 - acc: 0.1022 - val_loss: 2.2952 - val_acc: 0.1045\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.2830 - acc: 0.1151 - val_loss: 2.2546 - val_acc: 0.1505\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.2685 - acc: 0.1290 - val_loss: 2.2997 - val_acc: 0.1144\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.2262 - acc: 0.1419 - val_loss: 2.1831 - val_acc: 0.1696\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1287 - acc: 0.1772 - val_loss: 2.1319 - val_acc: 0.1826\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0790 - acc: 0.1895 - val_loss: 2.1431 - val_acc: 0.1796\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0837 - acc: 0.1895 - val_loss: 2.1884 - val_acc: 0.1786\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1036 - acc: 0.1729 - val_loss: 2.0735 - val_acc: 0.1908\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0732 - acc: 0.1838 - val_loss: 2.2897 - val_acc: 0.1538\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0330 - acc: 0.1994 - val_loss: 1.9977 - val_acc: 0.2019\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0305 - acc: 0.2023 - val_loss: 2.2425 - val_acc: 0.1572\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0463 - acc: 0.1999 - val_loss: 2.0905 - val_acc: 0.1926\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1512 - acc: 0.1574 - val_loss: 2.2948 - val_acc: 0.1090\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1834 - acc: 0.1545 - val_loss: 2.1131 - val_acc: 0.1873\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0679 - acc: 0.1905 - val_loss: 2.0356 - val_acc: 0.1996\n",
            "Epoch 20/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0239 - acc: 0.2025 - val_loss: 1.9876 - val_acc: 0.2076\n",
            "Epoch 21/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0125 - acc: 0.2080 - val_loss: 1.9499 - val_acc: 0.2174\n",
            "Epoch 22/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 2.0234 - acc: 0.2078 - val_loss: 1.9444 - val_acc: 0.2239\n",
            "Epoch 23/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0001 - acc: 0.2125 - val_loss: 1.9513 - val_acc: 0.2198\n",
            "Epoch 24/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.9855 - acc: 0.2129 - val_loss: 1.9368 - val_acc: 0.2210\n",
            "Epoch 25/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9777 - acc: 0.2193 - val_loss: 1.9485 - val_acc: 0.2282\n",
            "Epoch 26/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9976 - acc: 0.2163 - val_loss: 1.9846 - val_acc: 0.2209\n",
            "Epoch 27/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9985 - acc: 0.2135 - val_loss: 2.0411 - val_acc: 0.2098\n",
            "Epoch 28/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9901 - acc: 0.2170 - val_loss: 1.9373 - val_acc: 0.2258\n",
            "Epoch 29/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0733 - acc: 0.1957 - val_loss: 2.4239 - val_acc: 0.1378\n",
            "Epoch 30/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0682 - acc: 0.1930 - val_loss: 2.0963 - val_acc: 0.1754\n",
            "Epoch 31/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 2.0339 - acc: 0.2043 - val_loss: 1.9966 - val_acc: 0.2148\n",
            "Epoch 32/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9879 - acc: 0.2161 - val_loss: 1.9852 - val_acc: 0.2128\n",
            "Epoch 33/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9797 - acc: 0.2214 - val_loss: 1.9532 - val_acc: 0.2278\n",
            "Epoch 34/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9665 - acc: 0.2263 - val_loss: 2.0804 - val_acc: 0.1968\n",
            "Epoch 35/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9728 - acc: 0.2268 - val_loss: 1.9165 - val_acc: 0.2399\n",
            "Epoch 36/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9591 - acc: 0.2332 - val_loss: 1.9088 - val_acc: 0.2419\n",
            "Epoch 37/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9476 - acc: 0.2380 - val_loss: 1.9753 - val_acc: 0.2171\n",
            "Epoch 38/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9497 - acc: 0.2423 - val_loss: 2.2965 - val_acc: 0.1352\n",
            "Epoch 39/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1738 - acc: 0.1500 - val_loss: 2.1904 - val_acc: 0.1746\n",
            "Epoch 40/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0562 - acc: 0.1994 - val_loss: 1.9553 - val_acc: 0.2321\n",
            "Epoch 41/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9833 - acc: 0.2230 - val_loss: 1.9548 - val_acc: 0.2274\n",
            "Epoch 42/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9584 - acc: 0.2330 - val_loss: 1.9220 - val_acc: 0.2496\n",
            "Epoch 43/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9416 - acc: 0.2427 - val_loss: 1.8883 - val_acc: 0.2537\n",
            "Epoch 44/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.9230 - acc: 0.2515 - val_loss: 1.8877 - val_acc: 0.2652\n",
            "Epoch 45/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9217 - acc: 0.2603 - val_loss: 1.8635 - val_acc: 0.2766\n",
            "Epoch 46/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9161 - acc: 0.2637 - val_loss: 2.1591 - val_acc: 0.1834\n",
            "Epoch 47/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0020 - acc: 0.2290 - val_loss: 2.4140 - val_acc: 0.1201\n",
            "Epoch 48/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9935 - acc: 0.2318 - val_loss: 2.0327 - val_acc: 0.2483\n",
            "Epoch 49/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9167 - acc: 0.2632 - val_loss: 1.8493 - val_acc: 0.2825\n",
            "Epoch 50/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8767 - acc: 0.2788 - val_loss: 1.7879 - val_acc: 0.3095\n",
            "Epoch 51/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8593 - acc: 0.2870 - val_loss: 1.8344 - val_acc: 0.2873\n",
            "Epoch 52/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8762 - acc: 0.2816 - val_loss: 2.2025 - val_acc: 0.1704\n",
            "Epoch 53/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8393 - acc: 0.2969 - val_loss: 1.7420 - val_acc: 0.3232\n",
            "Epoch 54/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8611 - acc: 0.2896 - val_loss: 1.8973 - val_acc: 0.2729\n",
            "Epoch 55/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8066 - acc: 0.3130 - val_loss: 1.7314 - val_acc: 0.3353\n",
            "Epoch 56/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7761 - acc: 0.3227 - val_loss: 2.2906 - val_acc: 0.2359\n",
            "Epoch 57/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7888 - acc: 0.3237 - val_loss: 1.7852 - val_acc: 0.3307\n",
            "Epoch 58/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7917 - acc: 0.3217 - val_loss: 2.5884 - val_acc: 0.1643\n",
            "Epoch 59/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8083 - acc: 0.3202 - val_loss: 1.8864 - val_acc: 0.2858\n",
            "Epoch 60/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8747 - acc: 0.3009 - val_loss: 3.0930 - val_acc: 0.1167\n",
            "Epoch 61/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8077 - acc: 0.3179 - val_loss: 1.7312 - val_acc: 0.3436\n",
            "Epoch 62/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7402 - acc: 0.3473 - val_loss: 1.6822 - val_acc: 0.3714\n",
            "Epoch 63/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9553 - acc: 0.2637 - val_loss: 2.1989 - val_acc: 0.1732\n",
            "Epoch 64/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0016 - acc: 0.2436 - val_loss: 1.8605 - val_acc: 0.3122\n",
            "Epoch 65/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8678 - acc: 0.2897 - val_loss: 1.7509 - val_acc: 0.3407\n",
            "Epoch 66/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8189 - acc: 0.3091 - val_loss: 1.7596 - val_acc: 0.3221\n",
            "Epoch 67/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8084 - acc: 0.3153 - val_loss: 1.7088 - val_acc: 0.3440\n",
            "Epoch 68/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7500 - acc: 0.3401 - val_loss: 1.6683 - val_acc: 0.3690\n",
            "Epoch 69/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7332 - acc: 0.3540 - val_loss: 1.6554 - val_acc: 0.4027\n",
            "Epoch 70/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7002 - acc: 0.3733 - val_loss: 1.6429 - val_acc: 0.4183\n",
            "Epoch 71/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.7202 - acc: 0.3702 - val_loss: 2.1775 - val_acc: 0.2444\n",
            "Epoch 72/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.6964 - acc: 0.3800 - val_loss: 1.6539 - val_acc: 0.4221\n",
            "Epoch 73/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6526 - acc: 0.4137 - val_loss: 1.7119 - val_acc: 0.3982\n",
            "Epoch 74/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6294 - acc: 0.4305 - val_loss: 1.5600 - val_acc: 0.4624\n",
            "Epoch 75/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6457 - acc: 0.4337 - val_loss: 2.5193 - val_acc: 0.1798\n",
            "Epoch 76/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6329 - acc: 0.4309 - val_loss: 1.9113 - val_acc: 0.3423\n",
            "Epoch 77/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6155 - acc: 0.4497 - val_loss: 3.0704 - val_acc: 0.1307\n",
            "Epoch 78/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.1066 - acc: 0.2206 - val_loss: 2.0817 - val_acc: 0.2537\n",
            "Epoch 79/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.8762 - acc: 0.3235 - val_loss: 1.7530 - val_acc: 0.3744\n",
            "Epoch 80/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7401 - acc: 0.3877 - val_loss: 2.2757 - val_acc: 0.1637\n",
            "Epoch 81/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.9226 - acc: 0.3117 - val_loss: 1.8850 - val_acc: 0.3226\n",
            "Epoch 82/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7330 - acc: 0.3980 - val_loss: 1.5503 - val_acc: 0.4741\n",
            "Epoch 83/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6717 - acc: 0.4229 - val_loss: 1.6726 - val_acc: 0.3973\n",
            "Epoch 84/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6159 - acc: 0.4371 - val_loss: 1.5016 - val_acc: 0.4781\n",
            "Epoch 85/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5725 - acc: 0.4587 - val_loss: 1.4733 - val_acc: 0.4888\n",
            "Epoch 86/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5731 - acc: 0.4617 - val_loss: 1.4092 - val_acc: 0.5124\n",
            "Epoch 87/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5544 - acc: 0.4675 - val_loss: 1.5388 - val_acc: 0.4793\n",
            "Epoch 88/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6577 - acc: 0.4215 - val_loss: 1.5287 - val_acc: 0.4494\n",
            "Epoch 89/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6484 - acc: 0.4298 - val_loss: 1.4840 - val_acc: 0.4867\n",
            "Epoch 90/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5093 - acc: 0.4769 - val_loss: 1.3883 - val_acc: 0.5231\n",
            "Epoch 91/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4750 - acc: 0.4998 - val_loss: 1.3538 - val_acc: 0.5383\n",
            "Epoch 92/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4628 - acc: 0.5029 - val_loss: 1.3536 - val_acc: 0.5432\n",
            "Epoch 93/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.8705 - acc: 0.3381 - val_loss: 1.7826 - val_acc: 0.3864\n",
            "Epoch 94/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7166 - acc: 0.3949 - val_loss: 1.6258 - val_acc: 0.4209\n",
            "Epoch 95/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.6933 - acc: 0.4075 - val_loss: 1.6609 - val_acc: 0.4121\n",
            "Epoch 96/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7177 - acc: 0.4004 - val_loss: 1.7802 - val_acc: 0.3639\n",
            "Epoch 97/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6488 - acc: 0.4328 - val_loss: 2.7271 - val_acc: 0.1391\n",
            "Epoch 98/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7566 - acc: 0.3818 - val_loss: 2.0642 - val_acc: 0.3013\n",
            "Epoch 99/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5885 - acc: 0.4516 - val_loss: 1.5854 - val_acc: 0.4333\n",
            "Epoch 100/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5209 - acc: 0.4783 - val_loss: 1.3659 - val_acc: 0.5255\n",
            "Epoch 101/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.6139 - acc: 0.4417 - val_loss: 1.4150 - val_acc: 0.5203\n",
            "Epoch 102/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5516 - acc: 0.4648 - val_loss: 1.5898 - val_acc: 0.4432\n",
            "Epoch 103/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5054 - acc: 0.4824 - val_loss: 1.3838 - val_acc: 0.5270\n",
            "Epoch 104/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4597 - acc: 0.5042 - val_loss: 1.3799 - val_acc: 0.5221\n",
            "Epoch 105/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4326 - acc: 0.5183 - val_loss: 1.3063 - val_acc: 0.5593\n",
            "Epoch 106/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4133 - acc: 0.5218 - val_loss: 1.5470 - val_acc: 0.4787\n",
            "Epoch 107/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4213 - acc: 0.5218 - val_loss: 1.3201 - val_acc: 0.5557\n",
            "Epoch 108/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9041 - acc: 0.3207 - val_loss: 2.2416 - val_acc: 0.1712\n",
            "Epoch 109/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.8369 - acc: 0.3437 - val_loss: 2.0502 - val_acc: 0.2742\n",
            "Epoch 110/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8341 - acc: 0.3481 - val_loss: 2.2769 - val_acc: 0.1652\n",
            "Epoch 111/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.9599 - acc: 0.2973 - val_loss: 1.8208 - val_acc: 0.3664\n",
            "Epoch 112/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7950 - acc: 0.3645 - val_loss: 1.6426 - val_acc: 0.4422\n",
            "Epoch 113/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6463 - acc: 0.4244 - val_loss: 1.6212 - val_acc: 0.4280\n",
            "Epoch 114/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5597 - acc: 0.4580 - val_loss: 1.5159 - val_acc: 0.4766\n",
            "Epoch 115/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5428 - acc: 0.4639 - val_loss: 1.3910 - val_acc: 0.5153\n",
            "Epoch 116/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5986 - acc: 0.4459 - val_loss: 1.4742 - val_acc: 0.5046\n",
            "Epoch 117/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5071 - acc: 0.4795 - val_loss: 1.3368 - val_acc: 0.5372\n",
            "Epoch 118/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.5194 - acc: 0.4746 - val_loss: 1.4199 - val_acc: 0.5138\n",
            "Epoch 119/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4621 - acc: 0.5037 - val_loss: 1.3089 - val_acc: 0.5486\n",
            "Epoch 120/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4573 - acc: 0.5037 - val_loss: 1.5130 - val_acc: 0.4682\n",
            "Epoch 121/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4302 - acc: 0.5122 - val_loss: 1.3432 - val_acc: 0.5363\n",
            "Epoch 122/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4128 - acc: 0.5180 - val_loss: 1.3697 - val_acc: 0.5478\n",
            "Epoch 123/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3996 - acc: 0.5297 - val_loss: 1.3050 - val_acc: 0.5604\n",
            "Epoch 124/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3810 - acc: 0.5347 - val_loss: 1.3066 - val_acc: 0.5491\n",
            "Epoch 125/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.5583 - acc: 0.4690 - val_loss: 2.2849 - val_acc: 0.2657\n",
            "Epoch 126/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6153 - acc: 0.4450 - val_loss: 1.4466 - val_acc: 0.5033\n",
            "Epoch 127/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4847 - acc: 0.4953 - val_loss: 1.4118 - val_acc: 0.5071\n",
            "Epoch 128/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4534 - acc: 0.5064 - val_loss: 1.2966 - val_acc: 0.5603\n",
            "Epoch 129/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4245 - acc: 0.5188 - val_loss: 1.4713 - val_acc: 0.5056\n",
            "Epoch 130/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4091 - acc: 0.5292 - val_loss: 1.2642 - val_acc: 0.5729\n",
            "Epoch 131/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3823 - acc: 0.5335 - val_loss: 1.2440 - val_acc: 0.5799\n",
            "Epoch 132/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3683 - acc: 0.5414 - val_loss: 1.2267 - val_acc: 0.5882\n",
            "Epoch 133/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3446 - acc: 0.5529 - val_loss: 1.2207 - val_acc: 0.5884\n",
            "Epoch 134/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3374 - acc: 0.5528 - val_loss: 1.3535 - val_acc: 0.5314\n",
            "Epoch 135/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3382 - acc: 0.5554 - val_loss: 1.2132 - val_acc: 0.5921\n",
            "Epoch 136/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3277 - acc: 0.5605 - val_loss: 2.0293 - val_acc: 0.4117\n",
            "Epoch 137/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3335 - acc: 0.5557 - val_loss: 1.2000 - val_acc: 0.5957\n",
            "Epoch 138/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3040 - acc: 0.5697 - val_loss: 1.2173 - val_acc: 0.5927\n",
            "Epoch 139/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3098 - acc: 0.5688 - val_loss: 1.2307 - val_acc: 0.5917\n",
            "Epoch 140/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3346 - acc: 0.5627 - val_loss: 1.4023 - val_acc: 0.5326\n",
            "Epoch 141/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4757 - acc: 0.5023 - val_loss: 2.4840 - val_acc: 0.2616\n",
            "Epoch 142/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4182 - acc: 0.5298 - val_loss: 1.2744 - val_acc: 0.5860\n",
            "Epoch 143/500\n",
            "42000/42000 [==============================] - 1s 34us/step - loss: 1.3548 - acc: 0.5487 - val_loss: 1.2745 - val_acc: 0.5742\n",
            "Epoch 144/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3352 - acc: 0.5540 - val_loss: 2.4849 - val_acc: 0.3246\n",
            "Epoch 145/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3532 - acc: 0.5531 - val_loss: 1.2564 - val_acc: 0.5943\n",
            "Epoch 146/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3113 - acc: 0.5658 - val_loss: 1.2242 - val_acc: 0.5987\n",
            "Epoch 147/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.2842 - acc: 0.5785 - val_loss: 1.2016 - val_acc: 0.6086\n",
            "Epoch 148/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3485 - acc: 0.5571 - val_loss: 1.3296 - val_acc: 0.5583\n",
            "Epoch 149/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3438 - acc: 0.5562 - val_loss: 1.3073 - val_acc: 0.5664\n",
            "Epoch 150/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3070 - acc: 0.5728 - val_loss: 1.2252 - val_acc: 0.5902\n",
            "Epoch 151/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3222 - acc: 0.5625 - val_loss: 1.3046 - val_acc: 0.5591\n",
            "Epoch 152/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3430 - acc: 0.5565 - val_loss: 1.3125 - val_acc: 0.5486\n",
            "Epoch 153/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3483 - acc: 0.5597 - val_loss: 1.7608 - val_acc: 0.3727\n",
            "Epoch 154/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3706 - acc: 0.5526 - val_loss: 1.1965 - val_acc: 0.6114\n",
            "Epoch 155/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3349 - acc: 0.5667 - val_loss: 1.3785 - val_acc: 0.5560\n",
            "Epoch 156/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3122 - acc: 0.5735 - val_loss: 1.1616 - val_acc: 0.6197\n",
            "Epoch 157/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2743 - acc: 0.5893 - val_loss: 1.1512 - val_acc: 0.6209\n",
            "Epoch 158/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2524 - acc: 0.5990 - val_loss: 1.1380 - val_acc: 0.6274\n",
            "Epoch 159/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3272 - acc: 0.5646 - val_loss: 1.8858 - val_acc: 0.3507\n",
            "Epoch 160/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4295 - acc: 0.5261 - val_loss: 1.2400 - val_acc: 0.5901\n",
            "Epoch 161/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3027 - acc: 0.5741 - val_loss: 1.1890 - val_acc: 0.6098\n",
            "Epoch 162/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2810 - acc: 0.5862 - val_loss: 1.2303 - val_acc: 0.5934\n",
            "Epoch 163/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2514 - acc: 0.5987 - val_loss: 1.4243 - val_acc: 0.5553\n",
            "Epoch 164/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2398 - acc: 0.6008 - val_loss: 1.9811 - val_acc: 0.3674\n",
            "Epoch 165/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2696 - acc: 0.5951 - val_loss: 1.1692 - val_acc: 0.6176\n",
            "Epoch 166/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2367 - acc: 0.6039 - val_loss: 1.3701 - val_acc: 0.5305\n",
            "Epoch 167/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.2145 - acc: 0.6135 - val_loss: 1.1239 - val_acc: 0.6433\n",
            "Epoch 168/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2009 - acc: 0.6186 - val_loss: 1.2314 - val_acc: 0.6001\n",
            "Epoch 169/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2053 - acc: 0.6169 - val_loss: 1.1893 - val_acc: 0.6226\n",
            "Epoch 170/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2088 - acc: 0.6173 - val_loss: 1.2431 - val_acc: 0.6064\n",
            "Epoch 171/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1756 - acc: 0.6290 - val_loss: 1.1534 - val_acc: 0.6439\n",
            "Epoch 172/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1643 - acc: 0.6380 - val_loss: 1.0900 - val_acc: 0.6632\n",
            "Epoch 173/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1344 - acc: 0.6470 - val_loss: 1.0491 - val_acc: 0.6750\n",
            "Epoch 174/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1269 - acc: 0.6487 - val_loss: 1.2911 - val_acc: 0.5783\n",
            "Epoch 175/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1220 - acc: 0.6483 - val_loss: 1.0183 - val_acc: 0.6898\n",
            "Epoch 176/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0984 - acc: 0.6604 - val_loss: 1.1278 - val_acc: 0.6416\n",
            "Epoch 177/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1066 - acc: 0.6634 - val_loss: 1.0684 - val_acc: 0.6716\n",
            "Epoch 178/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0799 - acc: 0.6703 - val_loss: 2.4799 - val_acc: 0.2106\n",
            "Epoch 179/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.8243 - acc: 0.3761 - val_loss: 2.1801 - val_acc: 0.2254\n",
            "Epoch 180/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6039 - acc: 0.4645 - val_loss: 1.8247 - val_acc: 0.3742\n",
            "Epoch 181/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6080 - acc: 0.4590 - val_loss: 1.8747 - val_acc: 0.3528\n",
            "Epoch 182/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.5591 - acc: 0.4735 - val_loss: 1.4127 - val_acc: 0.5331\n",
            "Epoch 183/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4393 - acc: 0.5280 - val_loss: 1.3558 - val_acc: 0.5498\n",
            "Epoch 184/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3618 - acc: 0.5588 - val_loss: 1.3913 - val_acc: 0.5388\n",
            "Epoch 185/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3063 - acc: 0.5850 - val_loss: 1.2420 - val_acc: 0.6061\n",
            "Epoch 186/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3282 - acc: 0.5738 - val_loss: 1.2836 - val_acc: 0.5872\n",
            "Epoch 187/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2517 - acc: 0.6070 - val_loss: 1.3307 - val_acc: 0.5672\n",
            "Epoch 188/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2905 - acc: 0.5912 - val_loss: 1.7316 - val_acc: 0.4456\n",
            "Epoch 189/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2474 - acc: 0.6079 - val_loss: 1.2010 - val_acc: 0.6214\n",
            "Epoch 190/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2051 - acc: 0.6226 - val_loss: 1.2393 - val_acc: 0.6059\n",
            "Epoch 191/500\n",
            "42000/42000 [==============================] - 1s 34us/step - loss: 1.2061 - acc: 0.6205 - val_loss: 1.1096 - val_acc: 0.6542\n",
            "Epoch 192/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2851 - acc: 0.5970 - val_loss: 2.5118 - val_acc: 0.1629\n",
            "Epoch 193/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.7901 - acc: 0.3948 - val_loss: 1.8745 - val_acc: 0.3697\n",
            "Epoch 194/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4512 - acc: 0.5271 - val_loss: 1.6186 - val_acc: 0.4693\n",
            "Epoch 195/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3412 - acc: 0.5750 - val_loss: 1.3770 - val_acc: 0.5502\n",
            "Epoch 196/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2712 - acc: 0.5990 - val_loss: 1.2440 - val_acc: 0.6062\n",
            "Epoch 197/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2397 - acc: 0.6123 - val_loss: 1.3735 - val_acc: 0.5528\n",
            "Epoch 198/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2439 - acc: 0.6090 - val_loss: 1.2190 - val_acc: 0.6205\n",
            "Epoch 199/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2047 - acc: 0.6272 - val_loss: 1.0758 - val_acc: 0.6646\n",
            "Epoch 200/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1682 - acc: 0.6384 - val_loss: 1.1106 - val_acc: 0.6589\n",
            "Epoch 201/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3042 - acc: 0.5868 - val_loss: 1.7442 - val_acc: 0.4738\n",
            "Epoch 202/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2175 - acc: 0.6236 - val_loss: 1.0923 - val_acc: 0.6655\n",
            "Epoch 203/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1565 - acc: 0.6432 - val_loss: 1.0635 - val_acc: 0.6706\n",
            "Epoch 204/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1504 - acc: 0.6441 - val_loss: 1.0841 - val_acc: 0.6552\n",
            "Epoch 205/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1272 - acc: 0.6538 - val_loss: 1.0265 - val_acc: 0.6898\n",
            "Epoch 206/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1445 - acc: 0.6491 - val_loss: 1.1883 - val_acc: 0.6173\n",
            "Epoch 207/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1211 - acc: 0.6596 - val_loss: 1.1790 - val_acc: 0.6308\n",
            "Epoch 208/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1343 - acc: 0.6546 - val_loss: 1.0341 - val_acc: 0.6821\n",
            "Epoch 209/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1167 - acc: 0.6591 - val_loss: 0.9916 - val_acc: 0.6997\n",
            "Epoch 210/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0908 - acc: 0.6677 - val_loss: 1.0039 - val_acc: 0.6904\n",
            "Epoch 211/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0740 - acc: 0.6727 - val_loss: 0.9707 - val_acc: 0.7022\n",
            "Epoch 212/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0754 - acc: 0.6717 - val_loss: 0.9993 - val_acc: 0.6907\n",
            "Epoch 213/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0500 - acc: 0.6846 - val_loss: 1.0241 - val_acc: 0.6888\n",
            "Epoch 214/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3476 - acc: 0.5774 - val_loss: 2.7546 - val_acc: 0.2823\n",
            "Epoch 215/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.4445 - acc: 0.5355 - val_loss: 1.3023 - val_acc: 0.5853\n",
            "Epoch 216/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.1962 - acc: 0.6273 - val_loss: 1.1324 - val_acc: 0.6452\n",
            "Epoch 217/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1366 - acc: 0.6531 - val_loss: 1.0232 - val_acc: 0.6823\n",
            "Epoch 218/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1666 - acc: 0.6405 - val_loss: 2.0639 - val_acc: 0.3649\n",
            "Epoch 219/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1488 - acc: 0.6469 - val_loss: 1.0609 - val_acc: 0.6759\n",
            "Epoch 220/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1532 - acc: 0.6458 - val_loss: 1.0931 - val_acc: 0.6613\n",
            "Epoch 221/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1441 - acc: 0.6455 - val_loss: 1.3950 - val_acc: 0.5352\n",
            "Epoch 222/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1465 - acc: 0.6448 - val_loss: 1.0072 - val_acc: 0.6942\n",
            "Epoch 223/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1107 - acc: 0.6589 - val_loss: 1.2546 - val_acc: 0.6143\n",
            "Epoch 224/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1022 - acc: 0.6661 - val_loss: 1.0661 - val_acc: 0.6709\n",
            "Epoch 225/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0508 - acc: 0.6835 - val_loss: 1.0772 - val_acc: 0.6669\n",
            "Epoch 226/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0563 - acc: 0.6784 - val_loss: 1.0393 - val_acc: 0.6743\n",
            "Epoch 227/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0445 - acc: 0.6848 - val_loss: 0.9802 - val_acc: 0.6952\n",
            "Epoch 228/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0373 - acc: 0.6842 - val_loss: 0.9492 - val_acc: 0.7066\n",
            "Epoch 229/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0280 - acc: 0.6922 - val_loss: 0.9304 - val_acc: 0.7172\n",
            "Epoch 230/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0467 - acc: 0.6836 - val_loss: 1.0349 - val_acc: 0.6741\n",
            "Epoch 231/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0227 - acc: 0.6925 - val_loss: 0.9258 - val_acc: 0.7172\n",
            "Epoch 232/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9984 - acc: 0.6975 - val_loss: 0.9235 - val_acc: 0.7188\n",
            "Epoch 233/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9902 - acc: 0.7010 - val_loss: 0.9109 - val_acc: 0.7185\n",
            "Epoch 234/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0479 - acc: 0.6825 - val_loss: 1.5690 - val_acc: 0.5360\n",
            "Epoch 235/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0440 - acc: 0.6864 - val_loss: 1.0364 - val_acc: 0.6752\n",
            "Epoch 236/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0034 - acc: 0.6987 - val_loss: 0.9235 - val_acc: 0.7188\n",
            "Epoch 237/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9759 - acc: 0.7041 - val_loss: 0.9077 - val_acc: 0.7218\n",
            "Epoch 238/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9647 - acc: 0.7094 - val_loss: 0.9193 - val_acc: 0.7192\n",
            "Epoch 239/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9826 - acc: 0.7008 - val_loss: 0.9933 - val_acc: 0.6944\n",
            "Epoch 240/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.3281 - acc: 0.5783 - val_loss: 3.4118 - val_acc: 0.1028\n",
            "Epoch 241/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 2.0774 - acc: 0.2700 - val_loss: 2.1902 - val_acc: 0.2193\n",
            "Epoch 242/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6670 - acc: 0.4408 - val_loss: 1.7104 - val_acc: 0.4146\n",
            "Epoch 243/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3672 - acc: 0.5606 - val_loss: 1.3709 - val_acc: 0.5494\n",
            "Epoch 244/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3115 - acc: 0.5875 - val_loss: 1.6720 - val_acc: 0.4341\n",
            "Epoch 245/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3691 - acc: 0.5629 - val_loss: 1.4026 - val_acc: 0.5657\n",
            "Epoch 246/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2823 - acc: 0.5969 - val_loss: 1.5193 - val_acc: 0.5445\n",
            "Epoch 247/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.3066 - acc: 0.5916 - val_loss: 1.2461 - val_acc: 0.6052\n",
            "Epoch 248/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2002 - acc: 0.6299 - val_loss: 1.0855 - val_acc: 0.6624\n",
            "Epoch 249/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1576 - acc: 0.6466 - val_loss: 0.9822 - val_acc: 0.7003\n",
            "Epoch 250/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1156 - acc: 0.6573 - val_loss: 1.0588 - val_acc: 0.6669\n",
            "Epoch 251/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1481 - acc: 0.6464 - val_loss: 2.1572 - val_acc: 0.2939\n",
            "Epoch 252/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2800 - acc: 0.5950 - val_loss: 1.2330 - val_acc: 0.5916\n",
            "Epoch 253/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2462 - acc: 0.6079 - val_loss: 1.7160 - val_acc: 0.4465\n",
            "Epoch 254/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2158 - acc: 0.6193 - val_loss: 1.2571 - val_acc: 0.5966\n",
            "Epoch 255/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1384 - acc: 0.6488 - val_loss: 1.0835 - val_acc: 0.6576\n",
            "Epoch 256/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0809 - acc: 0.6695 - val_loss: 1.0584 - val_acc: 0.6588\n",
            "Epoch 257/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0469 - acc: 0.6783 - val_loss: 0.9308 - val_acc: 0.7138\n",
            "Epoch 258/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0363 - acc: 0.6872 - val_loss: 0.9696 - val_acc: 0.6984\n",
            "Epoch 259/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.1056 - acc: 0.6619 - val_loss: 1.0119 - val_acc: 0.6836\n",
            "Epoch 260/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0383 - acc: 0.6861 - val_loss: 0.9156 - val_acc: 0.7243\n",
            "Epoch 261/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1505 - acc: 0.6528 - val_loss: 2.9855 - val_acc: 0.2952\n",
            "Epoch 262/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.2423 - acc: 0.6146 - val_loss: 1.3588 - val_acc: 0.5887\n",
            "Epoch 263/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0909 - acc: 0.6684 - val_loss: 0.9911 - val_acc: 0.6943\n",
            "Epoch 264/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0550 - acc: 0.6786 - val_loss: 0.9535 - val_acc: 0.7066\n",
            "Epoch 265/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0942 - acc: 0.6653 - val_loss: 1.0305 - val_acc: 0.6800\n",
            "Epoch 266/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0762 - acc: 0.6713 - val_loss: 0.9580 - val_acc: 0.7126\n",
            "Epoch 267/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0356 - acc: 0.6870 - val_loss: 0.9616 - val_acc: 0.7093\n",
            "Epoch 268/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0132 - acc: 0.6931 - val_loss: 0.9190 - val_acc: 0.7229\n",
            "Epoch 269/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9987 - acc: 0.6985 - val_loss: 0.8978 - val_acc: 0.7296\n",
            "Epoch 270/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1006 - acc: 0.6641 - val_loss: 1.1883 - val_acc: 0.6261\n",
            "Epoch 271/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1081 - acc: 0.6601 - val_loss: 0.9630 - val_acc: 0.7059\n",
            "Epoch 272/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0420 - acc: 0.6800 - val_loss: 0.9155 - val_acc: 0.7239\n",
            "Epoch 273/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0020 - acc: 0.6951 - val_loss: 0.8836 - val_acc: 0.7323\n",
            "Epoch 274/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0095 - acc: 0.6935 - val_loss: 0.9247 - val_acc: 0.7172\n",
            "Epoch 275/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0623 - acc: 0.6740 - val_loss: 2.1480 - val_acc: 0.3612\n",
            "Epoch 276/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0886 - acc: 0.6652 - val_loss: 1.0015 - val_acc: 0.6884\n",
            "Epoch 277/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0027 - acc: 0.6966 - val_loss: 0.9190 - val_acc: 0.7192\n",
            "Epoch 278/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9926 - acc: 0.7017 - val_loss: 1.3695 - val_acc: 0.5717\n",
            "Epoch 279/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0639 - acc: 0.6737 - val_loss: 0.9650 - val_acc: 0.7029\n",
            "Epoch 280/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9926 - acc: 0.6983 - val_loss: 0.9121 - val_acc: 0.7214\n",
            "Epoch 281/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1456 - acc: 0.6427 - val_loss: 2.9607 - val_acc: 0.1726\n",
            "Epoch 282/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2655 - acc: 0.6053 - val_loss: 1.4721 - val_acc: 0.5204\n",
            "Epoch 283/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0987 - acc: 0.6623 - val_loss: 1.1035 - val_acc: 0.6554\n",
            "Epoch 284/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0466 - acc: 0.6852 - val_loss: 0.9764 - val_acc: 0.7027\n",
            "Epoch 285/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0484 - acc: 0.6778 - val_loss: 1.0842 - val_acc: 0.6609\n",
            "Epoch 286/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0170 - acc: 0.6919 - val_loss: 0.9037 - val_acc: 0.7239\n",
            "Epoch 287/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9884 - acc: 0.7044 - val_loss: 0.8888 - val_acc: 0.7289\n",
            "Epoch 288/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9574 - acc: 0.7095 - val_loss: 0.8978 - val_acc: 0.7303\n",
            "Epoch 289/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9520 - acc: 0.7144 - val_loss: 0.8722 - val_acc: 0.7351\n",
            "Epoch 290/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9397 - acc: 0.7110 - val_loss: 0.9705 - val_acc: 0.6983\n",
            "Epoch 291/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9427 - acc: 0.7157 - val_loss: 0.9250 - val_acc: 0.7134\n",
            "Epoch 292/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9672 - acc: 0.7057 - val_loss: 0.8894 - val_acc: 0.7317\n",
            "Epoch 293/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0943 - acc: 0.6607 - val_loss: 2.1155 - val_acc: 0.3631\n",
            "Epoch 294/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0669 - acc: 0.6725 - val_loss: 1.3445 - val_acc: 0.5791\n",
            "Epoch 295/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2020 - acc: 0.6245 - val_loss: 1.1644 - val_acc: 0.6205\n",
            "Epoch 296/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0807 - acc: 0.6645 - val_loss: 1.0955 - val_acc: 0.6558\n",
            "Epoch 297/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0086 - acc: 0.6897 - val_loss: 0.9718 - val_acc: 0.6998\n",
            "Epoch 298/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9844 - acc: 0.7014 - val_loss: 0.8978 - val_acc: 0.7248\n",
            "Epoch 299/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0413 - acc: 0.6800 - val_loss: 1.8785 - val_acc: 0.4077\n",
            "Epoch 300/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.1059 - acc: 0.6584 - val_loss: 1.0733 - val_acc: 0.6657\n",
            "Epoch 301/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0157 - acc: 0.6854 - val_loss: 0.9671 - val_acc: 0.7004\n",
            "Epoch 302/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9754 - acc: 0.7031 - val_loss: 0.8867 - val_acc: 0.7277\n",
            "Epoch 303/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9759 - acc: 0.7009 - val_loss: 0.9303 - val_acc: 0.7167\n",
            "Epoch 304/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9516 - acc: 0.7109 - val_loss: 0.9473 - val_acc: 0.7081\n",
            "Epoch 305/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9377 - acc: 0.7137 - val_loss: 0.8745 - val_acc: 0.7326\n",
            "Epoch 306/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9133 - acc: 0.7254 - val_loss: 0.8493 - val_acc: 0.7446\n",
            "Epoch 307/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9055 - acc: 0.7260 - val_loss: 0.8557 - val_acc: 0.7386\n",
            "Epoch 308/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8926 - acc: 0.7310 - val_loss: 0.8513 - val_acc: 0.7380\n",
            "Epoch 309/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8836 - acc: 0.7345 - val_loss: 0.8976 - val_acc: 0.7207\n",
            "Epoch 310/500\n",
            "42000/42000 [==============================] - 1s 34us/step - loss: 0.8912 - acc: 0.7324 - val_loss: 0.8443 - val_acc: 0.7455\n",
            "Epoch 311/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8712 - acc: 0.7371 - val_loss: 0.8505 - val_acc: 0.7415\n",
            "Epoch 312/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8756 - acc: 0.7379 - val_loss: 0.9793 - val_acc: 0.6956\n",
            "Epoch 313/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9062 - acc: 0.7272 - val_loss: 0.8785 - val_acc: 0.7422\n",
            "Epoch 314/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8689 - acc: 0.7413 - val_loss: 0.8211 - val_acc: 0.7527\n",
            "Epoch 315/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8568 - acc: 0.7414 - val_loss: 0.8069 - val_acc: 0.7548\n",
            "Epoch 316/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8586 - acc: 0.7413 - val_loss: 0.8244 - val_acc: 0.7520\n",
            "Epoch 317/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8733 - acc: 0.7394 - val_loss: 2.7860 - val_acc: 0.2566\n",
            "Epoch 318/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9746 - acc: 0.7045 - val_loss: 2.1487 - val_acc: 0.3676\n",
            "Epoch 319/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8966 - acc: 0.7293 - val_loss: 0.9362 - val_acc: 0.7221\n",
            "Epoch 320/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8798 - acc: 0.7390 - val_loss: 1.0468 - val_acc: 0.6666\n",
            "Epoch 321/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8533 - acc: 0.7432 - val_loss: 0.8186 - val_acc: 0.7573\n",
            "Epoch 322/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8537 - acc: 0.7474 - val_loss: 0.8059 - val_acc: 0.7593\n",
            "Epoch 323/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8384 - acc: 0.7499 - val_loss: 0.8041 - val_acc: 0.7631\n",
            "Epoch 324/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8417 - acc: 0.7486 - val_loss: 2.3128 - val_acc: 0.4592\n",
            "Epoch 325/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9602 - acc: 0.7105 - val_loss: 0.8293 - val_acc: 0.7513\n",
            "Epoch 326/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9987 - acc: 0.6958 - val_loss: 1.3293 - val_acc: 0.5802\n",
            "Epoch 327/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9088 - acc: 0.7276 - val_loss: 0.8945 - val_acc: 0.7232\n",
            "Epoch 328/500\n",
            "42000/42000 [==============================] - 1s 31us/step - loss: 0.8857 - acc: 0.7341 - val_loss: 1.7659 - val_acc: 0.4242\n",
            "Epoch 329/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1739 - acc: 0.6363 - val_loss: 1.3168 - val_acc: 0.5922\n",
            "Epoch 330/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0129 - acc: 0.6909 - val_loss: 0.9931 - val_acc: 0.7053\n",
            "Epoch 331/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9439 - acc: 0.7149 - val_loss: 0.8573 - val_acc: 0.7448\n",
            "Epoch 332/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9249 - acc: 0.7205 - val_loss: 0.8511 - val_acc: 0.7440\n",
            "Epoch 333/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9324 - acc: 0.7155 - val_loss: 0.8515 - val_acc: 0.7418\n",
            "Epoch 334/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8923 - acc: 0.7326 - val_loss: 0.8627 - val_acc: 0.7444\n",
            "Epoch 335/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8969 - acc: 0.7275 - val_loss: 0.8217 - val_acc: 0.7518\n",
            "Epoch 336/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2333 - acc: 0.6089 - val_loss: 1.2653 - val_acc: 0.5931\n",
            "Epoch 337/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0585 - acc: 0.6781 - val_loss: 0.9060 - val_acc: 0.7236\n",
            "Epoch 338/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0116 - acc: 0.6910 - val_loss: 0.8595 - val_acc: 0.7412\n",
            "Epoch 339/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9902 - acc: 0.7023 - val_loss: 1.6655 - val_acc: 0.5151\n",
            "Epoch 340/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0477 - acc: 0.6793 - val_loss: 0.8747 - val_acc: 0.7347\n",
            "Epoch 341/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9975 - acc: 0.6970 - val_loss: 0.8619 - val_acc: 0.7398\n",
            "Epoch 342/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9573 - acc: 0.7086 - val_loss: 0.8562 - val_acc: 0.7412\n",
            "Epoch 343/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9197 - acc: 0.7211 - val_loss: 0.8267 - val_acc: 0.7482\n",
            "Epoch 344/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9111 - acc: 0.7262 - val_loss: 0.8930 - val_acc: 0.7261\n",
            "Epoch 345/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9252 - acc: 0.7187 - val_loss: 1.7517 - val_acc: 0.5692\n",
            "Epoch 346/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.0572 - acc: 0.6746 - val_loss: 1.0522 - val_acc: 0.6686\n",
            "Epoch 347/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9711 - acc: 0.7035 - val_loss: 0.8615 - val_acc: 0.7418\n",
            "Epoch 348/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9039 - acc: 0.7295 - val_loss: 0.8274 - val_acc: 0.7513\n",
            "Epoch 349/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8845 - acc: 0.7325 - val_loss: 0.8095 - val_acc: 0.7605\n",
            "Epoch 350/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8562 - acc: 0.7417 - val_loss: 0.8069 - val_acc: 0.7616\n",
            "Epoch 351/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8520 - acc: 0.7435 - val_loss: 0.7987 - val_acc: 0.7606\n",
            "Epoch 352/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8426 - acc: 0.7452 - val_loss: 0.8478 - val_acc: 0.7435\n",
            "Epoch 353/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9987 - acc: 0.6884 - val_loss: 2.2729 - val_acc: 0.2823\n",
            "Epoch 354/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1079 - acc: 0.6550 - val_loss: 1.1637 - val_acc: 0.6318\n",
            "Epoch 355/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9819 - acc: 0.7000 - val_loss: 0.8922 - val_acc: 0.7309\n",
            "Epoch 356/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9321 - acc: 0.7155 - val_loss: 0.8494 - val_acc: 0.7463\n",
            "Epoch 357/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9093 - acc: 0.7236 - val_loss: 0.8485 - val_acc: 0.7425\n",
            "Epoch 358/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9516 - acc: 0.7093 - val_loss: 0.8569 - val_acc: 0.7454\n",
            "Epoch 359/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9292 - acc: 0.7206 - val_loss: 0.8272 - val_acc: 0.7492\n",
            "Epoch 360/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8953 - acc: 0.7323 - val_loss: 0.8139 - val_acc: 0.7561\n",
            "Epoch 361/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8814 - acc: 0.7347 - val_loss: 0.8023 - val_acc: 0.7614\n",
            "Epoch 362/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8606 - acc: 0.7398 - val_loss: 0.8037 - val_acc: 0.7612\n",
            "Epoch 363/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8518 - acc: 0.7418 - val_loss: 0.7984 - val_acc: 0.7619\n",
            "Epoch 364/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8407 - acc: 0.7436 - val_loss: 0.8024 - val_acc: 0.7568\n",
            "Epoch 365/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8708 - acc: 0.7334 - val_loss: 0.8373 - val_acc: 0.7498\n",
            "Epoch 366/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8427 - acc: 0.7439 - val_loss: 0.8187 - val_acc: 0.7586\n",
            "Epoch 367/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8262 - acc: 0.7508 - val_loss: 0.7948 - val_acc: 0.7678\n",
            "Epoch 368/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9593 - acc: 0.7085 - val_loss: 2.3984 - val_acc: 0.4179\n",
            "Epoch 369/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9787 - acc: 0.7006 - val_loss: 1.0105 - val_acc: 0.6872\n",
            "Epoch 370/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8982 - acc: 0.7275 - val_loss: 0.8305 - val_acc: 0.7542\n",
            "Epoch 371/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9178 - acc: 0.7213 - val_loss: 0.8388 - val_acc: 0.7477\n",
            "Epoch 372/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8764 - acc: 0.7344 - val_loss: 0.8121 - val_acc: 0.7548\n",
            "Epoch 373/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8536 - acc: 0.7411 - val_loss: 0.8194 - val_acc: 0.7541\n",
            "Epoch 374/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8500 - acc: 0.7405 - val_loss: 0.8235 - val_acc: 0.7588\n",
            "Epoch 375/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8348 - acc: 0.7468 - val_loss: 0.7932 - val_acc: 0.7658\n",
            "Epoch 376/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8233 - acc: 0.7535 - val_loss: 0.7890 - val_acc: 0.7689\n",
            "Epoch 377/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8056 - acc: 0.7566 - val_loss: 0.8058 - val_acc: 0.7600\n",
            "Epoch 378/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8291 - acc: 0.7504 - val_loss: 0.7715 - val_acc: 0.7718\n",
            "Epoch 379/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9876 - acc: 0.6967 - val_loss: 1.1096 - val_acc: 0.6506\n",
            "Epoch 380/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9739 - acc: 0.7011 - val_loss: 0.9026 - val_acc: 0.7264\n",
            "Epoch 381/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8749 - acc: 0.7327 - val_loss: 0.8058 - val_acc: 0.7604\n",
            "Epoch 382/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8280 - acc: 0.7501 - val_loss: 0.7975 - val_acc: 0.7615\n",
            "Epoch 383/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8281 - acc: 0.7525 - val_loss: 0.8087 - val_acc: 0.7612\n",
            "Epoch 384/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8060 - acc: 0.7572 - val_loss: 1.4628 - val_acc: 0.5820\n",
            "Epoch 385/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8832 - acc: 0.7328 - val_loss: 1.0699 - val_acc: 0.6671\n",
            "Epoch 386/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8634 - acc: 0.7411 - val_loss: 0.7970 - val_acc: 0.7622\n",
            "Epoch 387/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8192 - acc: 0.7547 - val_loss: 0.7873 - val_acc: 0.7685\n",
            "Epoch 388/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8043 - acc: 0.7594 - val_loss: 0.7870 - val_acc: 0.7686\n",
            "Epoch 389/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8144 - acc: 0.7557 - val_loss: 0.8245 - val_acc: 0.7636\n",
            "Epoch 390/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8085 - acc: 0.7594 - val_loss: 0.7858 - val_acc: 0.7634\n",
            "Epoch 391/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7838 - acc: 0.7630 - val_loss: 0.7618 - val_acc: 0.7728\n",
            "Epoch 392/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7999 - acc: 0.7567 - val_loss: 0.9626 - val_acc: 0.7102\n",
            "Epoch 393/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7886 - acc: 0.7647 - val_loss: 0.7577 - val_acc: 0.7788\n",
            "Epoch 394/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8023 - acc: 0.7576 - val_loss: 0.8098 - val_acc: 0.7594\n",
            "Epoch 395/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7800 - acc: 0.7687 - val_loss: 0.9817 - val_acc: 0.7074\n",
            "Epoch 396/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7895 - acc: 0.7658 - val_loss: 0.7704 - val_acc: 0.7756\n",
            "Epoch 397/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7814 - acc: 0.7692 - val_loss: 0.7627 - val_acc: 0.7752\n",
            "Epoch 398/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7827 - acc: 0.7621 - val_loss: 1.5820 - val_acc: 0.5641\n",
            "Epoch 399/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8244 - acc: 0.7494 - val_loss: 0.7909 - val_acc: 0.7672\n",
            "Epoch 400/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7622 - acc: 0.7726 - val_loss: 0.7943 - val_acc: 0.7669\n",
            "Epoch 401/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7543 - acc: 0.7715 - val_loss: 0.7459 - val_acc: 0.7871\n",
            "Epoch 402/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8507 - acc: 0.7430 - val_loss: 1.0512 - val_acc: 0.6748\n",
            "Epoch 403/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8032 - acc: 0.7579 - val_loss: 0.7652 - val_acc: 0.7793\n",
            "Epoch 404/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8384 - acc: 0.7443 - val_loss: 1.9664 - val_acc: 0.3791\n",
            "Epoch 405/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.6299 - acc: 0.4617 - val_loss: 1.6290 - val_acc: 0.4726\n",
            "Epoch 406/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.2343 - acc: 0.6191 - val_loss: 1.1195 - val_acc: 0.6473\n",
            "Epoch 407/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 1.1073 - acc: 0.6589 - val_loss: 0.9952 - val_acc: 0.6877\n",
            "Epoch 408/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0160 - acc: 0.6904 - val_loss: 0.9850 - val_acc: 0.6974\n",
            "Epoch 409/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9547 - acc: 0.7108 - val_loss: 0.9303 - val_acc: 0.7114\n",
            "Epoch 410/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9424 - acc: 0.7131 - val_loss: 0.8672 - val_acc: 0.7421\n",
            "Epoch 411/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8992 - acc: 0.7299 - val_loss: 0.8383 - val_acc: 0.7520\n",
            "Epoch 412/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8732 - acc: 0.7372 - val_loss: 0.8186 - val_acc: 0.7578\n",
            "Epoch 413/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8913 - acc: 0.7308 - val_loss: 0.8057 - val_acc: 0.7604\n",
            "Epoch 414/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8974 - acc: 0.7282 - val_loss: 2.3827 - val_acc: 0.3387\n",
            "Epoch 415/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0392 - acc: 0.6773 - val_loss: 0.9796 - val_acc: 0.6990\n",
            "Epoch 416/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8852 - acc: 0.7324 - val_loss: 0.8273 - val_acc: 0.7518\n",
            "Epoch 417/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9211 - acc: 0.7192 - val_loss: 1.6054 - val_acc: 0.4757\n",
            "Epoch 418/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9327 - acc: 0.7153 - val_loss: 0.9271 - val_acc: 0.7121\n",
            "Epoch 419/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8827 - acc: 0.7345 - val_loss: 0.7951 - val_acc: 0.7626\n",
            "Epoch 420/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8440 - acc: 0.7473 - val_loss: 0.7780 - val_acc: 0.7706\n",
            "Epoch 421/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8389 - acc: 0.7468 - val_loss: 0.7979 - val_acc: 0.7644\n",
            "Epoch 422/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8163 - acc: 0.7559 - val_loss: 0.7754 - val_acc: 0.7734\n",
            "Epoch 423/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8014 - acc: 0.7581 - val_loss: 0.7805 - val_acc: 0.7694\n",
            "Epoch 424/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8037 - acc: 0.7600 - val_loss: 0.8255 - val_acc: 0.7533\n",
            "Epoch 425/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7924 - acc: 0.7610 - val_loss: 0.7767 - val_acc: 0.7745\n",
            "Epoch 426/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7900 - acc: 0.7620 - val_loss: 0.7969 - val_acc: 0.7653\n",
            "Epoch 427/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7859 - acc: 0.7635 - val_loss: 1.2421 - val_acc: 0.6847\n",
            "Epoch 428/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8102 - acc: 0.7535 - val_loss: 0.7666 - val_acc: 0.7766\n",
            "Epoch 429/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7753 - acc: 0.7677 - val_loss: 0.7545 - val_acc: 0.7788\n",
            "Epoch 430/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7761 - acc: 0.7642 - val_loss: 1.2576 - val_acc: 0.6111\n",
            "Epoch 431/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7820 - acc: 0.7661 - val_loss: 0.7549 - val_acc: 0.7815\n",
            "Epoch 432/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7508 - acc: 0.7766 - val_loss: 0.7458 - val_acc: 0.7862\n",
            "Epoch 433/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8855 - acc: 0.7296 - val_loss: 1.3498 - val_acc: 0.5639\n",
            "Epoch 434/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.9278 - acc: 0.7181 - val_loss: 0.8637 - val_acc: 0.7406\n",
            "Epoch 435/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8596 - acc: 0.7397 - val_loss: 0.8105 - val_acc: 0.7627\n",
            "Epoch 436/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8297 - acc: 0.7487 - val_loss: 0.8459 - val_acc: 0.7398\n",
            "Epoch 437/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8231 - acc: 0.7507 - val_loss: 0.7899 - val_acc: 0.7643\n",
            "Epoch 438/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7836 - acc: 0.7617 - val_loss: 0.7508 - val_acc: 0.7777\n",
            "Epoch 439/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8626 - acc: 0.7352 - val_loss: 0.7971 - val_acc: 0.7662\n",
            "Epoch 440/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7879 - acc: 0.7617 - val_loss: 0.7404 - val_acc: 0.7788\n",
            "Epoch 441/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0635 - acc: 0.6797 - val_loss: 1.4324 - val_acc: 0.5247\n",
            "Epoch 442/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9025 - acc: 0.7255 - val_loss: 0.8120 - val_acc: 0.7552\n",
            "Epoch 443/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8424 - acc: 0.7447 - val_loss: 0.8167 - val_acc: 0.7567\n",
            "Epoch 444/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7994 - acc: 0.7596 - val_loss: 0.7773 - val_acc: 0.7692\n",
            "Epoch 445/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8240 - acc: 0.7532 - val_loss: 2.4242 - val_acc: 0.5165\n",
            "Epoch 446/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8557 - acc: 0.7424 - val_loss: 0.8091 - val_acc: 0.7621\n",
            "Epoch 447/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8032 - acc: 0.7583 - val_loss: 0.7694 - val_acc: 0.7702\n",
            "Epoch 448/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7776 - acc: 0.7681 - val_loss: 0.7578 - val_acc: 0.7783\n",
            "Epoch 449/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7563 - acc: 0.7740 - val_loss: 0.7565 - val_acc: 0.7794\n",
            "Epoch 450/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9228 - acc: 0.7199 - val_loss: 1.8070 - val_acc: 0.4173\n",
            "Epoch 451/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9188 - acc: 0.7199 - val_loss: 0.8266 - val_acc: 0.7543\n",
            "Epoch 452/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8322 - acc: 0.7498 - val_loss: 0.7634 - val_acc: 0.7741\n",
            "Epoch 453/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8367 - acc: 0.7465 - val_loss: 1.0361 - val_acc: 0.6778\n",
            "Epoch 454/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8272 - acc: 0.7516 - val_loss: 0.7538 - val_acc: 0.7798\n",
            "Epoch 455/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7786 - acc: 0.7623 - val_loss: 0.7384 - val_acc: 0.7859\n",
            "Epoch 456/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7943 - acc: 0.7628 - val_loss: 0.7510 - val_acc: 0.7777\n",
            "Epoch 457/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7717 - acc: 0.7669 - val_loss: 0.7311 - val_acc: 0.7826\n",
            "Epoch 458/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7558 - acc: 0.7734 - val_loss: 0.7187 - val_acc: 0.7882\n",
            "Epoch 459/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7469 - acc: 0.7734 - val_loss: 0.7299 - val_acc: 0.7887\n",
            "Epoch 460/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8133 - acc: 0.7529 - val_loss: 0.8075 - val_acc: 0.7589\n",
            "Epoch 461/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9522 - acc: 0.7107 - val_loss: 0.9866 - val_acc: 0.6858\n",
            "Epoch 462/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8729 - acc: 0.7363 - val_loss: 0.7843 - val_acc: 0.7682\n",
            "Epoch 463/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4559 - acc: 0.5291 - val_loss: 2.1077 - val_acc: 0.2772\n",
            "Epoch 464/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.4539 - acc: 0.5294 - val_loss: 1.3912 - val_acc: 0.5424\n",
            "Epoch 465/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.1615 - acc: 0.6356 - val_loss: 0.9722 - val_acc: 0.6994\n",
            "Epoch 466/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 1.0406 - acc: 0.6808 - val_loss: 0.8845 - val_acc: 0.7294\n",
            "Epoch 467/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9940 - acc: 0.6950 - val_loss: 0.8497 - val_acc: 0.7426\n",
            "Epoch 468/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9320 - acc: 0.7167 - val_loss: 0.9843 - val_acc: 0.6911\n",
            "Epoch 469/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.9498 - acc: 0.7135 - val_loss: 0.8126 - val_acc: 0.7603\n",
            "Epoch 470/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8920 - acc: 0.7283 - val_loss: 0.8020 - val_acc: 0.7618\n",
            "Epoch 471/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8537 - acc: 0.7415 - val_loss: 0.7771 - val_acc: 0.7711\n",
            "Epoch 472/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8380 - acc: 0.7441 - val_loss: 1.0515 - val_acc: 0.6767\n",
            "Epoch 473/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8516 - acc: 0.7427 - val_loss: 0.7855 - val_acc: 0.7673\n",
            "Epoch 474/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8141 - acc: 0.7548 - val_loss: 0.7554 - val_acc: 0.7787\n",
            "Epoch 475/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7994 - acc: 0.7579 - val_loss: 0.7580 - val_acc: 0.7774\n",
            "Epoch 476/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7961 - acc: 0.7596 - val_loss: 0.7916 - val_acc: 0.7622\n",
            "Epoch 477/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8007 - acc: 0.7560 - val_loss: 1.0399 - val_acc: 0.6775\n",
            "Epoch 478/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.8151 - acc: 0.7516 - val_loss: 0.7716 - val_acc: 0.7754\n",
            "Epoch 479/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7870 - acc: 0.7600 - val_loss: 0.7331 - val_acc: 0.7844\n",
            "Epoch 480/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7896 - acc: 0.7604 - val_loss: 0.7532 - val_acc: 0.7795\n",
            "Epoch 481/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7679 - acc: 0.7687 - val_loss: 0.7315 - val_acc: 0.7853\n",
            "Epoch 482/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7691 - acc: 0.7645 - val_loss: 0.7523 - val_acc: 0.7808\n",
            "Epoch 483/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7530 - acc: 0.7728 - val_loss: 0.7432 - val_acc: 0.7858\n",
            "Epoch 484/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7619 - acc: 0.7683 - val_loss: 0.8007 - val_acc: 0.7660\n",
            "Epoch 485/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7503 - acc: 0.7730 - val_loss: 0.7544 - val_acc: 0.7816\n",
            "Epoch 486/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7327 - acc: 0.7783 - val_loss: 0.7261 - val_acc: 0.7906\n",
            "Epoch 487/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7207 - acc: 0.7809 - val_loss: 0.7293 - val_acc: 0.7884\n",
            "Epoch 488/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7347 - acc: 0.7771 - val_loss: 0.7399 - val_acc: 0.7844\n",
            "Epoch 489/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7430 - acc: 0.7756 - val_loss: 0.7441 - val_acc: 0.7900\n",
            "Epoch 490/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7322 - acc: 0.7811 - val_loss: 0.7388 - val_acc: 0.7853\n",
            "Epoch 491/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7385 - acc: 0.7782 - val_loss: 0.9678 - val_acc: 0.7166\n",
            "Epoch 492/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7325 - acc: 0.7798 - val_loss: 0.7318 - val_acc: 0.7909\n",
            "Epoch 493/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7153 - acc: 0.7848 - val_loss: 0.7185 - val_acc: 0.7908\n",
            "Epoch 494/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7064 - acc: 0.7851 - val_loss: 0.7150 - val_acc: 0.7941\n",
            "Epoch 495/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7162 - acc: 0.7834 - val_loss: 0.7235 - val_acc: 0.7918\n",
            "Epoch 496/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.6974 - acc: 0.7882 - val_loss: 0.7350 - val_acc: 0.7906\n",
            "Epoch 497/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.7031 - acc: 0.7862 - val_loss: 0.7175 - val_acc: 0.7954\n",
            "Epoch 498/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7236 - acc: 0.7807 - val_loss: 1.3416 - val_acc: 0.5892\n",
            "Epoch 499/500\n",
            "42000/42000 [==============================] - 1s 33us/step - loss: 0.8611 - acc: 0.7364 - val_loss: 0.8034 - val_acc: 0.7602\n",
            "Epoch 500/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.7636 - acc: 0.7682 - val_loss: 1.7582 - val_acc: 0.5937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2dded452e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_kacYrMAKxh",
        "colab_type": "text"
      },
      "source": [
        "# Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions) (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fgr8YBVAK4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IpfZhnAXwc",
        "colab_type": "text"
      },
      "source": [
        "# Implement batch normalization for training the neural network (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxo2EgG9ALFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zEsE_KeAehc",
        "colab_type": "text"
      },
      "source": [
        "# Understand the differences and trade-offs between traditional and NN classifiers with the help of classification metrics (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiBiFHMgAgUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}