{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glaiml/prj8-neural-net-img-classification/blob/master/vgk-neuner-img-classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMhsX4iHBJvt",
        "colab_type": "text"
      },
      "source": [
        "Vinayak G Kudva Project submission for GLAIML \n",
        "https://github.com/glaiml/prj8-neural-net-img-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiQaKY_-gmj",
        "colab_type": "text"
      },
      "source": [
        "# The Real Problem\n",
        "\n",
        "Recognizing multi-digit numbers in photographs captured at street level is an important\n",
        "component of modern-day map making. A classic example of a corpus of such street\n",
        "level photographs is Google’s Street View imagery comprised of hundreds of millions of\n",
        "geo-located 360 degree panoramic images. The ability to automatically transcribe an\n",
        "address number from a geo-located patch of pixels and associate the transcribed\n",
        "number with a known street address helps pinpoint, with a high degree of accuracy, the\n",
        "location of the building it represents.\n",
        "More broadly, recognizing numbers in photographs is a problem of interest to the optical\n",
        "character recognition community. While OCR on constrained domains like document\n",
        "processing is well studied, arbitrary multi-character text recognition in photographs is\n",
        "still highly challenging. This difficulty arises due to the wide variability in the visual\n",
        "appearance of text in the wild on account of a large range of fonts, colors, styles,\n",
        "orientations, and character arrangements. The recognition problem is further\n",
        "complicated by environmental factors such as lighting, shadows, specularities, and\n",
        "occlusions as well as by image acquisition factors such as resolution, motion, and focus\n",
        "blurs.\n",
        "In this project we will use dataset with images centred around a single digit (many of the\n",
        "images do contain some distractors at the sides). Although we are taking a sample of\n",
        "the data which is simpler, it is more complex than MNIST because of the distractors.\n",
        "\n",
        "# Project Description\n",
        "In this hands-on project the goal is to build a python code for image classification from\n",
        "scratch to understand the nitty gritties of building and training a model and further to\n",
        "understand the advantages of neural networks. First we will implement a simple KNN\n",
        "classifier and later implement a Neural Network to classify the images in the SVHN\n",
        "dataset. We will compare the computational efficiency and accuracy between the\n",
        "traditional methods and neural networks.\n",
        "\n",
        "# The Street View House Numbers (SVHN) Dataset\n",
        "SVHN is a real-world image dataset for developing machine learning and object\n",
        "recognition algorithms with minimal requirement on data formatting but comes from a\n",
        "significantly harder, unsolved, real world problem (recognizing digits and numbers in\n",
        "natural scene images). SVHN is obtained from house numbers in Google Street View\n",
        "images.\n",
        "\n",
        "# Overview\n",
        "The images come in two formats as shown below.\n",
        "Format 1 : Original images with character level bounding boxes.\n",
        "Format 2 : MNIST-like 32-by-32 images centered around a single character (many\n",
        "of the images do contain some distractors at the sides).\n",
        "\n",
        "The goal of this project is to take an image from the SVHN dataset and determine what that digit is.\n",
        "This is a multi-class classification problem with 10 classes, one for each digit 0-9. Digit '1' has label 1,\n",
        "'9' has label 9 and '0' has label 10.\n",
        "Although, there are close to 6,00,000 images in this dataset, we have extracted 60,000 images\n",
        "(42000 training and 18000 test images) to do this project. The data comes in a MNIST-like format of\n",
        "32-by-32 RGB images centred around a single digit (many of the images do contain some distractors\n",
        "at the sides).\n",
        "\n",
        "# Reference\n",
        "Acknowledgement for the datasets.\n",
        "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng\n",
        "Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop\n",
        "on Deep Learning and Unsupervised Feature Learning 2011. (https://research.google/pubs/pub37648/)\n",
        "\n",
        "http://ufldl.stanford.edu/housenumbers as the URL for this site when necessary\n",
        "\n",
        "# Downloads\n",
        "\n",
        "Refer to Olympus for project related files and instructions.\n",
        "Data Set:\n",
        "● The name of the dataset is SVHN_single_grey1.h5\n",
        "● The data is a subset of the original dataset. Use this subset only for the\n",
        "project.\n",
        "● Keep a copy of your dataset in your own google drive.\n",
        "\n",
        "# Project Objectives\n",
        "The objective of the project is to learn how to implement a simple image classification\n",
        "pipeline based on the k-Nearest Neighbour and a deep neural network.\n",
        "\n",
        "Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FatALKmAG_Tp",
        "colab_type": "text"
      },
      "source": [
        "## Connect to my drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjKg-g6_z9nS",
        "colab_type": "code",
        "outputId": "86e48436-ab3b-4037-b976-99168e81189b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcGnIhj9AAX2",
        "colab_type": "text"
      },
      "source": [
        "# Data fetching and understand the train/val/test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIPmFcLt65co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Open the file as readonly\n",
        "h5f = h5py.File('/content/drive/My Drive/Colab Notebooks/NuNetProject/SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training, test and validation set\n",
        "X_train = h5f['X_train'][:]\n",
        "y_train = h5f['y_train'][:]\n",
        "X_test = h5f['X_test'][:]\n",
        "y_test = h5f['y_test'][:]\n",
        "\n",
        "\n",
        "# Close this file\n",
        "h5f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1vwYC23Au-O",
        "colab_type": "code",
        "outputId": "c0c6bc5f-1b1a-42c1-fedc-9da745b796aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "print (X_test.shape)\n",
        "print (y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 32, 32)\n",
            "(42000,)\n",
            "(18000, 32, 32)\n",
            "(18000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DJLL0YAE91C",
        "colab_type": "code",
        "outputId": "89ad5291-d88b-467e-ab93-5285df6b5cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Flatten X Train and X Test\n",
        "X_train_flat = X_train.reshape(42000, 1024)\n",
        "print (X_train_flat.shape)\n",
        "\n",
        "X_test_flat = X_test.reshape(18000, 1024)\n",
        "print (X_test_flat.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 1024)\n",
            "(18000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSQ2SI7ADvM",
        "colab_type": "text"
      },
      "source": [
        "# Implement and apply an optimal k-Nearest Neighbor (kNN) classifier (7.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-yLXPt87ICY",
        "colab_type": "code",
        "outputId": "75685899-9589-4467-8593-447bf0ccce11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as pl\n",
        "Nc = range(5, 40)\n",
        "knnElbow = [KNeighborsClassifier(n_neighbors=i, weights='distance', metric = 'cosine', n_jobs = 50) for i in Nc]\n",
        "knnElbow\n",
        "score = [knnElbow[j].fit(X_train_flat, y_train).score(X_test_flat, y_test) for j in range(len(knnElbow))]\n",
        "print(score)\n",
        "pl.plot(Nc,score)\n",
        "pl.xlabel('Number of Neighbors')\n",
        "pl.ylabel('Score')\n",
        "pl.title('Elbow Curve')\n",
        "pl.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6113888888888889, 0.6208888888888889, 0.6264444444444445, 0.6285555555555555, 0.6312222222222222, 0.6348333333333334, 0.6371111111111111, 0.6392222222222222, 0.6396111111111111, 0.6429444444444444, 0.6433333333333333, 0.6411111111111111, 0.6442222222222223, 0.6436111111111111, 0.6438888888888888, 0.6438333333333334, 0.6447222222222222, 0.6450555555555556, 0.6444444444444445, 0.6449444444444444, 0.6467777777777778, 0.6475555555555556, 0.6478888888888888, 0.6468888888888888, 0.6478333333333334, 0.6474444444444445, 0.6479444444444444, 0.6471111111111111, 0.647, 0.6461666666666667, 0.6465, 0.6461111111111111, 0.6450555555555556, 0.6441111111111111, 0.6438888888888888]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUfHE4ZxzLiqeOJxzzkXl/wOiHCue2ad+xwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3juKxraDxp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KNC = KNeighborsClassifier(n_neighbors= 15, weights='distance', metric = 'cosine', n_jobs = 50)\n",
        "# Call Nearest Neighbour algorithm\n",
        "\n",
        "KNC.fit(X_train_flat, y_train)\n",
        "pred_train = KNC.predict(X_train_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SejeHwfibmY9",
        "colab_type": "code",
        "outputId": "9f4e146c-069c-4f65-d470-a5b4f76a75a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "mat_train = confusion_matrix(y_train,pred_train)\n",
        "print (\"Training Accuracy ::\")\n",
        "KNC.score(X_train_flat, y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy ::\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01N4Yeu9CSRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_y = KNC.predict(X_test_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Al0AdTwCTPP",
        "colab_type": "code",
        "outputId": "86adf33c-122d-4934-8b73-d079a6cec817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (\"Test Accuracy ::\")\n",
        "KNC.score(X_test_flat, y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy ::\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6433333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8BTKfTUAJaA",
        "colab_type": "text"
      },
      "source": [
        "# Print the classification metric report (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDd6MEoFCekO",
        "colab_type": "code",
        "outputId": "7142a922-95d8-44ea-af87-210bd4732d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"KNN Metrics = \\n\", metrics.classification_report(y_test, pred_y))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN Metrics = \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.74      0.65      1814\n",
            "           1       0.58      0.79      0.67      1828\n",
            "           2       0.76      0.62      0.69      1803\n",
            "           3       0.60      0.54      0.57      1719\n",
            "           4       0.73      0.78      0.75      1812\n",
            "           5       0.68      0.51      0.58      1768\n",
            "           6       0.62      0.56      0.59      1832\n",
            "           7       0.74      0.75      0.74      1808\n",
            "           8       0.55      0.55      0.55      1812\n",
            "           9       0.64      0.59      0.61      1804\n",
            "\n",
            "    accuracy                           0.64     18000\n",
            "   macro avg       0.65      0.64      0.64     18000\n",
            "weighted avg       0.65      0.64      0.64     18000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFKvuLHuAKjR",
        "colab_type": "text"
      },
      "source": [
        "# Implement and apply a deep neural network classifier including (feedforward neural network, RELU activations) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7_MslqeAJx2",
        "colab_type": "code",
        "outputId": "f6a28fbe-b17a-47cc-dc7b-54a347e0d366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        }
      },
      "source": [
        "# visualizing the first 10 images in the dataset and their labels\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 1))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(X_train[i], cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "print('label for each of the above image: %s' % (y_train[0:10]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAA9CAYAAACpzLMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO19WY9c13Xuqnmeiz2ym02ySYoiRVOU\nRFmKHMpxpMiCE8d2DARQXhIDmZCXPOYtec0/MBAEyYuTOEjiAE4kJZBki5pMmoMoUZQ4dJPsbpLd\n1V1d8zzch8L6+O2j011Vuffi4hJnvahUrD5nj2uv/X1rcPX7fXHEEUccccQRRxx5lMX9/7oBjjji\niCOOOOKII/+3xTF4HHHEEUccccSRR14cg8cRRxxxxBFHHHnkxTF4HHHEEUccccSRR14cg8cRRxxx\nxBFHHHnkxTF4HHHEEUccccSRR168u/3j1NRU3+VyiYhIOByWqakpERGZm5uTeDwuIiKtVks2NjZE\nRGR5eVm2trbw95lMRkREpqen5dChQyIicuzYMYnFYiIiUiwW5e7duyIicu3aNfn8889FRKRQKIjb\nPbDFfD6feDwevPerX/2qiIg899xz8thjj6FtGl6/sbEhn332mYiI/OEf/qFr2AC8/fbb6KPb7cZ7\nPR6P9Ho9fFZpNpuiv3e5XBiTAwcOiNc7GM5er4f28O+73S6e2e/3jefqewOBAD53Oh1pt9v42y++\n+EJERCqVCt71wgsvDO3jD3/4w34wGBQRkXw+LysrKyIyGGd9vtfrxTNbrZa0Wi20X/vicrnE5/Ph\nudqXer0utVoN7ex2u19qQ7fblU6ng76GQiEREUkkElgn2WxWstmsiAzmVH/zgx/8YNc+/t3f/V3/\nwYMHIiISDAYlHA6LiEgymcTnYDBotJ3nR9fsxsaGbG9vi4hIu92WaDSKdmlbXC6X6FhGIhHx+/14\npo5lq9XC83ksXC4X/r/T6WD+v//97w+dwx//+Md9fWav18N493o9CQQCIiLi9/uNNdhsNtEunatO\np2Osa1772v5qtSr1eh190edsbm7KjRs3MFb6m06ng3GYnJyU+fl5ERHZs2cPxvCv//qvh/bx7t27\n/Xv37omIyD/8wz/IW2+9JSIi29vbUi6XRUTk2WeflT/6oz8SEZFTp05hPEOhkCSTSTxL+9jr9dDH\nTqeDeWm1Wvg+FAoJj60+s9/vy61bt0RE5I033pCPPvpIRES2trYkn8+LyGDt6zgUi8WhffyLv/gL\nzGOn05FGoyEigz1dqVRERKRcLuNzrVbDb+r1ujGn+hy/34917vP50K9Go4F10ul0oFf498FgEHNn\n3bu6Pj0eD/72/Pnzu/ZxY2Ojr/us3+9jz+t7RQZjrM9uNpvQNcFgEO/pdru2n6vVquzZswfPWVtb\nw7t4T6hOiUajUq1WMWb6fSgUkkKhICKDta9zuLCwMHQOz5w509e1lslkoFdYR9+7d082NzfxN3Nz\ncyIyOP8ef/xxERGZmZmBvnG73di7vC/7/b5xZug+2NjYkDt37oiIyPr6OtZIOByGDs1kMoYe0v7+\n9Kc/HdrH3/u93+uvrq5i3HS9NxoNzAWfbYlEAu3nOa3Vamh/KBTCWHk8HuF1otJut/G3vH4CgQCe\nMzs7K9/97ndFROTll1+GLcJtW1xctO2jg/A44ogjjjjiiCOPvOyK8LjdbliOCwsLcubMGRERefLJ\nJ2Fl1+t1OXfunIgMEBtFe7LZrDz11FMiIvL888/Lk08+KSKDW59a+m63Gxbr2bNn5e233xYRkcuX\nL+M5nU4HN7eTJ0/Kb/7mb4qIyJEjR9DOQqGAW+7Ro0dldnZ25AFgNIYRDJfLZViejOroLajZbGJ8\nqtUqbv6BQMB4plqsbNXq/4sMbs6KdHU6HeOmp7eE+/fvG9+rtf7CCy8M7WMoFMINJpfLid6iV1dX\ncWNkYcu62WwaKJNa0PpfEfOWG4/HDdSDEQd9jsvlwvf1eh1j5fP5gKSEw2FjrHYTRs74Rs9z63a7\n8dnj8RjvVMSRkYput4u2pFIpPLNSqWBsqtUqPnNb/X4/nl+v1zHPvV7PQBAZ4RsmbrfbQGl0LPm2\nHAgEsA9cLpdxS9R2ulwuPIfH2+PxYA7r9bqBROnfcvv1Gfq32n+3243f841rFOl2u2gzz1E4HEZ7\nGHnltgWDQXz2er3GftU9WqvV8Jx6vY41WKvVMO9+vx9j2Gq1JJfLiYjI559/DgS63++jX+1223YP\n7SRerxdt4Dni+W2328b4282Lx+NBOyORiEQiERExb8KNRsPoo+qher2Oz4lEAn3Xtmgf7do5imh7\n/X6/oS+0LY1Gw0CP+DOjHPp9p9PB+8PhMPRgPp+X+/fv4/mMtOj3PDa61vU3+v+8FkaRWCyGM2Zm\nZgbPabVa2JciYqyLVColIiITExOyd+9eERHZv38/0NB+v28gkbpm2+22oX8V0fL5fDgD8vk8ft/p\ndLAukskk0J5ms4kxGUVeeeUVoIArKytAN2/evInx57OwVCqhncFgEKhLOp2Gfg0EAvh9uVwGulUo\nFHB2BoNBzBezAoz8VKtVo78qvGZ3kl1nOR6PY8FOT0+DQtq/f78kEgkRGRyg+tJWq4WFs3//fhzG\nzz77LBbFrVu3AD0eOHBAJicnRUTkm9/8Jga40WhgMkVEvvKVr4iIyEsvvQTDqdfryYcffigiIu+8\n8w4m9uWXX5aDBw/u2mkWt9uNjcWQLivxVqsFw+PGjRuALbe2trC4Dhw4gLY988wzsn//fhEZbDId\nQz54+v2+lEolERH5+7//e/nXf/1XtIkPDJ3YRqNhGFHjKKBkMonF0uv18N5isWhA3mwc8AGuf8uL\ny+v1GoeZ/j4YDGINWGky/Xs+qLxeLzYKK/p+v28YnLsJ/x0rHTZ+RB4amDx+bJz0+33Mp/ZFZACL\nqxJ3uVwGNccKTsfA6/UaY6lipTGHbU6r2MHc3W4X7/J6vWiz2+3G8xuNBj5ze/1+PygnnqtarWaM\nlfbB5XLhezYg3W43Dk0+OJgeGkV8Ph90QDqdxjMrlQr0zfz8PH7TarWgTF0uF+YoFAoZh7uu9wcP\nHsj6+rqIDC4Z2rZoNAo9tLi4KAsLCxgrhfJLpRIOsGazibb1+33jIB0m1vHcSbRtvV7PoJf1vaFQ\nCGMSj8fxORQK4bmtVgt9L5VKuFxWKhU8kw8VviD8T4XXpogYFycdJ6/XaxhZvF+ZvmF9wS4OemFb\nW1vD30YiEUmn0/i90tTFYhGHbyKRwDNrtRrW6p49e3CIjyLJZBKGysGDB7Hn2CCt1Wo4M9jw570S\niUSM/cqi+5Lng/dToVCAYeDz+aB7EokEjLEDBw7IgQMH8HvWbaOIghrpdBpryuv1yrVr10RkQKXx\nmaC6IZvN4sw+ePAg9pbIQ/2zsrIiFy9eFBGRq1evGsaSzku32zX0ie6/Xq9nXGJ0HLxe79BLskNp\nOeKII4444ogjj7zsivD4fD6Znp4WkYG1qBZ0OBwGMvD555/LhQsXRETkzp07sLaOHj0qx48fF5HB\nrePTTz8VEZEf/ehHsL6/+93vyre+9S0RGTginTp1SkREvvjiC8DHLpdLnnjiCRERefzxx2HxbW5u\nAml5//33ZWZmRkQGFqWiK6OI9YbPULXC2RcvXpSzZ8+KyAChYuhZ5fXXX8ct6/Tp0/IHf/AHIiLy\n4osvGjdS67tFBrcv7Qt/3+12DfqPIVK+RQ0Tn89n0EyMIKlDX6fTgXWs7dXfsHXPN3x+pn4fDAYN\nWoIRDUa3GMlhhJCRCOt47STFYhFrKhwOAz4Oh8PoC98K2u025jkcDgPq3dzcBFIYCoVwIy6Xy5jr\nZrOJm1IsFsPtUeQhlN9oNGxpAo/Hg/6Ni+7wfDPK1G630Uev12tQWjr2/X4fa4cRiWg0ivazw3Ol\nUsG6YLqQHbbZOZad/fUd+t9xkMhoNIo5z2azuOXmcjnQ2nNzc0B1RB7SFN1uF+Pf7XaBbORyOaA6\n169fl+vXr4vIgM7VOY1Go7gVf+1rXzP6qGOSzWahYwqFgjG2vAaGCVOK1n3At3l2NuXf6Pyys386\nncb4RKNR/KbdbmMet7e3oc82NzelWCziXUy5cztVeN8Pk9XVVcORXPdQtVrFOM3OzhrIhp1zMq93\nXoMiAoQnl8shaCSZTAKR6Ha76He5XMZeYV2v/y5iUsSjSCwWw1k4OzuLM69arYoGT4TDYcMxW9dj\nuVyGrtra2oLu8Xq9WMuBQABrn53KWef6fD4j0EXHJ5lMYkwWFxfBymxsbBhozDD5p3/6J1lcXBSR\nwZ549tlnRWSwD1SXbGxsGOtE9e7Jkyfl6aefxvjovNfrdcM+UGovHo8DNapWq8ba5L7vhC4zDTqM\nQt/V4AkEAvAoP3XqFBoYDAaxSQqFArzFK5UKorGOHz8OPxte1OVyGcbP4cOHQXvt3bvX8GT/xS9+\nISKDTasQ8/T0tMEDK9x89+5dTHg+nx+LU6/Vagbfrwttc3NT/vu//1tERD788EMszGq1avD97Bei\nm/u//uu/ZGlpSUREXnvtNfn+978vIoMIFqYf9L2ZTAYKq9VqYeEzPBwIBLCQU6nUWAaP1ceFIWym\nqxg2VqXpdrtxoMbjcXzPSpDpEPa94PeyguHDvtvtwhBhOop5+2FSKBSMcWLFwcJjr7/nzcOUE9N6\nVn8GHTM2yKy/1/HgQ/9/hy7g6CorpaXi8/mgULh9HOnocrkwn6FQCMaD3+/Hs/igZ5iYKadAIGAc\ngnZ+Nf8TikTftXfvXly2VlZWsD8WFhZw2LAhzxFJd+/ehWFz48YNGDx37twRjTxhoyUQCOB71gFH\njhyBkfPkk0/ikC6VStgriURirL24k/BY8WemnDiyLxwOY0zS6bRxQOrcMa0ZCoWwNhqNhkEp8yFq\nZ8Syrhoma2trhk5UqogNHj64o9GooX/5sqTicrkwt4VCAQZPu93GXrf6cPE5ob4ik5OT+N7n86Gd\npVJpLB+eeDyO8U4mk0Z0oPaR6apIJII1cu3aNfinXr16FfuvXC6jPZFIBLr+9OnTcJUIBoOG/xJT\ne2rsHT58GOf07Ows3js9PW24iQyTDz74QG7evCkiA93w27/92yIyAD7UEPriiy9g4Ik8pMCOHj2K\nyz/vv3q9jradPHkStJff70c7L168iPObI5at/q+6rpnGt9KpduJQWo444ogjjjjiyCMvQxEeRWwO\nHToEq42difx+vxEpoRbc9PS0EfE0MTEhIoObkjpzVSoVwKz1eh2W8v79++F4XKvVYDkGAgGDXmFK\nhSMDxoEnG40G+uXz+WBBnzt3Tt555x0RGThnKVowOzsL6C4Wi+HGWyqVkN+mWq3K7du3RUTkhz/8\nIW4Yf/qnfwrKhB30zpw5g74HAgED7tU+BoNB3GYikQi89UcRa2SL3e2x3W4bUL7OVzqdBnQ+MzOD\ndjJd1e/3DSdOtdCr1SpQrwcPHhiRS4rqiIjhOK3tHMUBTWVzcxNtD4VCuH0xNdfv9w3HaoZ3+Z1M\nzekzu92uccPk52hfQ6GQgaLwLdvu1sH01ihizeekn9lhkdEtr9cLhCcUChmop10kjBUO1vHhNjJ8\nzDlTmH7kz+NSBdzHqakpODsyEjU5OWnr0Nlut7F3b9y4AQr68uXLQIIZze31euhLp9PBfrp48aKB\nECq6/MILL+BzvV43UCy9CY8iVvSGUR1G7RjNY2dp/X0gEABiE4lEDDSTEVOmcdlRX8fQmpNHx4QR\nnXGQusnJSUMXKJWztbWFz+vr65jbZDKJd1qDFhi1VXTi1q1b0PXZbBa6OxKJGI6vOjZutxvocqPR\nAALDUbhWtHKYNJtNtIHbrM/S99pFXVlpemUOisUi9E06nUb7mZZkp2WOmLRSjuwiwNGT40RMKooq\nMkBMFaVZWFiAw3YymcTZ1u12cTZwlPTHH38MuqpWq+E3/X5fvv71r4uIyBNPPAG0anl5GeudaXa/\n34/vOdcaR/+OIrsaPJFIBA/m8EgO3eSDKRKJYAGm02mDBtBDc25uDputXC4bC0cXYDqdhmFQKBSM\nCBD9fa/Xw3N4wXKo3CgSDAaxEYPBICb23Llz4Fp9Ph/e9fLLL4OGm56exmLM5XLy3nvvichgklUJ\nVqtVPMcaeqp9YX8nkYfGm3WBah9rtZr87Gc/G7mP0WjUOAgZNrbziA+Hw6AXT58+DeNzz549tv4c\n3W7XoD10wxUKBVB7zWYTBiH7MXAoOPfZmrRsN9na2jLoVp2rWCwGQ6XVahnREbpJOPpG+y4yoCo4\nFF3XyL1796CMgsGgEd5uF+3Hxk+r1TKMhHGEE+VZQ9FZqWkfOXyXfdNYEbPBw2uNE0+ygcyHICdd\n5L70+30jqmicyBCmGcLhMIzuZDKJd/GFgKlDr9eLQ+Xq1atQsg8ePMAedblcRlSXrkE2DO7fvy8f\nf/yxiAwuXseOHRORgbGvl79erwcK7NNPP8WlbRSx+qOwkcNG1E70JUc96brlKLx+v4/54HQRjUbD\niNTU+Q6FQrjARSIR45Kh76rX6yOv18OHDxvuC6rHmTasVqugCvlg5bFhqrnZbOIC+fnnn2Nd7N+/\n3/DXY18kXSNMl/D4dbtdw0hQPaG6YzcpFAowosvlMvrA+8/j8RjzqfPg8XhgJLB+q1QqaHMsFjN8\nDDlamC8W7MOjf9vr9bDemXplH5hRhNdXsViEwRkMBmG09Ho9GCrpdBrPj8fjmPdjx45h3vP5POi8\n69ev48ybn5+H+0sikcBv2Jhhg5uTjLZaLfQ3HA4blL6dOJSWI4444ogjjjjyyMuuCE+n04EFd/Pm\nTUQ/zc7O4rbHiZHK5TIs5X6/D2vL4/HghuzxePAba84TfSYnu2s0Gkb8vVp6TIkwfN9sNkemQkTM\nW2u1WkV02O3bt42U/eql/mu/9muA7BhJymaz8tprr4nIAP5+/fXXMYY/+MEP8Bumb9hRjhONsaOW\nXTTOvXv34FD953/+50P7aBf9IWLeSDgCJBKJwEI/ePAgnOYSiQToyG63i/ZwdAX3ye/3Y/10u13D\nIVmFHQz535jeGia8FrRf/Hz9jj37OT26/j0n4otGo/h9sVgE5XHr1i2MTSwWw7xZEwNyoj8V/jwO\nvCwyuNXwLV5vUz6fz0h4yZQErylO36/7MhwO20Yk8ZwwwsN7lPcir1nef7VabSyEh+eRqRa+dTMi\nFwwGjVsr59tR1IXRR7/fj/nlm3C73TbWie5RTnCWTCaBJrTbbayHjY0N6IxRhKOxOp2OEbXHgQV2\n1AVH59XrdUSzMF2rzxL5MqWl48bRkIyGRKNRAz3j5G6j7kWPxwP0Y3NzE8i+3+8Hvb25uYn8MNPT\n08Ycci4r/X5rawu3/jt37oBSmZiYMPrN+YcYybFzSGbdN06AhLZfqah8Pm/kmVHhc5GpJW2TvpeT\nRPI+1mdGIhG0v1wuYwy5ZAOfE6VSCTo6l8sZASfjIDy8Bhk1Z73FOeY6nQ6Qq83NTZyRHMXGVGCx\nWLR1X+AcZto3q7A7Czs2W9tnJ7saPLlcDvCu1+s14EmGOPUllUoF4dXnz583EmOpAlpdXYVi4jok\n1gRD+pu1tTXQTO1220gypBuSqRnr5h8mvV4Pg7e1tYXoDs4cmUgkkDU6FAqBKuh0Okb4ri6u5557\nTp5//nn0RaNN2Hel3+/j99YIJvZ9UvH7/Ti03n///bF8eCqVilH3iJUp+4JwtkuOfuAoDqY4OXye\nwz118xWLRYyVtQ120Rj6LJVRlRD7n3CmX6v/CYe8cmI6/T37Qvh8PszV5uYmIhGXl5dxIMZiMVBp\nsVjMMNiZ0lLhLLvjCoeh8ryxcWI1HnmeeRzYAGAq0u457GvEdBg/n6PYrAfrOKGwfHGxpmTQdXT9\n+nUcBjMzM4aBrAqX63yxIT85OYn5qtVqyG6+tbVl0JF6YOTzeTwznU5DH6yvr4MyO3/+PHTGKGKN\nuuJUDWzwcOSg6jy/34/xLxaLGHP2/wiHw+ivx+MxMmmr8CWSPzPNwzToOH4SPP+FQgEUYiQSwdx+\n9tlncvr0afyeaSDWNTr2y8vL8sknn4jIIEro8OHDeJ9dZKT2S2Sgd/Si7nK5MLfZbNao2TSOwVMs\nFnGeVSqVHaOC7fYf68pGowEDptVqGRFe+pmz5LPhx3UQmZ4tl8voVyQSwdkjYm887CQcLTwzMwPD\nlVOZ9Ho9Y371TDp79izGJJfLoZ2xWMwAMljf6PhzzUP2PbX6V3KkqV3KlZ3EobQcccQRRxxxxJFH\nXnZFeLa2tnCzmpubM6AmtV4DgYARuaOVyv/zP/8Tv43H47ghv//++4A8Dx06ZNBhaslypdmlpSWk\noH7qqafg3JROp2G9cj0Tn883lkMoQ5uNRgMUDCMhqVTKSC7G1IhauOx97/f7DUc2rhyrbeN2clJB\nax4bfqdWbv7ggw+M5EzDhMfDmgxQ3+X3+43bFTsM6hrg2mccPcJJ7nK5HNrNVaWLxaKRhEwtd3Yq\n5DFvt9sjowPsYFyv13EDsaJTnFZenfA4BwfXfeHkZYVCAbfNra0ttDESiRjQM0eG8O34/0SUlrWu\nG9OefFOyK59h/Y1dPa/dnmlXCoGpFu6vtfzEOH3k57daLYxbs9nEOjp//jzGPJVK4abXbDYxp5zT\nhBFfpmd1LYo8pMJEzKrSrP96vYcV6m/cuIE8YVeuXBmrLAGLld6yi9gSeUj9BwIB6BJGTMvlMtYt\nI8eRSATzwrrHmj+JaTV2Vtf+sqvCMOl0Ohg/puE5/1e5XMbeYiSBUcZWq4Vgj+vXr2NsksmkUQ7F\nuia1Hzz/TN9xkAZX4h61fyIDfaD6t1qtGnmHtC9McVuDGBR9ajabWId+vx8o3cTEBBCVcDiMecvl\nckAlc7kcxpmfv729jfW+vr5uoHfjlnlR5/B9+/bh/CsUCmhDvV7H9+FwGPP1zjvvIECl3++D/nO5\nXPh9LBbDuuYcWhyByhQuCydmtNZcHJZPaagPDytuLubG0SlMA+igXrlyBUYOH/oPHjwwathwA3UT\nrK6uGtlvlSa7dOkSFkI0GgVUeebMGQzA0aNHh3pqs/DmyOfzWERMjUxOTuK9XFCQIx/K5TIMP2s0\nCytuLtzIPi0M3zMHzz4Zavh98sknhsIeJhxizX4bfr8fm4/pina7bVCKDDHqomYImakupjFyuRwS\nU3F2Vw6RZR7bCuWP6jcQDofxdxz9xona9PkiA+WvbanVauhfMpk0IuF0E3KxxXa7DWM8FAohmm12\ndtaIkNLPnOCON7P2cVSp1+uGgaFiNVTsns2RYmzA7BSqai1qqWL9vNNz7OqIjSKsbzqdDpQ1p69o\nt9tIhnr8+HHs+1qthj3BzxF5mK11YWEBBk+pVAIEf/36dWNfqrDhFA6H0Z67d+8aUZjj9JMLajK1\ny/421sKsvLbtkl6yD1K32zUS3qku4UgrTq4Yi8WMYo1qWFarVejCXC5n0O67CadwaLVaGDOrvwr7\nJmq7eGwajQbm/NKlSwa9wgkMeZ1bffl0LNn1gSNLOfpQdcYowmtzpxQR3BerywWPFUdmaeTa/Pw8\nPkciEcxDo9HAmuVi0pxpmQ3zWq1m0KfjAAELCwtw4zh8+DDW0Y0bN+CzViwW0R9ej7VaDZFZrVYL\n7XG5XEbNOh2r1dVVw5VEhddMu902Umiwa4sK02E7iUNpOeKII4444ogjj7zsivBYayexlWp3++bU\n5P1+H9YoQ4/dbhfOz1NTU3Bs9ng8sGTv3LkDhKfb7eI5b7/9Nqz706dPGxVZ1XqdmJgYy5JlK7JW\nq8GSbTQa6HssFsPN5+OPP5YbN26IyMAa1ZsBV99Np9PI33Hq1ClYynv27DGgf064ZQc3u91u3ABu\n376N3DvFYnGsW2UikcDzk8kkxpxzVzC0zWU7lpaWDKpAEZtut2s4Nusz9+/fj+fkcjmgIcVi0XAS\n5HXFibL45jSqLCwsGNC53vRDoRBoz0gkYiQP1H50Oh05evSoiAyoV85hocjitWvX8LdWyFTfdevW\nLazx+fl53L6s0RnsFDqOE2G9XjeQNkb+mLpiWoTHkBN4MSKrbbBGBtnRK5zenR2bOSLMmnhw3Jpv\nnKBN9xajDY1GA46bzWbTyEujc+TxeAwEUffQ7OwskgdyBBGjXjw+1tu7tuHevXvQT1xrbhQJBoNG\nbhGmzHjMVRjFsNISnItJEZt4PG7QDDom7Hjs9XqxXycmJoBMcw2yRqOBz9vb2yMjytboIU76x3mw\n7Kglpvi4LtXt27flq1/9qogMzgxGk+2QjXA4bMwJoxz6Xh7vZrM5MoKlwvmC2DGc97RdJCUzJawP\nkskkdNXExAQoSo/Hgz3B+X+KxaKBXNmtEUa7OUp2FPnGN76BM2x2dhZzce7cOeRE4vYXCgXMSywW\nM1B2XV+RSAS1vQ4dOoT2f/rppwgCyOfzhruMtpnRMGtdLdapw/biUErLGsYs8mVvb058pv/WbrcN\n7pmzYKqimZ+fh69LvV5HRNiVK1eweP1+P/wnVlZWwB8ePnwYg9dsNvGbRCIxFlXASejY8GB48saN\nG6CTlpaWDL8HFv3+/v37cvXqVREZ8JmvvPKKiIj8/u//PsL1mIpwu914L2eA5U188+ZNuXTp0pfG\ndhTxer3YQOFw2OA/OTRXN3G9XgeczIbr+vo6uFmRh+HCqVQK86iKVMQ0eJgj5+ggay0rVvqjHiTH\njh1DP7iWWrlcBg/NPkdbW1vYkFy3JhQKAYpdW1sDzLq8vIy5mpqawoHOobZ3797FGBw4cMBQxDvJ\nOAclJy3ktWONzNoptJkPbk7kyfQK/61d+62QPRdItUtqV6/XoaRGEY6e44MhmUxiXrg+E1+8OAKS\no1Y4apAjOri2lDXKg/Uch2bb0ZHj+u+wT0av1zPoZYbs7dwHRB7qWk6+ls1mQa1OTU3h+06nY0SU\ncoJYpUymp6eNWlBq5HBttVKpNHIh31wuZ+wt7UepVIKODoVCRuJaTnWgY5PP56F3Op0OzoxEImEk\n9NPPVnqLfSLt/OnYcORoqVGk0WgYKVe0v3v27DEiV/V762WFaRpOnKgRhPPz81gXnFiPE/d5vV4j\nYtbOb87n88H4Zf+sUeSFF4bXdqwAACAASURBVF4w2qaG1sbGhnFmMCWrfWTfpEAggNpbJ0+ehBtK\nNpvFGbO0tASDyhrRxhcXNowZWOEo0mHiUFqOOOKII4444sgjL7siPHzLYuuYHVy5pg7fIti7mm9Q\nPp9P9u3bJyIDBzS9nX722Wfy5ptvisiAQlAL3efz4b3BYNCwWBVFuXLlCpCEZ555BingRxG+nTJy\nxTlBlpaW8Jl/HwwGjZuK3ga55lc+n5ef/OQnIjLIO/Ttb39bRAYwIY+RXR0jr9cL59pf/OIXGBOu\nID+KNBoNW9SLo6JEHt56OGIun88Dst3a2gKU7/V6cQPgpHiFQgHjc//+fSNfhQojBYxuMcpkHZPd\n5NChQ3Ck8/l8xs2K67bp7Wh5eRnvyWQyhsO4jvft27eBKpRKJay72dlZ3HYY9hd56HTPVctzuZyR\nM4dvmONQWiw7JTPk24410odRC0aKGFHjm7Md+sTOyYzSMaXV6/Xw/Th0lv6eSx7orT4WixkooJ2j\nb6PRwDqq1WpGf5mKsnNs7na72Fvs6Mu5aFhCoRCQzEwmM1auoWg0aqBYui84sICpXe2DiJmLKRwO\no8bgvn37cIvOZrNYz6VSCX3nZG2xWAxjOzExged4vV4jTT/r8lGRus3NTfRvenoa+v3u3bty+fJl\ntFF1CpcD8Pl82H+rq6uISs1kMkDGfT6fgVTY0UZMzXm9Xjyf55yjCWu1mrGPh4k17xevI0bpdhJO\nSqrIN9fPisViRuSw6tC1tTXb/HdMNTOdy0FE4+b/euONN1BC6fDhw0gU+dJLL6FtH330EdYvI2/N\nZtOgLxVBf/zxx4HwdLtdBDVxrj1GPWu1Gp7DeeiYnqvX60YtuGEy1IfHLnul/ps2hEMr1QDo9Xr4\n7HK50MB9+/bJqVOnRGQAv+pk/uxnP5OPPvpIRAaHhFIwXq8XmTVffPFFcLnBYFDOnTsnIiL//u//\njsErFovgQtVXaJhwciN9b61WM8IN2e9BPc2PHz8OysTv92Ozrq+vY7M+ePAA3//jP/4jxu173/ue\n0T47CoHr4liLUz7xxBMj9U37ogu+2WwaCpf9ZzhagjeILiTOcsu+DuFw2KAmtb+cuM1KaTEEa5fM\njmmbYZJMJqEsisWikVSN/TrUWLt9+7YRLaBz0mw2Aa3evHkT/eAEg9PT09ic6+vrRkQd89aq0K2U\n8DiHIwv72zB8b627ZMflW3luVhy8d+3WIM8BR0rwgcF+RCxsUI0iTOdms1mMOe8TjvqoVqvoI+9d\nfq/P54ORs7y8jPllKoKNpXQ6baxV9rfQQ2Xv3r3QYZlMZqzDJJ1OG9FEnHlWP3OmeTYCObUCU1oT\nExO4AOnhImKGLjMtxYnbuO4RR5Tu2bMHRlEoFBq5XlihUIBOzGQysry8LCKDIq7qjvDkk0/CyEok\nEjjU+IJy9+5d6PQTJ06AsmP3CKaH3G43+sTRtoFAwKDZORqP/UrH8eHhCxvTwiJie4m11nfkvcKh\n6JzKRPu4tbUFfbOxsWHobqamVdgY4+zKHPI/irz99tuG0aLjf/z4ccMoVT9H7p81sasauiKCfXPi\nxAm4pORyOTxzbW3NoKbZdYPpXAZixhGH0nLEEUccccQRRx552dXk4wgT9mrniC2v12tYYUzHcH0d\nvfFyZXCv1ysXLlwQkUE6arXu2fEqHA4j78bp06cBiW1vb8PKK5fLiJy6c+cOKIdRaJ9wOIwbIFvB\nXK2brenHHntMvvnNb4qIyJEjRwx4Va3NSCQCT/Yf/ehHcDZeXl6W//iP/xCRAUz43HPPob92yZY4\nadqrr74K5OratWu4/Y4i1WrVto4OQ7DRaNRI3sfIiP4tp7lPp9OgDufm5kD5eL1eICm3bt0C5Mm3\nC0ZvuGZPsVjE7TASiYxUuVj7obc7vgVvbW0ZXv6aQ2h9fR3viUajxu+VxmLn7Pn5efR1cnISY9Dr\n9Yy8RIw8cK4QFb6NjOOwrO3kpGx25QkY3mXHV55nRpnq9fqXypeIDMZeb7xch8uawJJrr3Fklgq3\nbRRhhMfj8QDB4GrpvHY4Ssfv9wM1iMVioC/591tbW8ifE41GsTYZrWI9l8lkgH5w/qonnngC47yy\nsjJW9EsymTTKy+ia4SjCdrtt3JgZ7eHbr84d1wiLx+PGWHFeHUZGGNnVz36/39jfOv5erxe6eZhs\nb29Dv6dSKXnvvfdEZIDY6JjNz88bCTvtokM3NjaADk9NTRk1uRjVYSpV1w6XP2DKzJq/iud8nHW6\nU0kWa4QWRzHyHOr68nq9GIfZ2VmMN9f3W19fhz4tl8sGassUDjvx8p6wKxc0ily/fh3jlkgk4OQ+\nOzuLXFZ3794F+pTP540ktjo+7XbbyMmjey6TyeA5IgKEp16vI/lvr/ewyjyvdy6fEgwGjXEeJrsa\nPAwN80ByeB/zh1YomTetGh/PPPMMDpuVlRV59913RWQAj9mFuUajUfDTBw4cMLzg+dDk6JFxsmY2\nm01DcXOEFCdf+8Y3viEiA6NLYWNryKgq/VgsJmfOnMHnv/zLvxSRwQJRiPfKlSvy9NNPf6k9nKGT\nk/tls1kUMJ2enh4rqkCfK2JCnuxrxBQI14LiQ7rdbhtQrm6gdDqNMeQQVqYKmG5h4QOJfb3GyQzK\niQ+txfb04OPD0ev1AqKdmJjAplpeXsaB+ODBA/gNzMzMGMad9vvevXtQygzjNhoNg2phmo6V4jhK\n1prYj/lyDuXm8VZhZWEtYMp7iH9vJ1ZYnP0VeK8w1TUOpcXGL1OmU1NTmAuPxwMjhP37fD4fLgF7\n9+6Fkq1Wq2gz6xuu/cP7uFarYS1NT08bNIP2kQsiplKpseiQeDxuHAZq1EejUSOEmA9F9lPiOkx6\neJRKJcNA5QSoOia8TprNJgwY9u1hqiuVSoEKcrlcBlW2mxw9ehRzVSgUcHG4c+cO3BEOHjxoJERU\nKrJUKsHgefDgAc6J48ePG5dbnYd4PI4x297eNnxJ2VDVd7H7BUfArq+vj6VPOSrKSjXbRdpZw6W5\nJpTO1dzcHIw6n88HnyL2m1SaTuTL0ZkcYs/1xbg949SYdLvdcuXKFREZGKg6d4uLi3Axeeyxx3CZ\ntyac1Pfy+r1//z505MLCAsLeDxw4IL/yK78iIgPDh41eLjTOLjJ2hhyfHzv2a+QRcMQRRxxxxBFH\nHPn/VIbm4VGxOl/apc22eq9zaYZnnnlGRAbWvd6K3333XViRlUoFFlqv1wONEo/HAaeFw2Hcalqt\nluEYxcnXxnFkYqdAq7WoN4PJyUkkD5yenjaQLo5s4rwVaqE//vjj8qu/+qsiIvIv//IvgO6WlpZg\n+TINY3XitfNMFxleM4SFb8Jer9c2gROjKZw6XcSEanWO0uk0orSY1tzc3ISFXqvVjPbzOLM1budE\nzennh0kymTTKQOitXOQhVNpoNAAfT05OAjXMZDJAdS5evIjIv42NDaCS09PT+LywsGAkOONaXYws\nccI3uygOa5mJYWK9nXGZAF2zjPBYHSX5VqnoRCgUwlro9/vGrUnns9lsGunjGfmzS1RorQ81zl5k\nZKzb7RqJPHW+mAbg9P1cGXpxcRHVtTmR2b1794x0/DpWlUrFoDf0Brtv3z7ML9eiYvQpFArtWC3b\nTmKxGOay1WoB4YlEIkZpCbtkklZknUs1aL9YF3IkV6lUAqrTbDahn/jmz7mVYrEY0J5MJjNylNbB\ngwcxlnfv3pUPP/xQRAbr9+tf/7qIDBxWtd+dTge0MFN2165dQ0JQpkhY53IUHZcvajabRlJGO2d/\nrhVWKpVGRrBETKTWSh0zOs9zyHW7+IzRz3v37kUb/H4/5ofzEW1ubhpUOVPczGrsFGE5zpkh8jCX\nUaVSAQLG9bDm5uagazc3N42gB0YrGWHTs//mzZugw+bn56Ffjx49Chq0VCoZOaL0+aznOGhjFER5\n1xHgP7YmI+OXcBI0Vu4qk5OTiCqKx+MovPfOO++AcmBu1prsjCfZLiKFaYNGozGWAtre3jZqQulh\nwAZVKpXChucQTTYEeGJrtRomPBaLgT7h4nLlchmTPzExYRsuWa/Xjc3EyRvt6sbsJKFQCO1hmoyp\nCKswJ6yfQ6EQoOJMJmMUVFXDIpfLGaG/XC/MTjFoO6yfx8kMGgwG0a5+v4+NxPWPWq0WNs+xY8cA\nl7daLfh/XbhwAVEH7K80Oztr+PxovzlShnnlYrGIwyUWixm0IRsG44Slc+01ppd53XHWWjac2Rjj\nrNiBQMDIAKtjxQZpIBAwDhgVa0I8Frs6XKMIRyUyNL+wsCCnT58WkcFeVz8+Dt/lg23//v34DadY\nYCrH7XZj7tiYDIVCiB45duyYQevonrPSreNEaUWjUWN/6xqLx+NGJnIVjthpNpugf/x+v+GvxYVE\ntZ2RSARrlYtubm9vw0DK5/O2dGcwGISxZI2W2U2CwSASdr755pug8F988UVQ+Nls1vCt0zXLdaMu\nXLiAiB6+WFgvZrquO50OxoALAgeDQaN/+rndbsNI2NrawqV6FGEjhy/2VtrKjv5lY5YTQM7Pz2O8\nt7a2DGNMjR/2t2L/JetFhxNq6v7mZI+jCK9NTofAPqtsvBcKBXwOBoNof6PRMFxGVMcUCgXsxYmJ\nCazNiYkJrFmP52HtM7/fb7ibaN9ZD41yXjiUliOOOOKII4448sjL0MSDikh4vV7jpsEWFlua7Cmv\nt+6TJ08i2WAul0O+nZWVFVh/DK1mMhnbhG4iYuTD0baFQiHcdlwu11gIDzubRqNRwIp8A+cbHSNa\nbNFz/Rav1wsL10rL6M2Ka28xvM40ANMS1qiCcar78i3dCgnzODBMyFEITIfojTeRSBhjzunP7fKY\ncMQOf3a73baV4rk9w4RRN4Z3e72ecXPQ5Fn79u1D/9bX1+HgWigUMIeZTAbUCTtlM/Jz8OBBjEG1\nWgWylcvljNpGdlEl4yYd9Hq9RtQgO1/a1WDiGybTmBz9xjfAVqtlS8/xc62QPaNV7PzMCM84MLrf\n7zdQFB3zffv2YcxFBMhMLBZD+7nu3+LiIurscd20er1uIH7a/lAohPbv378f0SNTU1NG9W6ORNO/\nLRQKiMgcRVhPcAQRU1pWKlDfy2VBuGZWKpUyInBYlzDKZ0fDMULBFefL5bJRz2lUtC6fz8NN4cKF\nC3D2/973voe9wlForGdbrRacnBuNBnRNNBrFfrWiw7zGtR+lUgloCedssdac0vEeN4JJxMyxw6Ux\n7NA+Rlv5nPP5fEZJEG0DU46FQsGogaZizXvDekXHwePxGCWFxukjBwKxY7s1UlOFkdFsNos9cePG\nDUM/cZQhB15w1Xh13p6cnDSCoHgt251P2u7dZFdtZB1Uu0R8XJOm3+9jYDweDxb7yZMn8beffvop\nKASRh5M4Pz8PRfPYY4/Br2Jtbc0IWeOwcT3Y2DeC2zmKcDbpTqdj1A9RyimXy4F6S6VSRqicKiNW\nCK1WC3Cdx+OBTwtHNgUCAeNv2BdIE24lk0lsXDa6xumf/p4PSzZsODyVn6vjHIvFjEzXvND4kOPF\ny9lgmabkcHu7Tcwc+zg+PPV6HXN1+fJlOX/+vIgMjBkdv2PHjgFSn5ychGJfW1tDG44ePQoqZO/e\nvYgiCAQC6Eez2YTB8JWvfAWHzoULF4x6MGpcWaOiVMZJrGgVVnBM87JRzIcjXwgikYgBE3NWZLsM\nzLzueA6tVJqK9XAcp49MsXU6HbQ5FApBcXMECB+W3N+ZmRmkstjY2ECbNzY2sP/cbrcRzaKU5enT\np+Ev1G63QbEwjcg19/L5PGibUYSjV7m//DkUChlGi/pJeDweGOF79uzB2gsGg5ivYDBorFU7SplT\nHOi7RUyjgaO6wuHwyAb67du3QSnPzs4i+mZxcdGIXNT312o1zMPKygoo5RMnTuD8YF250/piXczP\nZLqSjXTO0p3NZg1fkWEyMzODdcTRuXyxLBQKWI9bW1sGPah9/JM/+RP5jd/4DREZ6CRda9euXYPv\n09WrV3GZZBqzWq0ahrDuD32HyOByrWshmUyOBQR4vV7jQmBHJ9VqNZzB+/btg4/W/v37Ybi+++67\n8KdjQ5f3Qb/ft02nwXPa7XYN+prD0pleHKZvHErLEUccccQRRxx55GVoHh614AqFglGOnm/3agmy\nRRaPx3FDnp6ehvW6tLQExCYSiRj5LPRWtri4iPd+9tlnyENQKpVgWXc6HVh84XDY8NYfJ/qFo4d6\nvZ7hlKttvn//PsrXz83NwRpli5IpPx0LkQGCoJRJo9HALW5hYcGoC6ZW/E9/+lN56623RGRQt+Q7\n3/kOns/OxuM4SrIDaCAQMCgkFWulci4dohY0o3nc31qtZtSX4rw6jNgwgsTt59ubXUK0YVIoFOAo\nuba2hrXj9XoBi3Opgvn5eVlaWhKRwU1Zx0ZRGZHBelR6c2JiwohS0X6XSiVQlFx6IBqNYo1YS4Vw\nn8fNUcOIip3TsojY0kmMfnCEEe+V3fYNRwMxJcQUjJ0T7061qHYTbbPb7UY7o9Eo2s9JNKvVqhFA\noOt0YmIClBYnDdU1ou/R9RUMBuXkyZMiIvLyyy/L4cOH8UwVhtRLpRLW2MbGBpC9UYTRQo6KYkfx\nYDAIFJlrRCWTSSCQnAtIx0Lky1Q835B1bFOplKFLeK8resmoyjiRdr/85S+xJ1555RU4m7fbbaPs\nECNzOg/r6+tAtzmwwOpsa5evjRNkcnBIIBAwIrP0XeVyGfs4m80CgRlFGH22Bh9wkk7dH1wjMBgM\nArFLJpNG4IeeN6urqzjz2PWB0Qym0lhPWikzlXFZgWg0irXAiBbXgrt37x4+nzhxAnO9uLiIIKVm\nsym//OUvMTZKV2UyGehm1hG1Ws0YN50jK6XMkb36G6a6dpJdDR5rGB/7k7C3O3+vn7PZrBw8eBCf\ndcLn5ubk13/910VkYBTpYg4EAnLixAkRGQzwp59+KiKDTaA1WPL5PJJaxeNxTIjP5zPCksdJsMRG\nCnO5zHP3+30sxna7jTHhcFbrotY23Lp1CxBmuVzGAl9YWDCoNIWBf/7znyOZk9frRdbSY8eOQal5\nvV4j9HqUPrKvgPaRDVfetBzZ0Ol0MM4cis51dx48eAADol6vY3xYUTHUznQYUzvWqLdRfXi2t7dx\nKGQyGfDi2WwWm2phYQEREel0GvNQKBRAV+ZyOYwxJwXjLLtut9sohGpHGbBRac2uPA7fzMLGCNMr\nvBetlw+WYe/a7ZJgd+BZKUo72mtcg6fZbBqZ2tn4sYv+ZCXYbreNaEKmMnXtq4+Bij4zkUhArxw/\nfhxrw+oPqONQKBQMo4vXwzCx+lZxgjb2mVDhMOCZmRn4QnICzHa7DQqd/Sg5SpINnng8Dv3BFxqX\ny2X4CHFk1Kh7sdVqIaz/ySefRKoAzZ6rv2F/DB2/TqcDqjmRSBj+o7y+mNLkemic+JUjjNjHVH9f\nrVYNym5cYdcKLhrN4djaHp7bVCqFMZmfnzeiJJWWtyZvZbqSk4zqXrGePXwR1d9zf0eRXq8HPXr4\n8GFjvWjbbty4AV+jiYkJzIsCGta+F4tFrOVIJGLoVzb2dBw4waDV/YIvXvrecDg81GfQobQcccQR\nRxxxxJFHXoZSWnbOQSxWGJSTDSolkEgkYNlNTEzAAuU6Km63GxYlJ2Wr1+vy+eefi8jAIVURhnQ6\njRsA17xh634UYWcvj8eDNpw8eVI+/vhjtE1h67W1NfSX89gwAsaRD5999hnQG7/fj9vPwsKCcWvR\n53CejkuXLsmPf/xjERH53d/9XfTRGqkwTDiXB+ca4ugHTjDHUSKcqG5mZgZzmkqlQGPdu3cPybEY\n6Wg2m1/KqSQyWFeMULBD9U7OibtJIBAw6i6p9d/r9YzkaRxxo9Ltdo3buv4b3755DHiuOp0Ofm+N\ncNHvo9Gogbqwc/04kVpM2eyU7HOniBG+ETGKws7J1pxJds7sjFxysklrMIHdPI/aR75tM5qjN71o\nNIobINMhHOlRr9cx/olEAgnsjh8/bjyToyR1foPBoFGigNPZM82kc5FKpcZCW3ldWfO52KFsjLpE\nIhGMTyAQMJAcpvR1/DmxKEfqJZNJIFpM+ej7dBw4kGJUdODUqVNwZUilUlh3qVQKSA7nafH5fNAj\njBTynt6p3A6vWc6bxgk1GeHhZIP1eh2/LxaL0O/qFL6bRCIRgxZmZ3YdP3ZCT6fToOc4waD1DOAI\nJj3bgsGgkWyX3Q7YOZ33JbMOmtQxGo0CjVGUcDcJBAKghRcXF9HfQqEAh+oPPvjAQGMUpbly5Qqo\nyZs3bxr0vo4P59sJBAJgcdbX142AFs7/wzQo6z9G31Uf7MTy7GrwsHIUeaiArKFpzBnqIpqYmDAW\nLE8+J4JiyFW/r9VqxoLVELef//zn4ACffvppLKIzZ84gsmLv3r0jLVqV+fl528FZX1/HwtzY2EDU\n2KVLl2zrKvH4VCoVOXv2LNqsBk86nUbUwqFDh4zDQNv81FNPgfNcX1+XN954Q0QGtJHW81pcXByp\nboiK2+0GVcNcOnvf88FfqVSw0DKZjBHarYrS7XZjXra2tvC37F/EUCsrbqZerOGhdhFEwySdTgMy\nDwaDMIpFHoYwp9Npo8CoGie8kTiRFqcKKBaLaK/b7TZCRpVKWF9fN2ozqeE8MzNjJAy0q7E1itj5\n6ehnbbPH47HNWs2+dfl83vBdYeNUFSv76nCY+U7+QmzIWbN3j9PHVCplm+BM2yFiQuTW2nfaZq7h\nxheyeDxuGDNMxdsZkGyQWJO+6b6fm5sbi9LS91k/83dsrPIe0n8TMRP2WdMR8L5hHy2mq5Ty44Sv\n1vnS8SyXyyPXmnr22WdxkHGUmM/nM7Irs9HC1BlHS7HPGq8v7q+OPftRMR3XbrfRDz5v2Ei4c+fO\nWEn5pqamjAs2p1ZR/ZXNZqErOQv4gQMHYIDv3bsX7Wm324YPkj6TI105GouN/VqthrHlRKcejwf6\niZM6jiLf+c535MUXXxSRgd7XNiwvL+NsW1pawr5cX183qFEtCn7p0iXo46mpKRhRhw8fxly3Wi3o\n7/v37xvzruPJdRzZmOSLAtPPO50dDqXliCOOOOKII4488rIrwuP1emE5ZjIZw/pWyy6VSsFiPXLk\nCCKSpqengVpwHgd2bG6327BS2RmqXq/Dws1kMrg5X7hwAU6oIgJP8BMnTsiRI0dEZGD9Kcw2ivAt\nkRMGTkxMAJp98OABLNwrV64A4ZmcnAQU3ul0JJfLicgA1fnnf/5nERk4Yem4PfXUU/L888+LyABi\nZDhe5Wtf+xqotLfeegsw+kcffYQok8OHD4Ma+/a3vz20j+ywyDQR3ySYXnG5XJj3bDZr3GY4saSi\nRgx3Wx1qrc/dTTgxn8jo1cSXlpbgeJzP5zFmTMEobC4yWHeK0nCJj2KxiPdHIhGgifV63Uj0qO/i\nFP2lUgnwcTAYNBz3uP875eUZJlakZKdbOScDZMibq2nr7xnhazabtg6gOzldc+QXw+hWamac3B+c\nJM7v9xsOydxvppm0ffV63ahxp8LRXvzMRqOB33NCOh4HTmfPFIjH4zHK3Yw7j5yIknMlcckJdmDV\nNTwxMWGUn2Aag+sM6fi3Wi3o1EqlYiSn0xsyU4iNRgN93ymh5TCZm5vD2q9Wq0bdKNWtVgpG29Vs\nNqErp6amjGhYzh3Ga82aA0rErEjP/eDIRS5FkkqlxnJczmaz0I/cfqY6k8kkGIJMJgPac+/evUB+\nGA0vl8vQUUzPxuNxI2koO3tz1CD/hveLojrhcPhLqOlu8tprrwGZ6Xa7iBo7f/48zieRh3vz/Pnz\n8sILL4jIgH1RxK1UKuH8OH78OCK5jhw5gr7fu3cPbiuM8PC+ZHo5HA4b88sll4btxV0NnmQyadSV\nUZiekxLF43GESh4/ftyoE8LhrDrYrVbL8KXgEGZVjlwXJRAI4Jnr6+vy9ttvi8ggwdVLL70kIoNE\nR/qcer2OqCgd3GGi7SwWi3jX/Pw8KKStrS1MyObmpvzkJz8RkcFkq1G3srIily9fxu+Vk+x0Ogh3\nfvXVV0G9sZ8ER0FkMhn5nd/5HREZKAzNSl0ul7GItre35eLFiyIi8rd/+7cj9dGu5gxn0ORoqWg0\nisN+dnYWGzebzRoGhBoN1iSBrGDsorHYk56TELLvENeHGSavv/46oFuOBgkGg1BMHPVRqVSQGKtY\nLBqZofUgO3DgANa+teYYRxKpwZtMJjFmnPmW6TtrGoBxhDe/NcycLxNMx3BKAO0j+wG0Wi0j+d5O\nmX7Z50SF53YnJcNrbRSp1+tGQjduD1M/vGZVZ7AhYfWN4gy8egCwQcppBNhQFHkYacgFddl/KRqN\nGn6Aw4QveezTwJGLbJBvbW2hbRy9U6/XjSSv7Lul47a1tQXjnCNK2X3A7/cbEbcqrJPYX2SU/nHU\njOr9SqVirDUOhVep1+vGBZsvonw2cLg808X6+0Qigc/tdttYp+zHpLKwsDBWKpNwOIxDWfWOyGAO\ndX1xorxQKGToA23z6uoq9Mf29jaM3FarhfUVi8Xw+1gsZmtcsR4PhUJoQ71ex/xzlOwoEovF8JyN\njQ0kc33nnXeMPupa297elnPnzonIYGy/9a1vicigeLaugbm5OaPmofr5XLlyBc9fXl62TaHCfotc\nt5DTyljpXDtxKC1HHHHEEUccceSRl6GFbthiUmtxZWUFll0oFAKNVSqVYKV+8sknuJmk02ncTDg/\nATtRseWrfy9iJgvz+Xx4/tmzZ1GzhR2P2+02aIa/+qu/GjoADPF7PB6DjtFSBI1GAzeu1dVV3A7/\n7d/+DVZnPp/HmDAMOT8/j+SBTz/9tBHhwxXMVZrNJqi6P/uzPwM69NZbb2H8u93uWHVROKEfV3Jn\np3SOkIpGo7DEU6kUaER2lGMqqFQqGRQL38Y5eoeTVbKzqV3uhHFKhFy+fBmITT6fN0pb6A2KHZIb\njQYcyRnmjUajRj0bV4VvzwAAB7NJREFUTjypbcxkMnhmNpvFWG5vb6O909PTcKi3Rp6pjIvw+Hw+\no7QLIzyciI0pLXb+03lrNBrYH4yWWJ1Buc0cIWNXXoGdU7XPImI7r7sJP6PRaBgoDNMb3Ea+OTOK\nyTdPnaNOp4M5LRQKeFYymTSqojMioN+zc681MmecfnI0EX9uNBpGjhWeC21/oVAAbZpMJvG3LpcL\nc1qpVPB5c3MTqDDn02KnbnZKt86voiq1Ws2oZ7ibrK6uGvSKjiXnhLEivPz/ipax47H2kf9G+8EO\nyTpX7IDMFBhHqnHCP+3vqNLr9YyzS88Gj8dj5L3RZ7rdbswVVz/f3NxEf5l2drvdmCsuoeT1evF7\nRroY8eV3eTwenLtcm3IUefPNN6HfNzY25NatWyIyQNB1zDkqzev1ynvvvYe+/9Zv/ZaIDKL2eK3p\n2rx16xZKsiwtLcFthaurM4JnjQZnXchnv8pOTui77tRWq4WEcqVSSd58800RkS9FJegkc3G+7e1t\nRBuJmNlX2TOdo7Q4CSFvPE6Ux0nNePBYSY3DqTPUy7wuZ9B8+umnsaDefPNNUFdcS4QjD/x+vzz3\n3HMiIvLiiy/is8/nw1ixomHhbJFHjx4F93vo0CH54IMPRGRA7dn5iOwk1rBO9u1g/xsu4KYGTzqd\nNkKy9b35fB7Kt1wuG0qFDwbe9LpIOVmlNTSXfThGnceVlRW0JRAIYPyY7uH55AOOjRBWiJxpef/+\n/UZUC/t1qITDYSTaTKfTRgQQR6Tx53HEGorOY6bC1IM1+zEfKrznVPigt0YM2WWqtYbhMy3IRUjH\nEW5PvV7H8wOBgOFjoePu8XiMApfcNjWAv/jiCyjWQqGAg6pSqaBfHO7NSSbj8TiSVU5OThrZZvX3\nfCirX91uwhFYfPlrNptGOgU2OPX7fD5vpLjQvvf7fYxVsVjE4ZbP542wYR2fQqFg+DlyXSv9W2sB\ny1GjtDhJK9cOrFQqoH8mJyeNsHTV+4lEAvqOs7pbs7Lr+mW/Ks6qzmcMG1RWw1T3yrhRdpxOpVar\nGWHUqk99Ph90aLPZNN6ttDuHcrMu9ng8eCavcU48yBm42R2E/aA4RUGhUBgrEu1v/uZvjP3EdCvT\nyDoOa2tr0Enr6+vw+VlcXATVH4/H4ef68ccfA8yw+nRx6gD2GeSabyps8DQaDSOtgZ04lJYjjjji\niCOOOPLIy64Iz8TEBKzR1dVVw7mNLVB2UlNYrl6vG1Y2IwCc5Itvy4w8MDTPN092YmLonL8fB+Hh\nqLFGowFrkZNacV6VV199FZRTPp+HtRsIBGDRP/7448ZtX8eEb26cf4LfxVA2O0fu27cPXvO9Xu9/\ndCsRMdENTtAVDofRnlQqBcg3HA4bNBbf+rjNDIvz2NpVYLfetBjxYwRvVEfCRCJhIDY6h5zXhz97\nPB7kxQgGg0a0nN4uOGpwdXUVCBK3d3t7G5EVPLeBQMBwZOUxsEY9jSpMafl8PiP6ya5UgbVOFkPe\nfFPiiCd2rmaElaOldHxqtZpBo+j3nOuEk+ONIjx3XBGZ1ynfMEXEuOVq++v1OhCes2fP4iZZLBZx\nQ7ZG73CSSYXORQQIz9TUFPZfIpEwqs/rvGs9rt3E7/cb6J8KO11z/S+unJ7L5dDfZrOJPc0ID1Mm\njPb4fD4jEESfn0wmjRsy092qtzjycZgwLcwJWznajKu7M4KfyWSM+oIcLcfjxKiYSjgcNmgqpqoZ\naeQgGXYAHmcvBoNB4/kcaMHvZPSRhWtCqVjz8DCqxfXQGL3mSCVr//Qz67ZxnJa3t7e/RA2KmGe5\n1Vmef6/uJlevXsV3fN5XKhWjDAdHW+q4+f1+rMFmswmU3RohyolOh+Wn21UbzczMgFvjmi4cxcG8\nWSwWwws5AZKIGEaLfmbP8UQiYWwIFT5I/H6/sZk4IRMnXxtHyTKl0263jeR4nIiN28lJERWCnZiY\nGDoJjUbDSArFRoJdJAwXSePDht87rkSjUcwjhyVz7aJutwuo9d69e7Z+LblczoC/ub87hbMyvWRX\nJ8nK7Y9K+8zPz+OQ4tBTLpTJhyb7ZgQCASPjqr6TYX89PHVstK8cpdDtdmHwhkIhg4dW+Z8aOyKm\nwcNzxWuNa8HxumOFzv4/XCBS+yBiXmisvjraX2sdK1Ws1lDSUVMLaJt1jYRCIcN/htcXZ6dl+pRD\ny5X62djYALzOycs4nLjf7xtRQHzw8IVMdSErbjak//iP/3ikPjL9bpc5V9+t7eQCikopswHDBk+5\nXDZSJej3TO9ub2/DH7DVamE9WDNy6/PHybTs9/thiFWrVRg8lUoFRVmZdi6XyxiDaDSKOW80Gmi7\n7iuRwfzoIcjRTHxh63Q6tjXl+GBl/8JerzdWyDavU77Yc4JE7iO7MjA12Ov1sHb4/UytW8UuLYSI\nGGvKTueM6zPY6XRsa5nxXrHWr9M2s7HKiVGZAut0Ovi9y+Uy/NfsQvIrlYpxOee54wvWsOz1DqXl\niCOOOOKII4488uIa1/JzxBFHHHHEEUcc+f9NHITHEUccccQRRxx55MUxeBxxxBFHHHHEkUdeHIPH\nEUccccQRRxx55MUxeBxxxBFHHHHEkUdeHIPHEUccccQRRxx55MUxeBxxxBFHHHHEkUde/hfjAJy/\nhqM4BwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuRUBBxt5zXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "7245f37e-3896-4167-be6f-1a6fd48c0630"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "trainY = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9S8keQw6V9_",
        "colab_type": "code",
        "outputId": "13def461-ab03-4803-97e9-8b0bc0939ab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "trainY[0:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWKs84zHAKqM",
        "colab_type": "code",
        "outputId": "12a7c8ae-2214-4fa9-d896-92d5f9615ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape = (1024,), activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation = 'relu'))\n",
        "model.add(Dense(10, activation = 'relu'))\n",
        "sgd = optimizers.Adam(lr = 0.1)\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,113,546\n",
            "Trainable params: 2,111,498\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxQLjdel4AHr",
        "colab_type": "code",
        "outputId": "2491ea69-5614-44b6-b95f-bc2f5f7eebba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "42000/42000 [==============================] - 10s 242us/step - loss: 2.3945 - acc: 0.1100 - val_loss: 2.2977 - val_acc: 0.1072\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.2432 - acc: 0.1582 - val_loss: 2.2770 - val_acc: 0.1347\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1837 - acc: 0.1735 - val_loss: 2.4156 - val_acc: 0.1074\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.1407 - acc: 0.2124 - val_loss: 2.1392 - val_acc: 0.2249\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1616 - acc: 0.1903 - val_loss: 2.4416 - val_acc: 0.1856\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1860 - acc: 0.1782 - val_loss: 2.0418 - val_acc: 0.2967\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.1025 - acc: 0.2282 - val_loss: 1.9460 - val_acc: 0.3156\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0933 - acc: 0.2286 - val_loss: 2.5094 - val_acc: 0.1636\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1056 - acc: 0.2137 - val_loss: 2.3574 - val_acc: 0.1643\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0908 - acc: 0.2281 - val_loss: 2.1717 - val_acc: 0.1418\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0622 - acc: 0.2374 - val_loss: 1.9290 - val_acc: 0.3197\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0145 - acc: 0.2810 - val_loss: 1.9620 - val_acc: 0.2903\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0547 - acc: 0.2366 - val_loss: 2.0182 - val_acc: 0.2669\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0096 - acc: 0.2809 - val_loss: 2.2182 - val_acc: 0.1611\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1361 - acc: 0.1897 - val_loss: 3.7393 - val_acc: 0.1053\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0881 - acc: 0.2289 - val_loss: 2.5670 - val_acc: 0.2146\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0465 - acc: 0.2505 - val_loss: 2.0715 - val_acc: 0.2596\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0229 - acc: 0.2704 - val_loss: 2.8361 - val_acc: 0.1594\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9804 - acc: 0.2970 - val_loss: 2.1812 - val_acc: 0.1807\n",
            "Epoch 20/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0549 - acc: 0.2366 - val_loss: 1.9898 - val_acc: 0.2782\n",
            "Epoch 21/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9881 - acc: 0.2997 - val_loss: 2.7190 - val_acc: 0.0989\n",
            "Epoch 22/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9785 - acc: 0.2965 - val_loss: 1.8777 - val_acc: 0.3612\n",
            "Epoch 23/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9898 - acc: 0.3120 - val_loss: 2.7121 - val_acc: 0.1158\n",
            "Epoch 24/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1652 - acc: 0.1769 - val_loss: 2.2198 - val_acc: 0.1824\n",
            "Epoch 25/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0722 - acc: 0.2548 - val_loss: 2.3153 - val_acc: 0.1456\n",
            "Epoch 26/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0415 - acc: 0.2732 - val_loss: 1.9953 - val_acc: 0.2789\n",
            "Epoch 27/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0202 - acc: 0.2753 - val_loss: 2.0631 - val_acc: 0.2508\n",
            "Epoch 28/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0805 - acc: 0.2337 - val_loss: 2.1738 - val_acc: 0.1664\n",
            "Epoch 29/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9941 - acc: 0.2887 - val_loss: 2.0205 - val_acc: 0.2804\n",
            "Epoch 30/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9972 - acc: 0.2931 - val_loss: 2.0414 - val_acc: 0.2574\n",
            "Epoch 31/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9725 - acc: 0.3103 - val_loss: 2.2374 - val_acc: 0.1654\n",
            "Epoch 32/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9373 - acc: 0.3172 - val_loss: 1.8663 - val_acc: 0.3587\n",
            "Epoch 33/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9071 - acc: 0.3547 - val_loss: 1.7988 - val_acc: 0.4188\n",
            "Epoch 34/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8557 - acc: 0.3787 - val_loss: 1.7790 - val_acc: 0.4017\n",
            "Epoch 35/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8996 - acc: 0.3559 - val_loss: 2.1694 - val_acc: 0.1933\n",
            "Epoch 36/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.0345 - acc: 0.2642 - val_loss: 2.0317 - val_acc: 0.2767\n",
            "Epoch 37/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9519 - acc: 0.3126 - val_loss: 1.8751 - val_acc: 0.3545\n",
            "Epoch 38/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9659 - acc: 0.2979 - val_loss: 2.6450 - val_acc: 0.1919\n",
            "Epoch 39/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.0010 - acc: 0.2806 - val_loss: 2.3063 - val_acc: 0.1161\n",
            "Epoch 40/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.0710 - acc: 0.2295 - val_loss: 1.9397 - val_acc: 0.3308\n",
            "Epoch 41/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9329 - acc: 0.3279 - val_loss: 1.8909 - val_acc: 0.3712\n",
            "Epoch 42/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8645 - acc: 0.3912 - val_loss: 1.8055 - val_acc: 0.4415\n",
            "Epoch 43/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8621 - acc: 0.3879 - val_loss: 1.8536 - val_acc: 0.4009\n",
            "Epoch 44/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8905 - acc: 0.3632 - val_loss: 2.4299 - val_acc: 0.1801\n",
            "Epoch 45/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.1297 - acc: 0.1891 - val_loss: 1.9952 - val_acc: 0.2689\n",
            "Epoch 46/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9913 - acc: 0.2915 - val_loss: 2.3122 - val_acc: 0.3229\n",
            "Epoch 47/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0644 - acc: 0.2377 - val_loss: 2.0701 - val_acc: 0.2334\n",
            "Epoch 48/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9921 - acc: 0.2921 - val_loss: 1.9696 - val_acc: 0.3174\n",
            "Epoch 49/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9671 - acc: 0.3176 - val_loss: 1.9898 - val_acc: 0.2828\n",
            "Epoch 50/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9234 - acc: 0.3527 - val_loss: 1.9413 - val_acc: 0.3594\n",
            "Epoch 51/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9157 - acc: 0.3471 - val_loss: 1.8710 - val_acc: 0.3301\n",
            "Epoch 52/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8933 - acc: 0.3599 - val_loss: 1.7570 - val_acc: 0.4540\n",
            "Epoch 53/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9338 - acc: 0.3304 - val_loss: 2.2097 - val_acc: 0.1782\n",
            "Epoch 54/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9373 - acc: 0.3194 - val_loss: 1.8193 - val_acc: 0.3912\n",
            "Epoch 55/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8675 - acc: 0.3807 - val_loss: 1.9226 - val_acc: 0.3760\n",
            "Epoch 56/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8640 - acc: 0.3920 - val_loss: 1.9915 - val_acc: 0.3400\n",
            "Epoch 57/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8427 - acc: 0.4025 - val_loss: 1.6614 - val_acc: 0.5057\n",
            "Epoch 58/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8508 - acc: 0.3928 - val_loss: 3.1352 - val_acc: 0.2653\n",
            "Epoch 59/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8547 - acc: 0.3969 - val_loss: 1.9591 - val_acc: 0.3496\n",
            "Epoch 60/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8040 - acc: 0.4225 - val_loss: 1.7488 - val_acc: 0.4507\n",
            "Epoch 61/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9709 - acc: 0.3061 - val_loss: 2.1076 - val_acc: 0.2659\n",
            "Epoch 62/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8998 - acc: 0.3646 - val_loss: 1.9051 - val_acc: 0.4133\n",
            "Epoch 63/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8421 - acc: 0.3977 - val_loss: 1.8813 - val_acc: 0.4176\n",
            "Epoch 64/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8318 - acc: 0.4004 - val_loss: 1.7529 - val_acc: 0.4140\n",
            "Epoch 65/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8639 - acc: 0.3777 - val_loss: 1.8240 - val_acc: 0.4162\n",
            "Epoch 66/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.7945 - acc: 0.4115 - val_loss: 2.1294 - val_acc: 0.3161\n",
            "Epoch 67/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8686 - acc: 0.3657 - val_loss: 1.6867 - val_acc: 0.4988\n",
            "Epoch 68/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8427 - acc: 0.3901 - val_loss: 2.5097 - val_acc: 0.1757\n",
            "Epoch 69/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9208 - acc: 0.3233 - val_loss: 3.6547 - val_acc: 0.1182\n",
            "Epoch 70/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9762 - acc: 0.2916 - val_loss: 1.9672 - val_acc: 0.3452\n",
            "Epoch 71/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9031 - acc: 0.3595 - val_loss: 1.7296 - val_acc: 0.4606\n",
            "Epoch 72/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9123 - acc: 0.3402 - val_loss: 2.1375 - val_acc: 0.3202\n",
            "Epoch 73/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8961 - acc: 0.3505 - val_loss: 2.5320 - val_acc: 0.2206\n",
            "Epoch 74/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9380 - acc: 0.3400 - val_loss: 3.3575 - val_acc: 0.1528\n",
            "Epoch 75/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0346 - acc: 0.2538 - val_loss: 2.0488 - val_acc: 0.2159\n",
            "Epoch 76/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9504 - acc: 0.3175 - val_loss: 1.8284 - val_acc: 0.3947\n",
            "Epoch 77/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0335 - acc: 0.2898 - val_loss: 2.0415 - val_acc: 0.2398\n",
            "Epoch 78/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9684 - acc: 0.3112 - val_loss: 2.0200 - val_acc: 0.3569\n",
            "Epoch 79/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9016 - acc: 0.3653 - val_loss: 1.7145 - val_acc: 0.4943\n",
            "Epoch 80/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8703 - acc: 0.3902 - val_loss: 1.8234 - val_acc: 0.4384\n",
            "Epoch 81/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8587 - acc: 0.3809 - val_loss: 1.9152 - val_acc: 0.3090\n",
            "Epoch 82/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8840 - acc: 0.3610 - val_loss: 1.7515 - val_acc: 0.4628\n",
            "Epoch 83/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8887 - acc: 0.3324 - val_loss: 1.7630 - val_acc: 0.4105\n",
            "Epoch 84/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8850 - acc: 0.3560 - val_loss: 1.9300 - val_acc: 0.3981\n",
            "Epoch 85/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8613 - acc: 0.3928 - val_loss: 1.7187 - val_acc: 0.4344\n",
            "Epoch 86/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8150 - acc: 0.4219 - val_loss: 1.7858 - val_acc: 0.4035\n",
            "Epoch 87/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8502 - acc: 0.3804 - val_loss: 1.6834 - val_acc: 0.4684\n",
            "Epoch 88/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8112 - acc: 0.4159 - val_loss: 1.6473 - val_acc: 0.4964\n",
            "Epoch 89/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8054 - acc: 0.4239 - val_loss: 1.7202 - val_acc: 0.4662\n",
            "Epoch 90/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8255 - acc: 0.4070 - val_loss: 1.6824 - val_acc: 0.4815\n",
            "Epoch 91/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.7889 - acc: 0.4257 - val_loss: 1.6720 - val_acc: 0.4792\n",
            "Epoch 92/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.7770 - acc: 0.4264 - val_loss: 1.7033 - val_acc: 0.4536\n",
            "Epoch 93/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8205 - acc: 0.3752 - val_loss: 1.7937 - val_acc: 0.3765\n",
            "Epoch 94/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8318 - acc: 0.3943 - val_loss: 1.6624 - val_acc: 0.4879\n",
            "Epoch 95/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.7894 - acc: 0.4320 - val_loss: 1.7208 - val_acc: 0.4610\n",
            "Epoch 96/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8249 - acc: 0.4097 - val_loss: 2.2916 - val_acc: 0.2539\n",
            "Epoch 97/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9320 - acc: 0.3257 - val_loss: 2.0775 - val_acc: 0.1886\n",
            "Epoch 98/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9753 - acc: 0.2715 - val_loss: 1.8501 - val_acc: 0.3823\n",
            "Epoch 99/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8491 - acc: 0.3929 - val_loss: 1.7446 - val_acc: 0.4443\n",
            "Epoch 100/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9195 - acc: 0.3350 - val_loss: 2.0882 - val_acc: 0.2567\n",
            "Epoch 101/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9341 - acc: 0.3187 - val_loss: 1.7890 - val_acc: 0.4162\n",
            "Epoch 102/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8934 - acc: 0.3557 - val_loss: 1.7700 - val_acc: 0.4325\n",
            "Epoch 103/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8636 - acc: 0.3725 - val_loss: 1.7281 - val_acc: 0.4647\n",
            "Epoch 104/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8205 - acc: 0.4089 - val_loss: 2.2130 - val_acc: 0.3534\n",
            "Epoch 105/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9264 - acc: 0.3311 - val_loss: 1.7362 - val_acc: 0.4512\n",
            "Epoch 106/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8594 - acc: 0.4008 - val_loss: 1.6570 - val_acc: 0.5069\n",
            "Epoch 107/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8385 - acc: 0.3876 - val_loss: 1.7583 - val_acc: 0.4697\n",
            "Epoch 108/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8215 - acc: 0.4146 - val_loss: 1.8478 - val_acc: 0.3657\n",
            "Epoch 109/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8351 - acc: 0.3946 - val_loss: 1.6239 - val_acc: 0.5558\n",
            "Epoch 110/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8368 - acc: 0.4028 - val_loss: 1.7272 - val_acc: 0.4243\n",
            "Epoch 111/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8472 - acc: 0.3837 - val_loss: 1.6281 - val_acc: 0.5157\n",
            "Epoch 112/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8245 - acc: 0.4105 - val_loss: 1.7950 - val_acc: 0.3779\n",
            "Epoch 113/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9231 - acc: 0.3319 - val_loss: 5.0389 - val_acc: 0.1714\n",
            "Epoch 114/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0296 - acc: 0.2560 - val_loss: 1.8927 - val_acc: 0.3582\n",
            "Epoch 115/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9084 - acc: 0.3705 - val_loss: 1.7278 - val_acc: 0.4821\n",
            "Epoch 116/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8351 - acc: 0.4181 - val_loss: 1.7158 - val_acc: 0.5166\n",
            "Epoch 117/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8856 - acc: 0.3742 - val_loss: 1.8294 - val_acc: 0.3890\n",
            "Epoch 118/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8648 - acc: 0.3884 - val_loss: 1.6249 - val_acc: 0.5268\n",
            "Epoch 119/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8704 - acc: 0.4007 - val_loss: 2.0816 - val_acc: 0.3047\n",
            "Epoch 120/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9289 - acc: 0.3309 - val_loss: 1.9752 - val_acc: 0.3674\n",
            "Epoch 121/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8457 - acc: 0.4066 - val_loss: 1.6615 - val_acc: 0.5031\n",
            "Epoch 122/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8954 - acc: 0.3425 - val_loss: 1.8022 - val_acc: 0.3980\n",
            "Epoch 123/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8209 - acc: 0.4114 - val_loss: 1.7381 - val_acc: 0.4731\n",
            "Epoch 124/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8217 - acc: 0.4243 - val_loss: 1.6412 - val_acc: 0.5386\n",
            "Epoch 125/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8514 - acc: 0.3798 - val_loss: 1.7627 - val_acc: 0.4414\n",
            "Epoch 126/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8239 - acc: 0.4278 - val_loss: 1.6201 - val_acc: 0.5561\n",
            "Epoch 127/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8623 - acc: 0.3970 - val_loss: 1.9124 - val_acc: 0.3497\n",
            "Epoch 128/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8857 - acc: 0.3749 - val_loss: 1.6469 - val_acc: 0.5329\n",
            "Epoch 129/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8017 - acc: 0.4409 - val_loss: 1.6603 - val_acc: 0.5224\n",
            "Epoch 130/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.7767 - acc: 0.4644 - val_loss: 1.6157 - val_acc: 0.5497\n",
            "Epoch 131/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.7727 - acc: 0.4261 - val_loss: 1.5983 - val_acc: 0.5549\n",
            "Epoch 132/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.7976 - acc: 0.4254 - val_loss: 2.0600 - val_acc: 0.3252\n",
            "Epoch 133/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8229 - acc: 0.4240 - val_loss: 1.5947 - val_acc: 0.5569\n",
            "Epoch 134/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8154 - acc: 0.4160 - val_loss: 1.7446 - val_acc: 0.4544\n",
            "Epoch 135/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8200 - acc: 0.4433 - val_loss: 1.7209 - val_acc: 0.4791\n",
            "Epoch 136/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8825 - acc: 0.3268 - val_loss: 1.7079 - val_acc: 0.4439\n",
            "Epoch 137/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8107 - acc: 0.4187 - val_loss: 1.6190 - val_acc: 0.5443\n",
            "Epoch 138/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8379 - acc: 0.3953 - val_loss: 1.7597 - val_acc: 0.4510\n",
            "Epoch 139/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8757 - acc: 0.3325 - val_loss: 1.8095 - val_acc: 0.3404\n",
            "Epoch 140/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8802 - acc: 0.3580 - val_loss: 2.0259 - val_acc: 0.2037\n",
            "Epoch 141/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9206 - acc: 0.3310 - val_loss: 2.1465 - val_acc: 0.2331\n",
            "Epoch 142/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0295 - acc: 0.2835 - val_loss: 2.4047 - val_acc: 0.3554\n",
            "Epoch 143/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9247 - acc: 0.3468 - val_loss: 1.8621 - val_acc: 0.3831\n",
            "Epoch 144/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0319 - acc: 0.2407 - val_loss: 2.2034 - val_acc: 0.1383\n",
            "Epoch 145/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1443 - acc: 0.1774 - val_loss: 2.0765 - val_acc: 0.2424\n",
            "Epoch 146/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0380 - acc: 0.2678 - val_loss: 2.0243 - val_acc: 0.2344\n",
            "Epoch 147/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9920 - acc: 0.2907 - val_loss: 1.7696 - val_acc: 0.4389\n",
            "Epoch 148/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9936 - acc: 0.3033 - val_loss: 2.0002 - val_acc: 0.3129\n",
            "Epoch 149/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9858 - acc: 0.3005 - val_loss: 1.7947 - val_acc: 0.4101\n",
            "Epoch 150/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9371 - acc: 0.3413 - val_loss: 1.8073 - val_acc: 0.3874\n",
            "Epoch 151/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9043 - acc: 0.3705 - val_loss: 1.6982 - val_acc: 0.4807\n",
            "Epoch 152/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9064 - acc: 0.3481 - val_loss: 1.8060 - val_acc: 0.4287\n",
            "Epoch 153/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9169 - acc: 0.3474 - val_loss: 2.2371 - val_acc: 0.1564\n",
            "Epoch 154/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9557 - acc: 0.2942 - val_loss: 1.8143 - val_acc: 0.4379\n",
            "Epoch 155/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8985 - acc: 0.3616 - val_loss: 1.7329 - val_acc: 0.4606\n",
            "Epoch 156/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9027 - acc: 0.3550 - val_loss: 1.7609 - val_acc: 0.4264\n",
            "Epoch 157/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8714 - acc: 0.3847 - val_loss: 1.8833 - val_acc: 0.3744\n",
            "Epoch 158/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9261 - acc: 0.3385 - val_loss: 1.6925 - val_acc: 0.4624\n",
            "Epoch 159/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8775 - acc: 0.3845 - val_loss: 1.7148 - val_acc: 0.4665\n",
            "Epoch 160/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9254 - acc: 0.3611 - val_loss: 1.8984 - val_acc: 0.3824\n",
            "Epoch 161/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9001 - acc: 0.3586 - val_loss: 1.7443 - val_acc: 0.4589\n",
            "Epoch 162/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8435 - acc: 0.3772 - val_loss: 2.1915 - val_acc: 0.2041\n",
            "Epoch 163/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8800 - acc: 0.3715 - val_loss: 1.6542 - val_acc: 0.4945\n",
            "Epoch 164/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8593 - acc: 0.3747 - val_loss: 1.6483 - val_acc: 0.5234\n",
            "Epoch 165/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8540 - acc: 0.3995 - val_loss: 1.7175 - val_acc: 0.5003\n",
            "Epoch 166/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9317 - acc: 0.2708 - val_loss: 1.8300 - val_acc: 0.3142\n",
            "Epoch 167/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8752 - acc: 0.3712 - val_loss: 1.7133 - val_acc: 0.4852\n",
            "Epoch 168/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0013 - acc: 0.2570 - val_loss: 2.1016 - val_acc: 0.1263\n",
            "Epoch 169/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1020 - acc: 0.1751 - val_loss: 2.1248 - val_acc: 0.1738\n",
            "Epoch 170/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0798 - acc: 0.2334 - val_loss: 1.9930 - val_acc: 0.2719\n",
            "Epoch 171/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0218 - acc: 0.2828 - val_loss: 1.9045 - val_acc: 0.2976\n",
            "Epoch 172/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0041 - acc: 0.2905 - val_loss: 2.0296 - val_acc: 0.2034\n",
            "Epoch 173/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1107 - acc: 0.1853 - val_loss: 2.2292 - val_acc: 0.1246\n",
            "Epoch 174/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.1543 - acc: 0.1585 - val_loss: 2.0830 - val_acc: 0.2061\n",
            "Epoch 175/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0589 - acc: 0.2440 - val_loss: 1.8970 - val_acc: 0.3839\n",
            "Epoch 176/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0646 - acc: 0.2460 - val_loss: 1.8915 - val_acc: 0.4356\n",
            "Epoch 177/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0074 - acc: 0.3070 - val_loss: 1.8081 - val_acc: 0.4795\n",
            "Epoch 178/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9823 - acc: 0.3228 - val_loss: 2.3620 - val_acc: 0.1911\n",
            "Epoch 179/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0611 - acc: 0.2092 - val_loss: 1.9789 - val_acc: 0.2254\n",
            "Epoch 180/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9900 - acc: 0.2908 - val_loss: 1.9764 - val_acc: 0.2993\n",
            "Epoch 181/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9938 - acc: 0.2903 - val_loss: 1.9955 - val_acc: 0.1958\n",
            "Epoch 182/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0217 - acc: 0.2247 - val_loss: 1.8315 - val_acc: 0.3816\n",
            "Epoch 183/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9535 - acc: 0.3425 - val_loss: 1.7635 - val_acc: 0.4446\n",
            "Epoch 184/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9370 - acc: 0.3374 - val_loss: 2.1480 - val_acc: 0.1783\n",
            "Epoch 185/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0101 - acc: 0.2662 - val_loss: 1.7892 - val_acc: 0.4404\n",
            "Epoch 186/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9710 - acc: 0.3017 - val_loss: 1.8346 - val_acc: 0.4370\n",
            "Epoch 187/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9940 - acc: 0.2732 - val_loss: 1.9485 - val_acc: 0.2537\n",
            "Epoch 188/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9371 - acc: 0.3404 - val_loss: 1.7140 - val_acc: 0.5019\n",
            "Epoch 189/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0848 - acc: 0.1967 - val_loss: 2.0124 - val_acc: 0.2547\n",
            "Epoch 190/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0635 - acc: 0.2197 - val_loss: 2.1360 - val_acc: 0.1569\n",
            "Epoch 191/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1071 - acc: 0.1952 - val_loss: 1.9989 - val_acc: 0.2896\n",
            "Epoch 192/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0069 - acc: 0.2929 - val_loss: 1.8135 - val_acc: 0.4501\n",
            "Epoch 193/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9337 - acc: 0.3636 - val_loss: 1.7227 - val_acc: 0.5077\n",
            "Epoch 194/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9344 - acc: 0.3658 - val_loss: 1.8432 - val_acc: 0.3847\n",
            "Epoch 195/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9059 - acc: 0.3884 - val_loss: 1.6965 - val_acc: 0.5082\n",
            "Epoch 196/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9355 - acc: 0.3353 - val_loss: 1.7879 - val_acc: 0.4226\n",
            "Epoch 197/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8937 - acc: 0.3806 - val_loss: 1.6789 - val_acc: 0.5262\n",
            "Epoch 198/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8645 - acc: 0.3822 - val_loss: 1.8134 - val_acc: 0.4253\n",
            "Epoch 199/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9155 - acc: 0.3513 - val_loss: 2.1380 - val_acc: 0.1881\n",
            "Epoch 200/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.1312 - acc: 0.1780 - val_loss: 2.0630 - val_acc: 0.2292\n",
            "Epoch 201/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0584 - acc: 0.2319 - val_loss: 1.8988 - val_acc: 0.3474\n",
            "Epoch 202/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9726 - acc: 0.3228 - val_loss: 1.8627 - val_acc: 0.4161\n",
            "Epoch 203/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9325 - acc: 0.3587 - val_loss: 1.7705 - val_acc: 0.4481\n",
            "Epoch 204/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9047 - acc: 0.3739 - val_loss: 1.6850 - val_acc: 0.5277\n",
            "Epoch 205/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8816 - acc: 0.3952 - val_loss: 1.7260 - val_acc: 0.5054\n",
            "Epoch 206/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8590 - acc: 0.4167 - val_loss: 1.6392 - val_acc: 0.5326\n",
            "Epoch 207/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8426 - acc: 0.4120 - val_loss: 1.6529 - val_acc: 0.5281\n",
            "Epoch 208/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9160 - acc: 0.3075 - val_loss: 1.7360 - val_acc: 0.4681\n",
            "Epoch 209/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8763 - acc: 0.4077 - val_loss: 1.6996 - val_acc: 0.5174\n",
            "Epoch 210/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9070 - acc: 0.3235 - val_loss: 1.8150 - val_acc: 0.4574\n",
            "Epoch 211/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9130 - acc: 0.3252 - val_loss: 1.9255 - val_acc: 0.3849\n",
            "Epoch 212/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8976 - acc: 0.3738 - val_loss: 1.6570 - val_acc: 0.5448\n",
            "Epoch 213/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8859 - acc: 0.3811 - val_loss: 1.6755 - val_acc: 0.5244\n",
            "Epoch 214/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.1413 - acc: 0.1853 - val_loss: 2.2695 - val_acc: 0.1909\n",
            "Epoch 215/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.1077 - acc: 0.2187 - val_loss: 1.9409 - val_acc: 0.3145\n",
            "Epoch 216/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0125 - acc: 0.3099 - val_loss: 1.8801 - val_acc: 0.3746\n",
            "Epoch 217/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0282 - acc: 0.2882 - val_loss: 2.0622 - val_acc: 0.2503\n",
            "Epoch 218/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0105 - acc: 0.2906 - val_loss: 1.7498 - val_acc: 0.4687\n",
            "Epoch 219/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0833 - acc: 0.2573 - val_loss: 1.8936 - val_acc: 0.3763\n",
            "Epoch 220/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0644 - acc: 0.2605 - val_loss: 2.0411 - val_acc: 0.2283\n",
            "Epoch 221/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0463 - acc: 0.2780 - val_loss: 1.7804 - val_acc: 0.4438\n",
            "Epoch 222/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0088 - acc: 0.3194 - val_loss: 1.7985 - val_acc: 0.4768\n",
            "Epoch 223/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9914 - acc: 0.3032 - val_loss: 1.7956 - val_acc: 0.4442\n",
            "Epoch 224/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9807 - acc: 0.3316 - val_loss: 1.7715 - val_acc: 0.4918\n",
            "Epoch 225/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9904 - acc: 0.3262 - val_loss: 1.8302 - val_acc: 0.4192\n",
            "Epoch 226/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9911 - acc: 0.2739 - val_loss: 1.7731 - val_acc: 0.4626\n",
            "Epoch 227/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9426 - acc: 0.3482 - val_loss: 1.7844 - val_acc: 0.4814\n",
            "Epoch 228/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9337 - acc: 0.3432 - val_loss: 1.7285 - val_acc: 0.4939\n",
            "Epoch 229/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9242 - acc: 0.3666 - val_loss: 1.7856 - val_acc: 0.3897\n",
            "Epoch 230/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9037 - acc: 0.3677 - val_loss: 1.7132 - val_acc: 0.4837\n",
            "Epoch 231/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9155 - acc: 0.3677 - val_loss: 2.2828 - val_acc: 0.2428\n",
            "Epoch 232/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9509 - acc: 0.3057 - val_loss: 2.1884 - val_acc: 0.2159\n",
            "Epoch 233/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9633 - acc: 0.3036 - val_loss: 1.7299 - val_acc: 0.4638\n",
            "Epoch 234/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8743 - acc: 0.3899 - val_loss: 1.7388 - val_acc: 0.4634\n",
            "Epoch 235/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8637 - acc: 0.4035 - val_loss: 1.6762 - val_acc: 0.5129\n",
            "Epoch 236/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9065 - acc: 0.3553 - val_loss: 2.0787 - val_acc: 0.2144\n",
            "Epoch 237/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9754 - acc: 0.2537 - val_loss: 1.8120 - val_acc: 0.4634\n",
            "Epoch 238/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9358 - acc: 0.3468 - val_loss: 1.7258 - val_acc: 0.4619\n",
            "Epoch 239/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9329 - acc: 0.3369 - val_loss: 1.8494 - val_acc: 0.4337\n",
            "Epoch 240/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9147 - acc: 0.3646 - val_loss: 1.6813 - val_acc: 0.5040\n",
            "Epoch 241/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8893 - acc: 0.3874 - val_loss: 1.7229 - val_acc: 0.4891\n",
            "Epoch 242/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8727 - acc: 0.4043 - val_loss: 2.4057 - val_acc: 0.3624\n",
            "Epoch 243/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8621 - acc: 0.3870 - val_loss: 1.7779 - val_acc: 0.4668\n",
            "Epoch 244/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9109 - acc: 0.3459 - val_loss: 2.0499 - val_acc: 0.2943\n",
            "Epoch 245/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9947 - acc: 0.2556 - val_loss: 1.8745 - val_acc: 0.3445\n",
            "Epoch 246/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9205 - acc: 0.3542 - val_loss: 1.7737 - val_acc: 0.4439\n",
            "Epoch 247/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9088 - acc: 0.3857 - val_loss: 1.6744 - val_acc: 0.5218\n",
            "Epoch 248/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8765 - acc: 0.4058 - val_loss: 1.6174 - val_acc: 0.5556\n",
            "Epoch 249/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8653 - acc: 0.4219 - val_loss: 1.6333 - val_acc: 0.5399\n",
            "Epoch 250/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8538 - acc: 0.4070 - val_loss: 1.6273 - val_acc: 0.5338\n",
            "Epoch 251/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8604 - acc: 0.4104 - val_loss: 1.8641 - val_acc: 0.4126\n",
            "Epoch 252/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9020 - acc: 0.3612 - val_loss: 1.7633 - val_acc: 0.4594\n",
            "Epoch 253/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8865 - acc: 0.4016 - val_loss: 1.7731 - val_acc: 0.4306\n",
            "Epoch 254/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8716 - acc: 0.3829 - val_loss: 1.7400 - val_acc: 0.4417\n",
            "Epoch 255/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8607 - acc: 0.4103 - val_loss: 1.6413 - val_acc: 0.5685\n",
            "Epoch 256/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8851 - acc: 0.3600 - val_loss: 2.3424 - val_acc: 0.2432\n",
            "Epoch 257/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9508 - acc: 0.2824 - val_loss: 1.7837 - val_acc: 0.3971\n",
            "Epoch 258/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8646 - acc: 0.3861 - val_loss: 1.7663 - val_acc: 0.4640\n",
            "Epoch 259/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0084 - acc: 0.2771 - val_loss: 1.9250 - val_acc: 0.3377\n",
            "Epoch 260/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9772 - acc: 0.3349 - val_loss: 1.8059 - val_acc: 0.4403\n",
            "Epoch 261/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9325 - acc: 0.3502 - val_loss: 1.7322 - val_acc: 0.4607\n",
            "Epoch 262/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9283 - acc: 0.3259 - val_loss: 1.8033 - val_acc: 0.4569\n",
            "Epoch 263/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9136 - acc: 0.3545 - val_loss: 1.9340 - val_acc: 0.3329\n",
            "Epoch 264/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9065 - acc: 0.3546 - val_loss: 1.8229 - val_acc: 0.4423\n",
            "Epoch 265/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8585 - acc: 0.3900 - val_loss: 1.8579 - val_acc: 0.4491\n",
            "Epoch 266/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8536 - acc: 0.3984 - val_loss: 1.7252 - val_acc: 0.4679\n",
            "Epoch 267/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8838 - acc: 0.3750 - val_loss: 2.2139 - val_acc: 0.2402\n",
            "Epoch 268/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0809 - acc: 0.2339 - val_loss: 2.2185 - val_acc: 0.1222\n",
            "Epoch 269/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1340 - acc: 0.1959 - val_loss: 2.0432 - val_acc: 0.2466\n",
            "Epoch 270/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0278 - acc: 0.2836 - val_loss: 1.8567 - val_acc: 0.3853\n",
            "Epoch 271/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9779 - acc: 0.3321 - val_loss: 1.8250 - val_acc: 0.3995\n",
            "Epoch 272/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9519 - acc: 0.3483 - val_loss: 1.8352 - val_acc: 0.4338\n",
            "Epoch 273/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9873 - acc: 0.3120 - val_loss: 1.8629 - val_acc: 0.4221\n",
            "Epoch 274/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9330 - acc: 0.3586 - val_loss: 1.7252 - val_acc: 0.4762\n",
            "Epoch 275/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9283 - acc: 0.3744 - val_loss: 1.6866 - val_acc: 0.5086\n",
            "Epoch 276/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9581 - acc: 0.3194 - val_loss: 1.8717 - val_acc: 0.3713\n",
            "Epoch 277/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9427 - acc: 0.3440 - val_loss: 1.7526 - val_acc: 0.4687\n",
            "Epoch 278/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9146 - acc: 0.3686 - val_loss: 1.7202 - val_acc: 0.4719\n",
            "Epoch 279/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8885 - acc: 0.3714 - val_loss: 2.0914 - val_acc: 0.2648\n",
            "Epoch 280/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0847 - acc: 0.2201 - val_loss: 2.1594 - val_acc: 0.1847\n",
            "Epoch 281/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0664 - acc: 0.2435 - val_loss: 1.9305 - val_acc: 0.3479\n",
            "Epoch 282/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0229 - acc: 0.2584 - val_loss: 2.0150 - val_acc: 0.2474\n",
            "Epoch 283/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0106 - acc: 0.2607 - val_loss: 1.8856 - val_acc: 0.3676\n",
            "Epoch 284/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9937 - acc: 0.2903 - val_loss: 1.9348 - val_acc: 0.3429\n",
            "Epoch 285/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0375 - acc: 0.2661 - val_loss: 2.1463 - val_acc: 0.1384\n",
            "Epoch 286/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1205 - acc: 0.1935 - val_loss: 2.0257 - val_acc: 0.2899\n",
            "Epoch 287/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0511 - acc: 0.2664 - val_loss: 1.8797 - val_acc: 0.4353\n",
            "Epoch 288/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0380 - acc: 0.2769 - val_loss: 1.9736 - val_acc: 0.3312\n",
            "Epoch 289/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.0331 - acc: 0.2998 - val_loss: 1.8314 - val_acc: 0.4607\n",
            "Epoch 290/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0067 - acc: 0.3149 - val_loss: 1.9129 - val_acc: 0.3578\n",
            "Epoch 291/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9801 - acc: 0.3172 - val_loss: 1.8528 - val_acc: 0.4573\n",
            "Epoch 292/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9613 - acc: 0.3352 - val_loss: 1.8852 - val_acc: 0.3713\n",
            "Epoch 293/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9905 - acc: 0.3048 - val_loss: 1.9192 - val_acc: 0.3899\n",
            "Epoch 294/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9805 - acc: 0.3096 - val_loss: 1.8492 - val_acc: 0.4183\n",
            "Epoch 295/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9810 - acc: 0.3230 - val_loss: 1.9592 - val_acc: 0.3132\n",
            "Epoch 296/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0056 - acc: 0.2933 - val_loss: 2.0026 - val_acc: 0.2355\n",
            "Epoch 297/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9927 - acc: 0.2960 - val_loss: 1.8147 - val_acc: 0.4529\n",
            "Epoch 298/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9611 - acc: 0.3424 - val_loss: 1.7407 - val_acc: 0.4797\n",
            "Epoch 299/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9696 - acc: 0.3198 - val_loss: 1.7606 - val_acc: 0.4621\n",
            "Epoch 300/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9391 - acc: 0.3425 - val_loss: 1.7603 - val_acc: 0.4588\n",
            "Epoch 301/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9297 - acc: 0.3472 - val_loss: 1.7855 - val_acc: 0.4838\n",
            "Epoch 302/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9200 - acc: 0.3629 - val_loss: 1.6734 - val_acc: 0.5322\n",
            "Epoch 303/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9130 - acc: 0.3690 - val_loss: 1.7098 - val_acc: 0.5080\n",
            "Epoch 304/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9105 - acc: 0.3553 - val_loss: 1.7108 - val_acc: 0.4823\n",
            "Epoch 305/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9487 - acc: 0.3134 - val_loss: 1.8331 - val_acc: 0.3516\n",
            "Epoch 306/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9264 - acc: 0.3301 - val_loss: 1.7048 - val_acc: 0.4858\n",
            "Epoch 307/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9577 - acc: 0.3079 - val_loss: 2.8590 - val_acc: 0.2386\n",
            "Epoch 308/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9580 - acc: 0.3044 - val_loss: 1.7486 - val_acc: 0.4703\n",
            "Epoch 309/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9275 - acc: 0.3420 - val_loss: 1.8251 - val_acc: 0.4024\n",
            "Epoch 310/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9123 - acc: 0.3568 - val_loss: 1.7167 - val_acc: 0.4961\n",
            "Epoch 311/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8893 - acc: 0.3892 - val_loss: 1.7012 - val_acc: 0.5172\n",
            "Epoch 312/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8884 - acc: 0.3815 - val_loss: 1.7361 - val_acc: 0.4592\n",
            "Epoch 313/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8614 - acc: 0.3912 - val_loss: 1.7646 - val_acc: 0.4595\n",
            "Epoch 314/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8812 - acc: 0.3738 - val_loss: 1.7142 - val_acc: 0.5047\n",
            "Epoch 315/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8805 - acc: 0.3622 - val_loss: 1.6506 - val_acc: 0.5485\n",
            "Epoch 316/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8697 - acc: 0.3995 - val_loss: 1.6257 - val_acc: 0.5455\n",
            "Epoch 317/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8697 - acc: 0.3972 - val_loss: 1.6802 - val_acc: 0.5272\n",
            "Epoch 318/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9094 - acc: 0.3386 - val_loss: 1.7994 - val_acc: 0.3580\n",
            "Epoch 319/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9001 - acc: 0.3536 - val_loss: 1.6774 - val_acc: 0.4728\n",
            "Epoch 320/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9171 - acc: 0.3543 - val_loss: 2.0892 - val_acc: 0.2499\n",
            "Epoch 321/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9918 - acc: 0.2882 - val_loss: 1.7907 - val_acc: 0.4138\n",
            "Epoch 322/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9000 - acc: 0.3739 - val_loss: 1.7072 - val_acc: 0.4842\n",
            "Epoch 323/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8814 - acc: 0.3933 - val_loss: 1.9535 - val_acc: 0.3092\n",
            "Epoch 324/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0348 - acc: 0.2489 - val_loss: 1.9433 - val_acc: 0.2923\n",
            "Epoch 325/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9642 - acc: 0.2918 - val_loss: 1.7958 - val_acc: 0.4168\n",
            "Epoch 326/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9174 - acc: 0.3507 - val_loss: 2.0070 - val_acc: 0.3538\n",
            "Epoch 327/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9184 - acc: 0.3571 - val_loss: 1.7616 - val_acc: 0.4388\n",
            "Epoch 328/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8985 - acc: 0.3601 - val_loss: 1.7666 - val_acc: 0.3997\n",
            "Epoch 329/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9536 - acc: 0.3036 - val_loss: 1.7583 - val_acc: 0.4656\n",
            "Epoch 330/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8993 - acc: 0.3670 - val_loss: 1.6834 - val_acc: 0.5063\n",
            "Epoch 331/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9099 - acc: 0.3617 - val_loss: 1.8048 - val_acc: 0.3891\n",
            "Epoch 332/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9153 - acc: 0.3603 - val_loss: 1.7071 - val_acc: 0.5055\n",
            "Epoch 333/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9012 - acc: 0.3796 - val_loss: 1.7670 - val_acc: 0.4247\n",
            "Epoch 334/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8979 - acc: 0.3487 - val_loss: 1.7832 - val_acc: 0.4479\n",
            "Epoch 335/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8915 - acc: 0.3669 - val_loss: 1.6742 - val_acc: 0.5205\n",
            "Epoch 336/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8917 - acc: 0.3804 - val_loss: 1.6763 - val_acc: 0.5176\n",
            "Epoch 337/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8750 - acc: 0.3806 - val_loss: 1.6745 - val_acc: 0.4877\n",
            "Epoch 338/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8515 - acc: 0.4156 - val_loss: 1.6215 - val_acc: 0.5224\n",
            "Epoch 339/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8398 - acc: 0.4386 - val_loss: 1.8366 - val_acc: 0.4353\n",
            "Epoch 340/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9385 - acc: 0.3502 - val_loss: 1.8915 - val_acc: 0.3802\n",
            "Epoch 341/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8980 - acc: 0.3802 - val_loss: 1.7581 - val_acc: 0.4921\n",
            "Epoch 342/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8787 - acc: 0.3890 - val_loss: 1.6714 - val_acc: 0.5146\n",
            "Epoch 343/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8648 - acc: 0.4005 - val_loss: 1.9291 - val_acc: 0.3387\n",
            "Epoch 344/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8758 - acc: 0.3830 - val_loss: 1.6706 - val_acc: 0.5092\n",
            "Epoch 345/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8673 - acc: 0.4058 - val_loss: 1.6105 - val_acc: 0.5623\n",
            "Epoch 346/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8882 - acc: 0.3754 - val_loss: 1.9033 - val_acc: 0.3328\n",
            "Epoch 347/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8633 - acc: 0.3880 - val_loss: 1.7034 - val_acc: 0.4820\n",
            "Epoch 348/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8457 - acc: 0.4095 - val_loss: 1.6133 - val_acc: 0.5492\n",
            "Epoch 349/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8262 - acc: 0.4235 - val_loss: 1.6259 - val_acc: 0.5396\n",
            "Epoch 350/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8188 - acc: 0.4259 - val_loss: 1.6082 - val_acc: 0.5429\n",
            "Epoch 351/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8183 - acc: 0.4126 - val_loss: 1.8644 - val_acc: 0.3821\n",
            "Epoch 352/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8134 - acc: 0.4118 - val_loss: 1.6571 - val_acc: 0.5438\n",
            "Epoch 353/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.7887 - acc: 0.4360 - val_loss: 1.5682 - val_acc: 0.5820\n",
            "Epoch 354/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8153 - acc: 0.4287 - val_loss: 1.8684 - val_acc: 0.3551\n",
            "Epoch 355/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8796 - acc: 0.3749 - val_loss: 1.7053 - val_acc: 0.4723\n",
            "Epoch 356/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8508 - acc: 0.4113 - val_loss: 1.7058 - val_acc: 0.4719\n",
            "Epoch 357/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8585 - acc: 0.3929 - val_loss: 2.0439 - val_acc: 0.2959\n",
            "Epoch 358/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9327 - acc: 0.3411 - val_loss: 2.0420 - val_acc: 0.2321\n",
            "Epoch 359/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.0625 - acc: 0.2537 - val_loss: 2.0564 - val_acc: 0.2589\n",
            "Epoch 360/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.0448 - acc: 0.2669 - val_loss: 1.9028 - val_acc: 0.4340\n",
            "Epoch 361/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0150 - acc: 0.3008 - val_loss: 1.9742 - val_acc: 0.3096\n",
            "Epoch 362/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0162 - acc: 0.2790 - val_loss: 1.8648 - val_acc: 0.4457\n",
            "Epoch 363/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.0488 - acc: 0.2753 - val_loss: 1.9573 - val_acc: 0.3824\n",
            "Epoch 364/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0104 - acc: 0.3001 - val_loss: 1.8447 - val_acc: 0.4287\n",
            "Epoch 365/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9854 - acc: 0.3331 - val_loss: 1.8813 - val_acc: 0.4061\n",
            "Epoch 366/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9826 - acc: 0.3185 - val_loss: 1.8056 - val_acc: 0.4916\n",
            "Epoch 367/500\n",
            "42000/42000 [==============================] - 1s 19us/step - loss: 1.9546 - acc: 0.3550 - val_loss: 1.7705 - val_acc: 0.4824\n",
            "Epoch 368/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9447 - acc: 0.3498 - val_loss: 1.9172 - val_acc: 0.4030\n",
            "Epoch 369/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9669 - acc: 0.3302 - val_loss: 1.7914 - val_acc: 0.5172\n",
            "Epoch 370/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9316 - acc: 0.3674 - val_loss: 1.7957 - val_acc: 0.4763\n",
            "Epoch 371/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9161 - acc: 0.3667 - val_loss: 1.7142 - val_acc: 0.5162\n",
            "Epoch 372/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9544 - acc: 0.3220 - val_loss: 1.8110 - val_acc: 0.4769\n",
            "Epoch 373/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9086 - acc: 0.3735 - val_loss: 1.6808 - val_acc: 0.5358\n",
            "Epoch 374/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8780 - acc: 0.3871 - val_loss: 1.7169 - val_acc: 0.5224\n",
            "Epoch 375/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8694 - acc: 0.3914 - val_loss: 1.6845 - val_acc: 0.5518\n",
            "Epoch 376/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9632 - acc: 0.3059 - val_loss: 1.9801 - val_acc: 0.2761\n",
            "Epoch 377/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9706 - acc: 0.2871 - val_loss: 1.8282 - val_acc: 0.3839\n",
            "Epoch 378/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9216 - acc: 0.3354 - val_loss: 1.7446 - val_acc: 0.5029\n",
            "Epoch 379/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8953 - acc: 0.3750 - val_loss: 1.7482 - val_acc: 0.5115\n",
            "Epoch 380/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8836 - acc: 0.3828 - val_loss: 1.8978 - val_acc: 0.3811\n",
            "Epoch 381/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9631 - acc: 0.2985 - val_loss: 2.1901 - val_acc: 0.1738\n",
            "Epoch 382/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0047 - acc: 0.2559 - val_loss: 1.8700 - val_acc: 0.3618\n",
            "Epoch 383/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9305 - acc: 0.3305 - val_loss: 1.7107 - val_acc: 0.5008\n",
            "Epoch 384/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9170 - acc: 0.3582 - val_loss: 1.7267 - val_acc: 0.5156\n",
            "Epoch 385/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8930 - acc: 0.3750 - val_loss: 1.6824 - val_acc: 0.5206\n",
            "Epoch 386/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8902 - acc: 0.3835 - val_loss: 1.7041 - val_acc: 0.5120\n",
            "Epoch 387/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8736 - acc: 0.3868 - val_loss: 1.6710 - val_acc: 0.5391\n",
            "Epoch 388/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8641 - acc: 0.3926 - val_loss: 1.6488 - val_acc: 0.5503\n",
            "Epoch 389/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8776 - acc: 0.3857 - val_loss: 1.6928 - val_acc: 0.4851\n",
            "Epoch 390/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8575 - acc: 0.3909 - val_loss: 1.6065 - val_acc: 0.5499\n",
            "Epoch 391/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8360 - acc: 0.4048 - val_loss: 1.5929 - val_acc: 0.5618\n",
            "Epoch 392/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8576 - acc: 0.4095 - val_loss: 1.6246 - val_acc: 0.5643\n",
            "Epoch 393/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8652 - acc: 0.3878 - val_loss: 1.7180 - val_acc: 0.4619\n",
            "Epoch 394/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8625 - acc: 0.3925 - val_loss: 1.6834 - val_acc: 0.5214\n",
            "Epoch 395/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8534 - acc: 0.4083 - val_loss: 1.6316 - val_acc: 0.5604\n",
            "Epoch 396/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8375 - acc: 0.4006 - val_loss: 1.7220 - val_acc: 0.4844\n",
            "Epoch 397/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8275 - acc: 0.3980 - val_loss: 1.6788 - val_acc: 0.5137\n",
            "Epoch 398/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8265 - acc: 0.4131 - val_loss: 1.8241 - val_acc: 0.3980\n",
            "Epoch 399/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8324 - acc: 0.4181 - val_loss: 1.5959 - val_acc: 0.5449\n",
            "Epoch 400/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9008 - acc: 0.3647 - val_loss: 2.0714 - val_acc: 0.2369\n",
            "Epoch 401/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9996 - acc: 0.2790 - val_loss: 1.9276 - val_acc: 0.3577\n",
            "Epoch 402/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9117 - acc: 0.3465 - val_loss: 1.7130 - val_acc: 0.5054\n",
            "Epoch 403/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0191 - acc: 0.2645 - val_loss: 1.9533 - val_acc: 0.3401\n",
            "Epoch 404/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9791 - acc: 0.2992 - val_loss: 1.8194 - val_acc: 0.4491\n",
            "Epoch 405/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0018 - acc: 0.2832 - val_loss: 1.9649 - val_acc: 0.2891\n",
            "Epoch 406/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0353 - acc: 0.2681 - val_loss: 1.8831 - val_acc: 0.3927\n",
            "Epoch 407/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.0090 - acc: 0.2979 - val_loss: 1.8343 - val_acc: 0.4461\n",
            "Epoch 408/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9922 - acc: 0.3199 - val_loss: 1.8932 - val_acc: 0.3936\n",
            "Epoch 409/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0140 - acc: 0.3061 - val_loss: 1.8007 - val_acc: 0.4711\n",
            "Epoch 410/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9875 - acc: 0.3234 - val_loss: 1.8592 - val_acc: 0.4583\n",
            "Epoch 411/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9642 - acc: 0.3351 - val_loss: 1.7444 - val_acc: 0.5058\n",
            "Epoch 412/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9719 - acc: 0.3227 - val_loss: 1.8145 - val_acc: 0.4267\n",
            "Epoch 413/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9346 - acc: 0.3488 - val_loss: 1.7201 - val_acc: 0.4949\n",
            "Epoch 414/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9314 - acc: 0.3460 - val_loss: 1.7959 - val_acc: 0.4622\n",
            "Epoch 415/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9254 - acc: 0.3532 - val_loss: 1.7924 - val_acc: 0.4725\n",
            "Epoch 416/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9123 - acc: 0.3590 - val_loss: 1.7437 - val_acc: 0.4824\n",
            "Epoch 417/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8939 - acc: 0.3779 - val_loss: 1.6546 - val_acc: 0.5408\n",
            "Epoch 418/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9166 - acc: 0.3824 - val_loss: 1.9836 - val_acc: 0.3158\n",
            "Epoch 419/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9621 - acc: 0.2945 - val_loss: 1.9225 - val_acc: 0.3290\n",
            "Epoch 420/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9487 - acc: 0.3213 - val_loss: 2.0042 - val_acc: 0.3384\n",
            "Epoch 421/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9865 - acc: 0.3074 - val_loss: 1.8572 - val_acc: 0.4117\n",
            "Epoch 422/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9490 - acc: 0.3385 - val_loss: 1.7456 - val_acc: 0.4908\n",
            "Epoch 423/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9115 - acc: 0.3784 - val_loss: 1.6714 - val_acc: 0.5193\n",
            "Epoch 424/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8966 - acc: 0.3943 - val_loss: 1.6670 - val_acc: 0.5311\n",
            "Epoch 425/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8831 - acc: 0.3942 - val_loss: 1.6871 - val_acc: 0.5164\n",
            "Epoch 426/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8616 - acc: 0.4000 - val_loss: 1.6294 - val_acc: 0.5545\n",
            "Epoch 427/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8573 - acc: 0.4104 - val_loss: 1.6870 - val_acc: 0.4799\n",
            "Epoch 428/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8425 - acc: 0.4065 - val_loss: 1.6157 - val_acc: 0.5493\n",
            "Epoch 429/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8488 - acc: 0.4235 - val_loss: 1.7000 - val_acc: 0.5038\n",
            "Epoch 430/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8524 - acc: 0.3870 - val_loss: 1.6366 - val_acc: 0.5362\n",
            "Epoch 431/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8410 - acc: 0.4144 - val_loss: 1.6607 - val_acc: 0.5336\n",
            "Epoch 432/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8384 - acc: 0.4132 - val_loss: 1.6315 - val_acc: 0.5401\n",
            "Epoch 433/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8442 - acc: 0.4201 - val_loss: 1.5970 - val_acc: 0.5656\n",
            "Epoch 434/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8370 - acc: 0.4186 - val_loss: 1.6112 - val_acc: 0.5606\n",
            "Epoch 435/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8300 - acc: 0.4173 - val_loss: 2.2008 - val_acc: 0.4556\n",
            "Epoch 436/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8221 - acc: 0.4376 - val_loss: 2.2132 - val_acc: 0.3061\n",
            "Epoch 437/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8999 - acc: 0.3582 - val_loss: 1.7862 - val_acc: 0.4164\n",
            "Epoch 438/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8286 - acc: 0.4032 - val_loss: 1.7228 - val_acc: 0.4889\n",
            "Epoch 439/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8480 - acc: 0.3984 - val_loss: 1.7259 - val_acc: 0.4952\n",
            "Epoch 440/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8423 - acc: 0.4126 - val_loss: 1.7875 - val_acc: 0.4611\n",
            "Epoch 441/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8774 - acc: 0.3961 - val_loss: 2.0799 - val_acc: 0.2903\n",
            "Epoch 442/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0535 - acc: 0.2490 - val_loss: 1.9371 - val_acc: 0.3669\n",
            "Epoch 443/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9668 - acc: 0.3090 - val_loss: 1.7816 - val_acc: 0.4547\n",
            "Epoch 444/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9125 - acc: 0.3740 - val_loss: 1.7272 - val_acc: 0.4754\n",
            "Epoch 445/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9562 - acc: 0.3266 - val_loss: 1.8242 - val_acc: 0.4364\n",
            "Epoch 446/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0158 - acc: 0.3086 - val_loss: 2.2212 - val_acc: 0.2296\n",
            "Epoch 447/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0494 - acc: 0.2782 - val_loss: 1.9210 - val_acc: 0.4075\n",
            "Epoch 448/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9700 - acc: 0.3297 - val_loss: 1.7938 - val_acc: 0.4543\n",
            "Epoch 449/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1177 - acc: 0.2212 - val_loss: 2.1330 - val_acc: 0.2676\n",
            "Epoch 450/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.1274 - acc: 0.2292 - val_loss: 2.0677 - val_acc: 0.3507\n",
            "Epoch 451/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0882 - acc: 0.2613 - val_loss: 1.9881 - val_acc: 0.3486\n",
            "Epoch 452/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0370 - acc: 0.2921 - val_loss: 1.9068 - val_acc: 0.4386\n",
            "Epoch 453/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.0077 - acc: 0.3102 - val_loss: 1.8536 - val_acc: 0.4287\n",
            "Epoch 454/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9732 - acc: 0.3341 - val_loss: 1.7860 - val_acc: 0.4606\n",
            "Epoch 455/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9363 - acc: 0.3479 - val_loss: 1.7654 - val_acc: 0.4852\n",
            "Epoch 456/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9337 - acc: 0.3533 - val_loss: 1.8073 - val_acc: 0.4426\n",
            "Epoch 457/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9226 - acc: 0.3581 - val_loss: 1.7020 - val_acc: 0.5158\n",
            "Epoch 458/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9273 - acc: 0.3773 - val_loss: 1.7689 - val_acc: 0.4499\n",
            "Epoch 459/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9003 - acc: 0.3688 - val_loss: 1.6852 - val_acc: 0.5218\n",
            "Epoch 460/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9075 - acc: 0.3601 - val_loss: 1.8228 - val_acc: 0.4304\n",
            "Epoch 461/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9194 - acc: 0.3504 - val_loss: 1.7273 - val_acc: 0.4927\n",
            "Epoch 462/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8865 - acc: 0.3825 - val_loss: 1.6659 - val_acc: 0.5347\n",
            "Epoch 463/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8910 - acc: 0.3865 - val_loss: 1.7297 - val_acc: 0.4847\n",
            "Epoch 464/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8793 - acc: 0.3918 - val_loss: 1.6612 - val_acc: 0.5190\n",
            "Epoch 465/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8844 - acc: 0.4011 - val_loss: 1.8327 - val_acc: 0.3864\n",
            "Epoch 466/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9794 - acc: 0.2661 - val_loss: 1.8345 - val_acc: 0.3803\n",
            "Epoch 467/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9463 - acc: 0.3178 - val_loss: 1.7582 - val_acc: 0.4796\n",
            "Epoch 468/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9222 - acc: 0.3568 - val_loss: 1.6812 - val_acc: 0.5241\n",
            "Epoch 469/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9460 - acc: 0.3453 - val_loss: 1.8832 - val_acc: 0.3888\n",
            "Epoch 470/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.9566 - acc: 0.3281 - val_loss: 1.8512 - val_acc: 0.4854\n",
            "Epoch 471/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9314 - acc: 0.3543 - val_loss: 1.7606 - val_acc: 0.5169\n",
            "Epoch 472/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9480 - acc: 0.3288 - val_loss: 1.8085 - val_acc: 0.4733\n",
            "Epoch 473/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9333 - acc: 0.3433 - val_loss: 1.7414 - val_acc: 0.5067\n",
            "Epoch 474/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9195 - acc: 0.3549 - val_loss: 1.7980 - val_acc: 0.3886\n",
            "Epoch 475/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9267 - acc: 0.3296 - val_loss: 1.7630 - val_acc: 0.4906\n",
            "Epoch 476/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8894 - acc: 0.3716 - val_loss: 1.7304 - val_acc: 0.5097\n",
            "Epoch 477/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8988 - acc: 0.3681 - val_loss: 1.8601 - val_acc: 0.3434\n",
            "Epoch 478/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9184 - acc: 0.3478 - val_loss: 1.8021 - val_acc: 0.4370\n",
            "Epoch 479/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9096 - acc: 0.3834 - val_loss: 1.6556 - val_acc: 0.5384\n",
            "Epoch 480/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.9102 - acc: 0.3799 - val_loss: 1.6959 - val_acc: 0.4883\n",
            "Epoch 481/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8778 - acc: 0.3800 - val_loss: 1.6788 - val_acc: 0.5381\n",
            "Epoch 482/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8720 - acc: 0.3914 - val_loss: 1.6543 - val_acc: 0.5352\n",
            "Epoch 483/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8616 - acc: 0.3966 - val_loss: 1.7079 - val_acc: 0.5133\n",
            "Epoch 484/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8676 - acc: 0.3841 - val_loss: 1.6597 - val_acc: 0.5169\n",
            "Epoch 485/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8683 - acc: 0.4098 - val_loss: 1.6967 - val_acc: 0.4973\n",
            "Epoch 486/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8662 - acc: 0.3967 - val_loss: 1.6103 - val_acc: 0.5480\n",
            "Epoch 487/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8511 - acc: 0.4118 - val_loss: 1.8693 - val_acc: 0.3864\n",
            "Epoch 488/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.9177 - acc: 0.3259 - val_loss: 1.7619 - val_acc: 0.4485\n",
            "Epoch 489/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8664 - acc: 0.3790 - val_loss: 1.7165 - val_acc: 0.5063\n",
            "Epoch 490/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8516 - acc: 0.3928 - val_loss: 1.6525 - val_acc: 0.5564\n",
            "Epoch 491/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8446 - acc: 0.4084 - val_loss: 1.6191 - val_acc: 0.5583\n",
            "Epoch 492/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8373 - acc: 0.4197 - val_loss: 1.6188 - val_acc: 0.5671\n",
            "Epoch 493/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 1.8411 - acc: 0.4105 - val_loss: 1.6180 - val_acc: 0.5603\n",
            "Epoch 494/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8259 - acc: 0.4149 - val_loss: 1.6557 - val_acc: 0.5423\n",
            "Epoch 495/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8212 - acc: 0.4241 - val_loss: 1.6338 - val_acc: 0.5343\n",
            "Epoch 496/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8090 - acc: 0.4275 - val_loss: 1.5777 - val_acc: 0.5822\n",
            "Epoch 497/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8510 - acc: 0.3891 - val_loss: 1.7837 - val_acc: 0.4309\n",
            "Epoch 498/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8911 - acc: 0.3597 - val_loss: 1.7089 - val_acc: 0.4700\n",
            "Epoch 499/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.8407 - acc: 0.3973 - val_loss: 1.6385 - val_acc: 0.5356\n",
            "Epoch 500/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.8383 - acc: 0.4279 - val_loss: 1.6376 - val_acc: 0.5419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1081e8d208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb5Y_lbsXULE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7eb6d92-86ff-4d01-ac52-ce09a0e70572"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(1024, input_shape = (1024,), activation = 'linear'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1024, activation = 'linear'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(10, activation = 'linear'))\n",
        "sgd = optimizers.Adam(lr = 0.3)\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()\n",
        "model2.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,113,546\n",
            "Trainable params: 2,111,498\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 1s 30us/step - loss: 3.0428 - acc: 0.1024 - val_loss: 2.3068 - val_acc: 0.1037\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3082 - acc: 0.1017 - val_loss: 2.3034 - val_acc: 0.1068\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3055 - acc: 0.1040 - val_loss: 2.3029 - val_acc: 0.1015\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3062 - acc: 0.1004 - val_loss: 2.3023 - val_acc: 0.1063\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3045 - acc: 0.1023 - val_loss: 2.3028 - val_acc: 0.1011\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3049 - acc: 0.1036 - val_loss: 2.3021 - val_acc: 0.1077\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3047 - acc: 0.1003 - val_loss: 2.3027 - val_acc: 0.0976\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3038 - acc: 0.1034 - val_loss: 2.3017 - val_acc: 0.1071\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3035 - acc: 0.1060 - val_loss: 2.3016 - val_acc: 0.1071\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3031 - acc: 0.1061 - val_loss: 2.3015 - val_acc: 0.1058\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1056 - val_loss: 2.3015 - val_acc: 0.0998\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3021 - acc: 0.1058 - val_loss: 2.3007 - val_acc: 0.1100\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3026 - acc: 0.1069 - val_loss: 2.2988 - val_acc: 0.1154\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3132 - acc: 0.1078 - val_loss: 2.3060 - val_acc: 0.1012\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3042 - acc: 0.1038 - val_loss: 2.3014 - val_acc: 0.0974\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1071 - val_loss: 2.3013 - val_acc: 0.1050\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3021 - acc: 0.1062 - val_loss: 2.3009 - val_acc: 0.1080\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3017 - acc: 0.1075 - val_loss: 2.2994 - val_acc: 0.1051\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.2997 - acc: 0.1109 - val_loss: 2.2979 - val_acc: 0.0971\n",
            "Epoch 20/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3038 - acc: 0.1121 - val_loss: 2.3081 - val_acc: 0.1148\n",
            "Epoch 21/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3265 - acc: 0.1068 - val_loss: 2.3018 - val_acc: 0.0923\n",
            "Epoch 22/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3017 - acc: 0.1063 - val_loss: 2.2998 - val_acc: 0.0958\n",
            "Epoch 23/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3111 - acc: 0.1136 - val_loss: 2.3154 - val_acc: 0.1054\n",
            "Epoch 24/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3045 - acc: 0.1025 - val_loss: 2.2994 - val_acc: 0.0998\n",
            "Epoch 25/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.2995 - acc: 0.1122 - val_loss: 2.2969 - val_acc: 0.1114\n",
            "Epoch 26/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3651 - acc: 0.1112 - val_loss: 3.8424 - val_acc: 0.0936\n",
            "Epoch 27/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3128 - acc: 0.1064 - val_loss: 2.3139 - val_acc: 0.0971\n",
            "Epoch 28/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3078 - acc: 0.1000 - val_loss: 2.3042 - val_acc: 0.0986\n",
            "Epoch 29/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3041 - acc: 0.1002 - val_loss: 2.3032 - val_acc: 0.0958\n",
            "Epoch 30/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3039 - acc: 0.1013 - val_loss: 2.3029 - val_acc: 0.0939\n",
            "Epoch 31/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3035 - acc: 0.1034 - val_loss: 2.3025 - val_acc: 0.0969\n",
            "Epoch 32/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3030 - acc: 0.1025 - val_loss: 2.3020 - val_acc: 0.1001\n",
            "Epoch 33/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1008 - val_loss: 2.3017 - val_acc: 0.1064\n",
            "Epoch 34/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3026 - acc: 0.1035 - val_loss: 2.3017 - val_acc: 0.1158\n",
            "Epoch 35/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3020 - acc: 0.1069 - val_loss: 2.3013 - val_acc: 0.1153\n",
            "Epoch 36/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3015 - acc: 0.1058 - val_loss: 2.3008 - val_acc: 0.1212\n",
            "Epoch 37/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3006 - acc: 0.1092 - val_loss: 2.3000 - val_acc: 0.1134\n",
            "Epoch 38/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.2994 - acc: 0.1116 - val_loss: 2.2990 - val_acc: 0.1203\n",
            "Epoch 39/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.2977 - acc: 0.1179 - val_loss: 2.2970 - val_acc: 0.1234\n",
            "Epoch 40/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.2968 - acc: 0.1166 - val_loss: 2.2940 - val_acc: 0.1250\n",
            "Epoch 41/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3385 - acc: 0.1189 - val_loss: 2.3308 - val_acc: 0.1237\n",
            "Epoch 42/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3315 - acc: 0.1023 - val_loss: 2.3304 - val_acc: 0.1102\n",
            "Epoch 43/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3026 - acc: 0.1097 - val_loss: 2.3000 - val_acc: 0.1132\n",
            "Epoch 44/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3005 - acc: 0.1135 - val_loss: 2.2994 - val_acc: 0.1229\n",
            "Epoch 45/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.2975 - acc: 0.1174 - val_loss: 2.2968 - val_acc: 0.1299\n",
            "Epoch 46/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3825 - acc: 0.1196 - val_loss: 2.3104 - val_acc: 0.0958\n",
            "Epoch 47/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.4350 - acc: 0.0993 - val_loss: 2.4391 - val_acc: 0.1068\n",
            "Epoch 48/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3760 - acc: 0.1010 - val_loss: 2.3365 - val_acc: 0.0986\n",
            "Epoch 49/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3647 - acc: 0.1008 - val_loss: 2.3035 - val_acc: 0.1008\n",
            "Epoch 50/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3056 - acc: 0.1025 - val_loss: 2.3029 - val_acc: 0.0987\n",
            "Epoch 51/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3051 - acc: 0.1017 - val_loss: 2.3028 - val_acc: 0.1018\n",
            "Epoch 52/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3044 - acc: 0.1005 - val_loss: 2.3028 - val_acc: 0.1006\n",
            "Epoch 53/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3040 - acc: 0.1009 - val_loss: 2.3026 - val_acc: 0.1020\n",
            "Epoch 54/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3041 - acc: 0.0995 - val_loss: 2.3025 - val_acc: 0.0986\n",
            "Epoch 55/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3039 - acc: 0.1014 - val_loss: 2.3025 - val_acc: 0.0957\n",
            "Epoch 56/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3036 - acc: 0.1001 - val_loss: 2.3024 - val_acc: 0.0882\n",
            "Epoch 57/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3036 - acc: 0.0996 - val_loss: 2.3024 - val_acc: 0.0867\n",
            "Epoch 58/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3037 - acc: 0.0999 - val_loss: 2.3025 - val_acc: 0.1059\n",
            "Epoch 59/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3033 - acc: 0.0981 - val_loss: 2.3024 - val_acc: 0.1040\n",
            "Epoch 60/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3033 - acc: 0.0981 - val_loss: 2.3022 - val_acc: 0.0997\n",
            "Epoch 61/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3034 - acc: 0.1002 - val_loss: 2.3023 - val_acc: 0.0972\n",
            "Epoch 62/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0979 - val_loss: 2.3022 - val_acc: 0.1038\n",
            "Epoch 63/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0986 - val_loss: 2.3022 - val_acc: 0.0957\n",
            "Epoch 64/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3024 - acc: 0.0963 - val_loss: 2.3020 - val_acc: 0.0979\n",
            "Epoch 65/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.0966 - val_loss: 2.3020 - val_acc: 0.0979\n",
            "Epoch 66/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.0987 - val_loss: 2.3019 - val_acc: 0.0938\n",
            "Epoch 67/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3023 - acc: 0.0985 - val_loss: 2.3019 - val_acc: 0.1029\n",
            "Epoch 68/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3023 - acc: 0.0984 - val_loss: 2.3018 - val_acc: 0.0936\n",
            "Epoch 69/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3020 - acc: 0.0997 - val_loss: 2.3012 - val_acc: 0.0884\n",
            "Epoch 70/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3016 - acc: 0.0967 - val_loss: 2.3008 - val_acc: 0.1066\n",
            "Epoch 71/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3009 - acc: 0.1007 - val_loss: 2.3001 - val_acc: 0.0965\n",
            "Epoch 72/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3001 - acc: 0.1005 - val_loss: 2.2989 - val_acc: 0.0994\n",
            "Epoch 73/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.2997 - acc: 0.0976 - val_loss: 2.3020 - val_acc: 0.0847\n",
            "Epoch 74/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4929 - acc: 0.0945 - val_loss: 2.3351 - val_acc: 0.1022\n",
            "Epoch 75/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4432 - acc: 0.0974 - val_loss: 2.4191 - val_acc: 0.0974\n",
            "Epoch 76/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4406 - acc: 0.0987 - val_loss: 2.6799 - val_acc: 0.0987\n",
            "Epoch 77/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.5119 - acc: 0.0995 - val_loss: 2.3135 - val_acc: 0.1024\n",
            "Epoch 78/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3167 - acc: 0.1029 - val_loss: 2.3060 - val_acc: 0.1016\n",
            "Epoch 79/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3082 - acc: 0.1006 - val_loss: 2.3044 - val_acc: 0.1006\n",
            "Epoch 80/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3051 - acc: 0.0991 - val_loss: 2.3038 - val_acc: 0.1014\n",
            "Epoch 81/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3047 - acc: 0.0999 - val_loss: 2.3033 - val_acc: 0.0996\n",
            "Epoch 82/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3043 - acc: 0.1010 - val_loss: 2.3031 - val_acc: 0.0956\n",
            "Epoch 83/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3036 - acc: 0.0990 - val_loss: 2.3029 - val_acc: 0.1018\n",
            "Epoch 84/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3035 - acc: 0.1013 - val_loss: 2.3028 - val_acc: 0.1033\n",
            "Epoch 85/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.0972 - val_loss: 2.3027 - val_acc: 0.1010\n",
            "Epoch 86/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3033 - acc: 0.1015 - val_loss: 2.3026 - val_acc: 0.0991\n",
            "Epoch 87/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3035 - acc: 0.0999 - val_loss: 2.3026 - val_acc: 0.0974\n",
            "Epoch 88/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3033 - acc: 0.0989 - val_loss: 2.3025 - val_acc: 0.0960\n",
            "Epoch 89/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3030 - acc: 0.1012 - val_loss: 2.3026 - val_acc: 0.0972\n",
            "Epoch 90/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.1001 - val_loss: 2.3025 - val_acc: 0.1003\n",
            "Epoch 91/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3035 - acc: 0.1004 - val_loss: 2.3024 - val_acc: 0.0996\n",
            "Epoch 92/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3030 - acc: 0.1009 - val_loss: 2.3024 - val_acc: 0.1001\n",
            "Epoch 93/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3035 - acc: 0.1012 - val_loss: 2.3023 - val_acc: 0.0963\n",
            "Epoch 94/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0985 - val_loss: 2.3023 - val_acc: 0.0984\n",
            "Epoch 95/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0994 - val_loss: 2.3023 - val_acc: 0.0984\n",
            "Epoch 96/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3030 - acc: 0.1014 - val_loss: 2.3022 - val_acc: 0.0989\n",
            "Epoch 97/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3039 - acc: 0.1001 - val_loss: 2.3022 - val_acc: 0.1003\n",
            "Epoch 98/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.0970 - val_loss: 2.3021 - val_acc: 0.0965\n",
            "Epoch 99/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.0995 - val_loss: 2.3021 - val_acc: 0.0934\n",
            "Epoch 100/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0983 - val_loss: 2.3021 - val_acc: 0.0896\n",
            "Epoch 101/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0999 - val_loss: 2.3019 - val_acc: 0.0968\n",
            "Epoch 102/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0996 - val_loss: 2.3021 - val_acc: 0.0998\n",
            "Epoch 103/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.0977 - val_loss: 2.3019 - val_acc: 0.0935\n",
            "Epoch 104/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0990 - val_loss: 2.3018 - val_acc: 0.0966\n",
            "Epoch 105/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.0972 - val_loss: 2.3031 - val_acc: 0.0992\n",
            "Epoch 106/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3074 - acc: 0.0964 - val_loss: 2.3040 - val_acc: 0.1017\n",
            "Epoch 107/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3042 - acc: 0.0967 - val_loss: 2.3032 - val_acc: 0.0962\n",
            "Epoch 108/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3033 - acc: 0.0997 - val_loss: 2.3028 - val_acc: 0.1003\n",
            "Epoch 109/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3031 - acc: 0.0999 - val_loss: 2.3028 - val_acc: 0.0985\n",
            "Epoch 110/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0961 - val_loss: 2.3026 - val_acc: 0.0968\n",
            "Epoch 111/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0966 - val_loss: 2.3027 - val_acc: 0.0950\n",
            "Epoch 112/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3033 - acc: 0.0984 - val_loss: 2.3027 - val_acc: 0.0939\n",
            "Epoch 113/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0980 - val_loss: 2.3025 - val_acc: 0.0981\n",
            "Epoch 114/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.0950 - val_loss: 2.3026 - val_acc: 0.0903\n",
            "Epoch 115/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.0963 - val_loss: 2.3024 - val_acc: 0.0933\n",
            "Epoch 116/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.0990 - val_loss: 2.3025 - val_acc: 0.0985\n",
            "Epoch 117/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3025 - acc: 0.0933 - val_loss: 2.3023 - val_acc: 0.0900\n",
            "Epoch 118/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3023 - acc: 0.0937 - val_loss: 2.3022 - val_acc: 0.0929\n",
            "Epoch 119/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.0949 - val_loss: 2.3021 - val_acc: 0.0899\n",
            "Epoch 120/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3018 - acc: 0.0918 - val_loss: 2.3018 - val_acc: 0.0918\n",
            "Epoch 121/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3020 - acc: 0.0920 - val_loss: 2.3016 - val_acc: 0.0928\n",
            "Epoch 122/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3017 - acc: 0.0911 - val_loss: 2.3012 - val_acc: 0.0867\n",
            "Epoch 123/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3012 - acc: 0.0922 - val_loss: 2.3005 - val_acc: 0.0974\n",
            "Epoch 124/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.2994 - acc: 0.0929 - val_loss: 2.3001 - val_acc: 0.0843\n",
            "Epoch 125/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.2989 - acc: 0.0930 - val_loss: 2.2987 - val_acc: 0.0832\n",
            "Epoch 126/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.2969 - acc: 0.0932 - val_loss: 2.2951 - val_acc: 0.0984\n",
            "Epoch 127/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3631 - acc: 0.0926 - val_loss: 2.4879 - val_acc: 0.1011\n",
            "Epoch 128/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4307 - acc: 0.0966 - val_loss: 2.4645 - val_acc: 0.0967\n",
            "Epoch 129/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.4050 - acc: 0.0998 - val_loss: 2.4028 - val_acc: 0.0996\n",
            "Epoch 130/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.4012 - acc: 0.0976 - val_loss: 2.3825 - val_acc: 0.0998\n",
            "Epoch 131/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3855 - acc: 0.0978 - val_loss: 2.3986 - val_acc: 0.0942\n",
            "Epoch 132/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4022 - acc: 0.0983 - val_loss: 2.3806 - val_acc: 0.0983\n",
            "Epoch 133/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.4053 - acc: 0.1010 - val_loss: 2.3734 - val_acc: 0.0979\n",
            "Epoch 134/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3988 - acc: 0.0979 - val_loss: 2.4375 - val_acc: 0.0949\n",
            "Epoch 135/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.4381 - acc: 0.0973 - val_loss: 2.4589 - val_acc: 0.0999\n",
            "Epoch 136/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.4042 - acc: 0.1006 - val_loss: 2.3681 - val_acc: 0.0978\n",
            "Epoch 137/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3770 - acc: 0.0983 - val_loss: 2.3580 - val_acc: 0.0902\n",
            "Epoch 138/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3528 - acc: 0.0968 - val_loss: 2.3696 - val_acc: 0.0923\n",
            "Epoch 139/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3883 - acc: 0.0986 - val_loss: 2.3990 - val_acc: 0.1012\n",
            "Epoch 140/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3732 - acc: 0.1009 - val_loss: 2.3430 - val_acc: 0.0967\n",
            "Epoch 141/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3512 - acc: 0.1002 - val_loss: 2.3715 - val_acc: 0.0997\n",
            "Epoch 142/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3634 - acc: 0.1008 - val_loss: 2.3602 - val_acc: 0.1001\n",
            "Epoch 143/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3648 - acc: 0.0997 - val_loss: 2.3559 - val_acc: 0.0993\n",
            "Epoch 144/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3518 - acc: 0.1000 - val_loss: 2.3358 - val_acc: 0.1004\n",
            "Epoch 145/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3397 - acc: 0.1001 - val_loss: 2.3406 - val_acc: 0.0971\n",
            "Epoch 146/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3580 - acc: 0.1006 - val_loss: 2.3597 - val_acc: 0.1008\n",
            "Epoch 147/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3594 - acc: 0.1006 - val_loss: 2.3559 - val_acc: 0.0973\n",
            "Epoch 148/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3564 - acc: 0.1008 - val_loss: 2.3751 - val_acc: 0.0991\n",
            "Epoch 149/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3821 - acc: 0.0964 - val_loss: 2.3444 - val_acc: 0.0994\n",
            "Epoch 150/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3496 - acc: 0.0995 - val_loss: 2.3152 - val_acc: 0.0966\n",
            "Epoch 151/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3232 - acc: 0.0995 - val_loss: 2.3146 - val_acc: 0.0974\n",
            "Epoch 152/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3262 - acc: 0.0988 - val_loss: 2.3084 - val_acc: 0.0974\n",
            "Epoch 153/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3112 - acc: 0.1012 - val_loss: 2.3098 - val_acc: 0.0973\n",
            "Epoch 154/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3073 - acc: 0.0985 - val_loss: 2.3030 - val_acc: 0.1012\n",
            "Epoch 155/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3046 - acc: 0.0987 - val_loss: 2.3030 - val_acc: 0.1000\n",
            "Epoch 156/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3041 - acc: 0.0999 - val_loss: 2.3030 - val_acc: 0.0989\n",
            "Epoch 157/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3039 - acc: 0.0988 - val_loss: 2.3029 - val_acc: 0.0973\n",
            "Epoch 158/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3039 - acc: 0.0988 - val_loss: 2.3028 - val_acc: 0.0971\n",
            "Epoch 159/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3050 - acc: 0.1018 - val_loss: 2.3027 - val_acc: 0.0962\n",
            "Epoch 160/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3036 - acc: 0.0998 - val_loss: 2.3027 - val_acc: 0.0952\n",
            "Epoch 161/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3036 - acc: 0.1012 - val_loss: 2.3027 - val_acc: 0.0957\n",
            "Epoch 162/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3042 - acc: 0.1007 - val_loss: 2.3027 - val_acc: 0.0947\n",
            "Epoch 163/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3039 - acc: 0.0995 - val_loss: 2.3026 - val_acc: 0.0953\n",
            "Epoch 164/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1041 - val_loss: 2.3026 - val_acc: 0.0958\n",
            "Epoch 165/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.1035 - val_loss: 2.3026 - val_acc: 0.0955\n",
            "Epoch 166/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3031 - acc: 0.1011 - val_loss: 2.3026 - val_acc: 0.0953\n",
            "Epoch 167/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3030 - acc: 0.1002 - val_loss: 2.3026 - val_acc: 0.0950\n",
            "Epoch 168/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3033 - acc: 0.0999 - val_loss: 2.3026 - val_acc: 0.0962\n",
            "Epoch 169/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1037 - val_loss: 2.3026 - val_acc: 0.0961\n",
            "Epoch 170/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3032 - acc: 0.1027 - val_loss: 2.3026 - val_acc: 0.0951\n",
            "Epoch 171/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.1004 - val_loss: 2.3026 - val_acc: 0.0938\n",
            "Epoch 172/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3033 - acc: 0.1022 - val_loss: 2.3026 - val_acc: 0.0969\n",
            "Epoch 173/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3031 - acc: 0.0995 - val_loss: 2.3026 - val_acc: 0.0956\n",
            "Epoch 174/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1030 - val_loss: 2.3026 - val_acc: 0.0968\n",
            "Epoch 175/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3032 - acc: 0.1003 - val_loss: 2.3026 - val_acc: 0.0971\n",
            "Epoch 176/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3032 - acc: 0.1023 - val_loss: 2.3026 - val_acc: 0.0970\n",
            "Epoch 177/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1019 - val_loss: 2.3026 - val_acc: 0.0963\n",
            "Epoch 178/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3028 - acc: 0.1027 - val_loss: 2.3026 - val_acc: 0.0983\n",
            "Epoch 179/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3029 - acc: 0.1016 - val_loss: 2.3026 - val_acc: 0.0968\n",
            "Epoch 180/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1023 - val_loss: 2.3026 - val_acc: 0.0961\n",
            "Epoch 181/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3025 - acc: 0.1020 - val_loss: 2.3026 - val_acc: 0.0981\n",
            "Epoch 182/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0996 - val_loss: 2.3026 - val_acc: 0.0977\n",
            "Epoch 183/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1003 - val_loss: 2.3026 - val_acc: 0.0980\n",
            "Epoch 184/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3030 - acc: 0.1012 - val_loss: 2.3026 - val_acc: 0.1004\n",
            "Epoch 185/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3025 - acc: 0.1028 - val_loss: 2.3026 - val_acc: 0.0973\n",
            "Epoch 186/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3026 - acc: 0.1043 - val_loss: 2.3026 - val_acc: 0.0984\n",
            "Epoch 187/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.1000 - val_loss: 2.3026 - val_acc: 0.0986\n",
            "Epoch 188/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.1040 - val_loss: 2.3027 - val_acc: 0.1001\n",
            "Epoch 189/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.1005 - val_loss: 2.3027 - val_acc: 0.1054\n",
            "Epoch 190/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1042 - val_loss: 2.3026 - val_acc: 0.0998\n",
            "Epoch 191/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1013 - val_loss: 2.3025 - val_acc: 0.1017\n",
            "Epoch 192/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1022 - val_loss: 2.3026 - val_acc: 0.0998\n",
            "Epoch 193/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0993 - val_loss: 2.3025 - val_acc: 0.1027\n",
            "Epoch 194/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0997 - val_loss: 2.3025 - val_acc: 0.1040\n",
            "Epoch 195/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3033 - acc: 0.1009 - val_loss: 2.3025 - val_acc: 0.1054\n",
            "Epoch 196/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3032 - acc: 0.1007 - val_loss: 2.3025 - val_acc: 0.1008\n",
            "Epoch 197/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.1025 - val_loss: 2.3025 - val_acc: 0.1028\n",
            "Epoch 198/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1035 - val_loss: 2.3026 - val_acc: 0.1029\n",
            "Epoch 199/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1005 - val_loss: 2.3026 - val_acc: 0.0998\n",
            "Epoch 200/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.1033 - val_loss: 2.3026 - val_acc: 0.0998\n",
            "Epoch 201/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.1034 - val_loss: 2.3026 - val_acc: 0.0996\n",
            "Epoch 202/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.1023 - val_loss: 2.3025 - val_acc: 0.1036\n",
            "Epoch 203/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.1035 - val_loss: 2.3026 - val_acc: 0.1020\n",
            "Epoch 204/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.1044 - val_loss: 2.3026 - val_acc: 0.1090\n",
            "Epoch 205/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.1019 - val_loss: 2.3026 - val_acc: 0.1013\n",
            "Epoch 206/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.1030 - val_loss: 2.3025 - val_acc: 0.1059\n",
            "Epoch 207/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.1043 - val_loss: 2.3024 - val_acc: 0.1040\n",
            "Epoch 208/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.1037 - val_loss: 2.3024 - val_acc: 0.1061\n",
            "Epoch 209/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3025 - acc: 0.1023 - val_loss: 2.3023 - val_acc: 0.1024\n",
            "Epoch 210/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.1035 - val_loss: 2.3024 - val_acc: 0.1087\n",
            "Epoch 211/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.1038 - val_loss: 2.3023 - val_acc: 0.1077\n",
            "Epoch 212/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.1031 - val_loss: 2.3024 - val_acc: 0.1044\n",
            "Epoch 213/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.1010 - val_loss: 2.3023 - val_acc: 0.1059\n",
            "Epoch 214/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3024 - acc: 0.1056 - val_loss: 2.3022 - val_acc: 0.1094\n",
            "Epoch 215/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3024 - acc: 0.1053 - val_loss: 2.3024 - val_acc: 0.1036\n",
            "Epoch 216/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3021 - acc: 0.1055 - val_loss: 2.3022 - val_acc: 0.1010\n",
            "Epoch 217/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3017 - acc: 0.1063 - val_loss: 2.3018 - val_acc: 0.1087\n",
            "Epoch 218/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3018 - acc: 0.1065 - val_loss: 2.3019 - val_acc: 0.1028\n",
            "Epoch 219/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3014 - acc: 0.1079 - val_loss: 2.3014 - val_acc: 0.1057\n",
            "Epoch 220/500\n",
            "42000/42000 [==============================] - 1s 19us/step - loss: 2.3007 - acc: 0.1110 - val_loss: 2.3003 - val_acc: 0.0986\n",
            "Epoch 221/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3046 - acc: 0.1083 - val_loss: 2.3277 - val_acc: 0.1039\n",
            "Epoch 222/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3704 - acc: 0.1118 - val_loss: 2.5611 - val_acc: 0.0981\n",
            "Epoch 223/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3656 - acc: 0.1007 - val_loss: 2.3847 - val_acc: 0.0992\n",
            "Epoch 224/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3157 - acc: 0.1017 - val_loss: 2.3068 - val_acc: 0.1088\n",
            "Epoch 225/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3584 - acc: 0.1066 - val_loss: 2.4896 - val_acc: 0.1099\n",
            "Epoch 226/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4107 - acc: 0.1021 - val_loss: 2.4072 - val_acc: 0.1006\n",
            "Epoch 227/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3713 - acc: 0.0987 - val_loss: 2.3867 - val_acc: 0.0979\n",
            "Epoch 228/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3748 - acc: 0.0977 - val_loss: 2.3484 - val_acc: 0.0903\n",
            "Epoch 229/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3672 - acc: 0.1031 - val_loss: 2.3651 - val_acc: 0.0993\n",
            "Epoch 230/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3540 - acc: 0.1034 - val_loss: 2.3612 - val_acc: 0.0992\n",
            "Epoch 231/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3525 - acc: 0.1080 - val_loss: 2.3406 - val_acc: 0.1061\n",
            "Epoch 232/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3424 - acc: 0.1043 - val_loss: 2.3196 - val_acc: 0.1031\n",
            "Epoch 233/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3431 - acc: 0.1043 - val_loss: 2.3282 - val_acc: 0.1068\n",
            "Epoch 234/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3532 - acc: 0.1011 - val_loss: 2.3337 - val_acc: 0.1013\n",
            "Epoch 235/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3432 - acc: 0.1000 - val_loss: 2.3320 - val_acc: 0.1022\n",
            "Epoch 236/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3515 - acc: 0.0993 - val_loss: 2.3559 - val_acc: 0.1036\n",
            "Epoch 237/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3663 - acc: 0.0992 - val_loss: 2.3606 - val_acc: 0.1026\n",
            "Epoch 238/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3457 - acc: 0.0995 - val_loss: 2.3439 - val_acc: 0.1011\n",
            "Epoch 239/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3391 - acc: 0.0988 - val_loss: 2.3237 - val_acc: 0.0964\n",
            "Epoch 240/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3425 - acc: 0.0985 - val_loss: 2.3415 - val_acc: 0.0991\n",
            "Epoch 241/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3569 - acc: 0.1000 - val_loss: 2.3472 - val_acc: 0.0964\n",
            "Epoch 242/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3357 - acc: 0.0986 - val_loss: 2.3352 - val_acc: 0.0964\n",
            "Epoch 243/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3393 - acc: 0.0980 - val_loss: 2.3268 - val_acc: 0.0966\n",
            "Epoch 244/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3327 - acc: 0.1002 - val_loss: 2.3290 - val_acc: 0.0976\n",
            "Epoch 245/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3425 - acc: 0.0989 - val_loss: 2.3272 - val_acc: 0.0967\n",
            "Epoch 246/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3580 - acc: 0.1000 - val_loss: 2.3670 - val_acc: 0.1091\n",
            "Epoch 247/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3519 - acc: 0.0980 - val_loss: 2.3430 - val_acc: 0.0984\n",
            "Epoch 248/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3423 - acc: 0.0990 - val_loss: 2.3293 - val_acc: 0.0967\n",
            "Epoch 249/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3450 - acc: 0.0980 - val_loss: 2.3343 - val_acc: 0.1036\n",
            "Epoch 250/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3464 - acc: 0.1003 - val_loss: 2.3316 - val_acc: 0.1006\n",
            "Epoch 251/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3535 - acc: 0.1033 - val_loss: 2.3344 - val_acc: 0.1002\n",
            "Epoch 252/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3465 - acc: 0.1022 - val_loss: 2.3301 - val_acc: 0.1005\n",
            "Epoch 253/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3412 - acc: 0.0996 - val_loss: 2.3267 - val_acc: 0.1022\n",
            "Epoch 254/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3323 - acc: 0.1012 - val_loss: 2.3223 - val_acc: 0.1030\n",
            "Epoch 255/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3273 - acc: 0.0994 - val_loss: 2.3291 - val_acc: 0.1048\n",
            "Epoch 256/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3349 - acc: 0.0992 - val_loss: 2.3302 - val_acc: 0.0949\n",
            "Epoch 257/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3428 - acc: 0.0993 - val_loss: 2.3403 - val_acc: 0.0958\n",
            "Epoch 258/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3371 - acc: 0.1006 - val_loss: 2.3301 - val_acc: 0.0866\n",
            "Epoch 259/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3271 - acc: 0.0975 - val_loss: 2.3269 - val_acc: 0.0947\n",
            "Epoch 260/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3345 - acc: 0.0974 - val_loss: 2.3326 - val_acc: 0.0995\n",
            "Epoch 261/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3400 - acc: 0.0983 - val_loss: 2.3285 - val_acc: 0.0974\n",
            "Epoch 262/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3333 - acc: 0.1008 - val_loss: 2.3305 - val_acc: 0.0989\n",
            "Epoch 263/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3345 - acc: 0.1008 - val_loss: 2.3291 - val_acc: 0.0976\n",
            "Epoch 264/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3409 - acc: 0.1025 - val_loss: 2.3302 - val_acc: 0.1026\n",
            "Epoch 265/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3513 - acc: 0.0999 - val_loss: 2.3455 - val_acc: 0.0941\n",
            "Epoch 266/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3429 - acc: 0.0981 - val_loss: 2.3368 - val_acc: 0.0954\n",
            "Epoch 267/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3487 - acc: 0.0985 - val_loss: 2.3334 - val_acc: 0.0984\n",
            "Epoch 268/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3363 - acc: 0.0987 - val_loss: 2.3332 - val_acc: 0.0932\n",
            "Epoch 269/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3307 - acc: 0.0988 - val_loss: 2.3228 - val_acc: 0.0888\n",
            "Epoch 270/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3316 - acc: 0.0994 - val_loss: 2.3315 - val_acc: 0.0992\n",
            "Epoch 271/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3327 - acc: 0.0986 - val_loss: 2.3127 - val_acc: 0.1032\n",
            "Epoch 272/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3311 - acc: 0.0966 - val_loss: 2.3383 - val_acc: 0.1006\n",
            "Epoch 273/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3388 - acc: 0.0984 - val_loss: 2.3373 - val_acc: 0.0954\n",
            "Epoch 274/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3359 - acc: 0.1001 - val_loss: 2.3200 - val_acc: 0.0959\n",
            "Epoch 275/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3383 - acc: 0.0981 - val_loss: 2.3279 - val_acc: 0.0974\n",
            "Epoch 276/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3356 - acc: 0.1001 - val_loss: 2.3218 - val_acc: 0.0923\n",
            "Epoch 277/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3430 - acc: 0.0978 - val_loss: 2.3333 - val_acc: 0.0974\n",
            "Epoch 278/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3362 - acc: 0.0993 - val_loss: 2.3424 - val_acc: 0.0984\n",
            "Epoch 279/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3401 - acc: 0.0977 - val_loss: 2.3318 - val_acc: 0.0959\n",
            "Epoch 280/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3294 - acc: 0.0985 - val_loss: 2.3222 - val_acc: 0.0963\n",
            "Epoch 281/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3277 - acc: 0.0972 - val_loss: 2.3192 - val_acc: 0.0982\n",
            "Epoch 282/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3300 - acc: 0.0988 - val_loss: 2.3383 - val_acc: 0.0961\n",
            "Epoch 283/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3399 - acc: 0.0990 - val_loss: 2.3358 - val_acc: 0.0961\n",
            "Epoch 284/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3516 - acc: 0.0991 - val_loss: 2.3602 - val_acc: 0.0972\n",
            "Epoch 285/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3471 - acc: 0.0981 - val_loss: 2.3262 - val_acc: 0.0966\n",
            "Epoch 286/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3611 - acc: 0.1000 - val_loss: 2.3426 - val_acc: 0.1013\n",
            "Epoch 287/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3492 - acc: 0.1001 - val_loss: 2.3303 - val_acc: 0.1006\n",
            "Epoch 288/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3421 - acc: 0.0974 - val_loss: 2.3251 - val_acc: 0.1028\n",
            "Epoch 289/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3467 - acc: 0.0997 - val_loss: 2.3514 - val_acc: 0.0991\n",
            "Epoch 290/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3637 - acc: 0.0993 - val_loss: 2.3597 - val_acc: 0.0981\n",
            "Epoch 291/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3573 - acc: 0.0982 - val_loss: 2.3639 - val_acc: 0.1017\n",
            "Epoch 292/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3651 - acc: 0.0966 - val_loss: 2.3499 - val_acc: 0.0992\n",
            "Epoch 293/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3516 - acc: 0.0996 - val_loss: 2.3398 - val_acc: 0.0983\n",
            "Epoch 294/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3565 - acc: 0.0997 - val_loss: 2.3367 - val_acc: 0.0892\n",
            "Epoch 295/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3471 - acc: 0.0965 - val_loss: 2.3213 - val_acc: 0.0997\n",
            "Epoch 296/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3451 - acc: 0.1004 - val_loss: 2.3528 - val_acc: 0.0955\n",
            "Epoch 297/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3478 - acc: 0.1010 - val_loss: 2.3328 - val_acc: 0.0952\n",
            "Epoch 298/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3455 - acc: 0.1008 - val_loss: 2.3393 - val_acc: 0.0938\n",
            "Epoch 299/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3405 - acc: 0.1010 - val_loss: 2.3304 - val_acc: 0.0927\n",
            "Epoch 300/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3371 - acc: 0.0973 - val_loss: 2.3244 - val_acc: 0.0935\n",
            "Epoch 301/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3360 - acc: 0.1000 - val_loss: 2.3235 - val_acc: 0.0954\n",
            "Epoch 302/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3347 - acc: 0.0978 - val_loss: 2.3346 - val_acc: 0.0929\n",
            "Epoch 303/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3496 - acc: 0.0968 - val_loss: 2.3211 - val_acc: 0.0924\n",
            "Epoch 304/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3380 - acc: 0.0981 - val_loss: 2.3233 - val_acc: 0.0949\n",
            "Epoch 305/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3378 - acc: 0.0985 - val_loss: 2.3237 - val_acc: 0.0950\n",
            "Epoch 306/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3298 - acc: 0.0977 - val_loss: 2.3173 - val_acc: 0.0928\n",
            "Epoch 307/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3274 - acc: 0.0975 - val_loss: 2.3273 - val_acc: 0.0942\n",
            "Epoch 308/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3497 - acc: 0.0971 - val_loss: 2.3392 - val_acc: 0.0957\n",
            "Epoch 309/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3412 - acc: 0.1004 - val_loss: 2.3415 - val_acc: 0.0976\n",
            "Epoch 310/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3462 - acc: 0.1011 - val_loss: 2.3461 - val_acc: 0.0962\n",
            "Epoch 311/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3402 - acc: 0.1017 - val_loss: 2.3321 - val_acc: 0.0952\n",
            "Epoch 312/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3336 - acc: 0.1001 - val_loss: 2.3314 - val_acc: 0.0926\n",
            "Epoch 313/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3364 - acc: 0.0997 - val_loss: 2.3267 - val_acc: 0.0940\n",
            "Epoch 314/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3287 - acc: 0.0998 - val_loss: 2.3142 - val_acc: 0.0946\n",
            "Epoch 315/500\n",
            "42000/42000 [==============================] - 1s 19us/step - loss: 2.3373 - acc: 0.1012 - val_loss: 2.3321 - val_acc: 0.0950\n",
            "Epoch 316/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3420 - acc: 0.0999 - val_loss: 2.3336 - val_acc: 0.0950\n",
            "Epoch 317/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3424 - acc: 0.1004 - val_loss: 2.3270 - val_acc: 0.0959\n",
            "Epoch 318/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3378 - acc: 0.1014 - val_loss: 2.3215 - val_acc: 0.0943\n",
            "Epoch 319/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3394 - acc: 0.1018 - val_loss: 2.3500 - val_acc: 0.0934\n",
            "Epoch 320/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3471 - acc: 0.0986 - val_loss: 2.3613 - val_acc: 0.0998\n",
            "Epoch 321/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3485 - acc: 0.0996 - val_loss: 2.3405 - val_acc: 0.0983\n",
            "Epoch 322/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3410 - acc: 0.1002 - val_loss: 2.3277 - val_acc: 0.0943\n",
            "Epoch 323/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3395 - acc: 0.0978 - val_loss: 2.3302 - val_acc: 0.0932\n",
            "Epoch 324/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3393 - acc: 0.0992 - val_loss: 2.3212 - val_acc: 0.0963\n",
            "Epoch 325/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3277 - acc: 0.0989 - val_loss: 2.3248 - val_acc: 0.0922\n",
            "Epoch 326/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3320 - acc: 0.0998 - val_loss: 2.3169 - val_acc: 0.0937\n",
            "Epoch 327/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3310 - acc: 0.1005 - val_loss: 2.3276 - val_acc: 0.0947\n",
            "Epoch 328/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3313 - acc: 0.0997 - val_loss: 2.3251 - val_acc: 0.1001\n",
            "Epoch 329/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3381 - acc: 0.0985 - val_loss: 2.3305 - val_acc: 0.0970\n",
            "Epoch 330/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3368 - acc: 0.0987 - val_loss: 2.3399 - val_acc: 0.0970\n",
            "Epoch 331/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3323 - acc: 0.1000 - val_loss: 2.3159 - val_acc: 0.0932\n",
            "Epoch 332/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3400 - acc: 0.1015 - val_loss: 2.3338 - val_acc: 0.0991\n",
            "Epoch 333/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3465 - acc: 0.1009 - val_loss: 2.3336 - val_acc: 0.0984\n",
            "Epoch 334/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3350 - acc: 0.1008 - val_loss: 2.3154 - val_acc: 0.0961\n",
            "Epoch 335/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3311 - acc: 0.1006 - val_loss: 2.3189 - val_acc: 0.0982\n",
            "Epoch 336/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3267 - acc: 0.1006 - val_loss: 2.3168 - val_acc: 0.0949\n",
            "Epoch 337/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3349 - acc: 0.1002 - val_loss: 2.3389 - val_acc: 0.0939\n",
            "Epoch 338/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3416 - acc: 0.0998 - val_loss: 2.3375 - val_acc: 0.0945\n",
            "Epoch 339/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3313 - acc: 0.1002 - val_loss: 2.3216 - val_acc: 0.0992\n",
            "Epoch 340/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3300 - acc: 0.0997 - val_loss: 2.3185 - val_acc: 0.1008\n",
            "Epoch 341/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3267 - acc: 0.0990 - val_loss: 2.3240 - val_acc: 0.1012\n",
            "Epoch 342/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3238 - acc: 0.1003 - val_loss: 2.3285 - val_acc: 0.0917\n",
            "Epoch 343/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3312 - acc: 0.0996 - val_loss: 2.3287 - val_acc: 0.0963\n",
            "Epoch 344/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3400 - acc: 0.1001 - val_loss: 2.3259 - val_acc: 0.0946\n",
            "Epoch 345/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3277 - acc: 0.0997 - val_loss: 2.3204 - val_acc: 0.0951\n",
            "Epoch 346/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3232 - acc: 0.0984 - val_loss: 2.3256 - val_acc: 0.0946\n",
            "Epoch 347/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3257 - acc: 0.1016 - val_loss: 2.3128 - val_acc: 0.0991\n",
            "Epoch 348/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3222 - acc: 0.0971 - val_loss: 2.3134 - val_acc: 0.0999\n",
            "Epoch 349/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3297 - acc: 0.0985 - val_loss: 2.3235 - val_acc: 0.0990\n",
            "Epoch 350/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3302 - acc: 0.0988 - val_loss: 2.3242 - val_acc: 0.0983\n",
            "Epoch 351/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3340 - acc: 0.0995 - val_loss: 2.3400 - val_acc: 0.0973\n",
            "Epoch 352/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3289 - acc: 0.0993 - val_loss: 2.3125 - val_acc: 0.0897\n",
            "Epoch 353/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3400 - acc: 0.0992 - val_loss: 2.3233 - val_acc: 0.1007\n",
            "Epoch 354/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3369 - acc: 0.0997 - val_loss: 2.3390 - val_acc: 0.1014\n",
            "Epoch 355/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3466 - acc: 0.1005 - val_loss: 2.3405 - val_acc: 0.0988\n",
            "Epoch 356/500\n",
            "42000/42000 [==============================] - 1s 19us/step - loss: 2.3346 - acc: 0.0980 - val_loss: 2.3349 - val_acc: 0.0969\n",
            "Epoch 357/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3415 - acc: 0.1003 - val_loss: 2.3404 - val_acc: 0.0969\n",
            "Epoch 358/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3381 - acc: 0.0991 - val_loss: 2.3283 - val_acc: 0.0999\n",
            "Epoch 359/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3278 - acc: 0.1009 - val_loss: 2.3195 - val_acc: 0.0967\n",
            "Epoch 360/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3307 - acc: 0.0994 - val_loss: 2.3231 - val_acc: 0.0999\n",
            "Epoch 361/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3262 - acc: 0.0985 - val_loss: 2.3187 - val_acc: 0.0994\n",
            "Epoch 362/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3227 - acc: 0.0967 - val_loss: 2.3167 - val_acc: 0.0956\n",
            "Epoch 363/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3352 - acc: 0.0998 - val_loss: 2.3325 - val_acc: 0.0971\n",
            "Epoch 364/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3309 - acc: 0.1000 - val_loss: 2.3226 - val_acc: 0.0970\n",
            "Epoch 365/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3388 - acc: 0.1005 - val_loss: 2.3287 - val_acc: 0.0976\n",
            "Epoch 366/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3321 - acc: 0.0980 - val_loss: 2.3165 - val_acc: 0.0931\n",
            "Epoch 367/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3302 - acc: 0.0969 - val_loss: 2.3173 - val_acc: 0.0919\n",
            "Epoch 368/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3281 - acc: 0.0969 - val_loss: 2.3440 - val_acc: 0.0961\n",
            "Epoch 369/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3511 - acc: 0.0993 - val_loss: 2.3449 - val_acc: 0.0932\n",
            "Epoch 370/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3473 - acc: 0.0978 - val_loss: 2.3368 - val_acc: 0.0968\n",
            "Epoch 371/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3553 - acc: 0.1010 - val_loss: 2.3350 - val_acc: 0.1014\n",
            "Epoch 372/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3594 - acc: 0.1019 - val_loss: 2.3470 - val_acc: 0.0997\n",
            "Epoch 373/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3559 - acc: 0.1008 - val_loss: 2.3430 - val_acc: 0.1009\n",
            "Epoch 374/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3572 - acc: 0.1019 - val_loss: 2.3453 - val_acc: 0.0997\n",
            "Epoch 375/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3473 - acc: 0.0969 - val_loss: 2.3463 - val_acc: 0.0935\n",
            "Epoch 376/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3507 - acc: 0.0986 - val_loss: 2.3504 - val_acc: 0.0930\n",
            "Epoch 377/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3460 - acc: 0.1013 - val_loss: 2.3293 - val_acc: 0.0947\n",
            "Epoch 378/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3582 - acc: 0.1006 - val_loss: 2.3535 - val_acc: 0.1001\n",
            "Epoch 379/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3633 - acc: 0.1014 - val_loss: 2.3384 - val_acc: 0.1029\n",
            "Epoch 380/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3542 - acc: 0.0986 - val_loss: 2.3367 - val_acc: 0.1023\n",
            "Epoch 381/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3547 - acc: 0.1028 - val_loss: 2.3451 - val_acc: 0.1063\n",
            "Epoch 382/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3633 - acc: 0.1007 - val_loss: 2.3468 - val_acc: 0.0964\n",
            "Epoch 383/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.4087 - acc: 0.1023 - val_loss: 2.4171 - val_acc: 0.1044\n",
            "Epoch 384/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.4094 - acc: 0.1006 - val_loss: 2.3619 - val_acc: 0.1137\n",
            "Epoch 385/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3748 - acc: 0.1000 - val_loss: 2.3645 - val_acc: 0.1118\n",
            "Epoch 386/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3833 - acc: 0.1025 - val_loss: 2.3681 - val_acc: 0.1038\n",
            "Epoch 387/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3597 - acc: 0.0985 - val_loss: 2.3310 - val_acc: 0.1095\n",
            "Epoch 388/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3597 - acc: 0.1014 - val_loss: 2.3506 - val_acc: 0.1017\n",
            "Epoch 389/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3572 - acc: 0.1035 - val_loss: 2.3292 - val_acc: 0.0972\n",
            "Epoch 390/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3484 - acc: 0.1005 - val_loss: 2.3052 - val_acc: 0.1027\n",
            "Epoch 391/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3603 - acc: 0.1002 - val_loss: 2.3146 - val_acc: 0.1033\n",
            "Epoch 392/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3500 - acc: 0.1003 - val_loss: 2.3199 - val_acc: 0.1060\n",
            "Epoch 393/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3364 - acc: 0.1015 - val_loss: 2.3260 - val_acc: 0.1055\n",
            "Epoch 394/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3166 - acc: 0.0997 - val_loss: 2.3145 - val_acc: 0.1059\n",
            "Epoch 395/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3117 - acc: 0.1003 - val_loss: 2.3075 - val_acc: 0.1059\n",
            "Epoch 396/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3077 - acc: 0.1009 - val_loss: 2.3099 - val_acc: 0.1056\n",
            "Epoch 397/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3085 - acc: 0.0998 - val_loss: 2.3112 - val_acc: 0.1054\n",
            "Epoch 398/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3074 - acc: 0.0985 - val_loss: 2.3076 - val_acc: 0.1048\n",
            "Epoch 399/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3072 - acc: 0.1008 - val_loss: 2.3061 - val_acc: 0.1049\n",
            "Epoch 400/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3081 - acc: 0.1004 - val_loss: 2.3084 - val_acc: 0.1045\n",
            "Epoch 401/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3074 - acc: 0.0991 - val_loss: 2.3058 - val_acc: 0.0989\n",
            "Epoch 402/500\n",
            "42000/42000 [==============================] - 1s 19us/step - loss: 2.3054 - acc: 0.0985 - val_loss: 2.3042 - val_acc: 0.0989\n",
            "Epoch 403/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3063 - acc: 0.0978 - val_loss: 2.3041 - val_acc: 0.0995\n",
            "Epoch 404/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3067 - acc: 0.0984 - val_loss: 2.3042 - val_acc: 0.0998\n",
            "Epoch 405/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3045 - acc: 0.0971 - val_loss: 2.3039 - val_acc: 0.0986\n",
            "Epoch 406/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3066 - acc: 0.0985 - val_loss: 2.3040 - val_acc: 0.1004\n",
            "Epoch 407/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3060 - acc: 0.0960 - val_loss: 2.3033 - val_acc: 0.0987\n",
            "Epoch 408/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3060 - acc: 0.1009 - val_loss: 2.3033 - val_acc: 0.1003\n",
            "Epoch 409/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3054 - acc: 0.0975 - val_loss: 2.3040 - val_acc: 0.0999\n",
            "Epoch 410/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3048 - acc: 0.0985 - val_loss: 2.3032 - val_acc: 0.1031\n",
            "Epoch 411/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3046 - acc: 0.1003 - val_loss: 2.3033 - val_acc: 0.1013\n",
            "Epoch 412/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3051 - acc: 0.0958 - val_loss: 2.3026 - val_acc: 0.1037\n",
            "Epoch 413/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3042 - acc: 0.0986 - val_loss: 2.3032 - val_acc: 0.1037\n",
            "Epoch 414/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3037 - acc: 0.0974 - val_loss: 2.3023 - val_acc: 0.1039\n",
            "Epoch 415/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3061 - acc: 0.0983 - val_loss: 2.3031 - val_acc: 0.1112\n",
            "Epoch 416/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3070 - acc: 0.0966 - val_loss: 2.3025 - val_acc: 0.1044\n",
            "Epoch 417/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3054 - acc: 0.0975 - val_loss: 2.3031 - val_acc: 0.1007\n",
            "Epoch 418/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3040 - acc: 0.0963 - val_loss: 2.3061 - val_acc: 0.1049\n",
            "Epoch 419/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3038 - acc: 0.0994 - val_loss: 2.3036 - val_acc: 0.1060\n",
            "Epoch 420/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3052 - acc: 0.0985 - val_loss: 2.3042 - val_acc: 0.1066\n",
            "Epoch 421/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3032 - acc: 0.0961 - val_loss: 2.3031 - val_acc: 0.1136\n",
            "Epoch 422/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.0972 - val_loss: 2.3026 - val_acc: 0.1119\n",
            "Epoch 423/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3032 - acc: 0.0994 - val_loss: 2.3033 - val_acc: 0.1073\n",
            "Epoch 424/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3030 - acc: 0.1004 - val_loss: 2.3026 - val_acc: 0.1062\n",
            "Epoch 425/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3027 - acc: 0.0973 - val_loss: 2.3026 - val_acc: 0.1082\n",
            "Epoch 426/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3025 - acc: 0.0970 - val_loss: 2.3025 - val_acc: 0.1077\n",
            "Epoch 427/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.0986 - val_loss: 2.3024 - val_acc: 0.1077\n",
            "Epoch 428/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3030 - acc: 0.0994 - val_loss: 2.3024 - val_acc: 0.1071\n",
            "Epoch 429/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.0976 - val_loss: 2.3024 - val_acc: 0.1046\n",
            "Epoch 430/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3025 - acc: 0.0974 - val_loss: 2.3024 - val_acc: 0.1011\n",
            "Epoch 431/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0985 - val_loss: 2.3023 - val_acc: 0.0966\n",
            "Epoch 432/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.0982 - val_loss: 2.3023 - val_acc: 0.1057\n",
            "Epoch 433/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3033 - acc: 0.0958 - val_loss: 2.3023 - val_acc: 0.1031\n",
            "Epoch 434/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.0945 - val_loss: 2.3023 - val_acc: 0.0987\n",
            "Epoch 435/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3023 - acc: 0.0978 - val_loss: 2.3022 - val_acc: 0.1002\n",
            "Epoch 436/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.0949 - val_loss: 2.3021 - val_acc: 0.1026\n",
            "Epoch 437/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3025 - acc: 0.0958 - val_loss: 2.3030 - val_acc: 0.1064\n",
            "Epoch 438/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3030 - acc: 0.0991 - val_loss: 2.3030 - val_acc: 0.1038\n",
            "Epoch 439/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.0967 - val_loss: 2.3029 - val_acc: 0.1002\n",
            "Epoch 440/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.0939 - val_loss: 2.3029 - val_acc: 0.1045\n",
            "Epoch 441/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3026 - acc: 0.0970 - val_loss: 2.3022 - val_acc: 0.1029\n",
            "Epoch 442/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3024 - acc: 0.0961 - val_loss: 2.3021 - val_acc: 0.1039\n",
            "Epoch 443/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3023 - acc: 0.0970 - val_loss: 2.3021 - val_acc: 0.1067\n",
            "Epoch 444/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3027 - acc: 0.0956 - val_loss: 2.3021 - val_acc: 0.1067\n",
            "Epoch 445/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3024 - acc: 0.0980 - val_loss: 2.3020 - val_acc: 0.1075\n",
            "Epoch 446/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3023 - acc: 0.0972 - val_loss: 2.3020 - val_acc: 0.1063\n",
            "Epoch 447/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.0966 - val_loss: 2.3019 - val_acc: 0.1105\n",
            "Epoch 448/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3028 - acc: 0.0966 - val_loss: 2.3035 - val_acc: 0.1071\n",
            "Epoch 449/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3044 - acc: 0.0939 - val_loss: 2.3041 - val_acc: 0.1023\n",
            "Epoch 450/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.0946 - val_loss: 2.3035 - val_acc: 0.1041\n",
            "Epoch 451/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3038 - acc: 0.0969 - val_loss: 2.3023 - val_acc: 0.0993\n",
            "Epoch 452/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3024 - acc: 0.0935 - val_loss: 2.3017 - val_acc: 0.1006\n",
            "Epoch 453/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3020 - acc: 0.0951 - val_loss: 2.3017 - val_acc: 0.1001\n",
            "Epoch 454/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3024 - acc: 0.0954 - val_loss: 2.3029 - val_acc: 0.1001\n",
            "Epoch 455/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3042 - acc: 0.0962 - val_loss: 2.3027 - val_acc: 0.0987\n",
            "Epoch 456/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3040 - acc: 0.0932 - val_loss: 2.3034 - val_acc: 0.0938\n",
            "Epoch 457/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3044 - acc: 0.0943 - val_loss: 2.3033 - val_acc: 0.0959\n",
            "Epoch 458/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3043 - acc: 0.0947 - val_loss: 2.3033 - val_acc: 0.0931\n",
            "Epoch 459/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3068 - acc: 0.0946 - val_loss: 2.3032 - val_acc: 0.0882\n",
            "Epoch 460/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3040 - acc: 0.0947 - val_loss: 2.3019 - val_acc: 0.0885\n",
            "Epoch 461/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3043 - acc: 0.0937 - val_loss: 2.3028 - val_acc: 0.0900\n",
            "Epoch 462/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3081 - acc: 0.0928 - val_loss: 2.3017 - val_acc: 0.0832\n",
            "Epoch 463/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3020 - acc: 0.0925 - val_loss: 2.3015 - val_acc: 0.0914\n",
            "Epoch 464/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3023 - acc: 0.0949 - val_loss: 2.3014 - val_acc: 0.0943\n",
            "Epoch 465/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3021 - acc: 0.0934 - val_loss: 2.3015 - val_acc: 0.0912\n",
            "Epoch 466/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3021 - acc: 0.0947 - val_loss: 2.3019 - val_acc: 0.0916\n",
            "Epoch 467/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3023 - acc: 0.0950 - val_loss: 2.3026 - val_acc: 0.0894\n",
            "Epoch 468/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3041 - acc: 0.0940 - val_loss: 2.3015 - val_acc: 0.0907\n",
            "Epoch 469/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3035 - acc: 0.0955 - val_loss: 2.3031 - val_acc: 0.0956\n",
            "Epoch 470/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3037 - acc: 0.0938 - val_loss: 2.3030 - val_acc: 0.0946\n",
            "Epoch 471/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3037 - acc: 0.0955 - val_loss: 2.3010 - val_acc: 0.0866\n",
            "Epoch 472/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3046 - acc: 0.0919 - val_loss: 2.3026 - val_acc: 0.0905\n",
            "Epoch 473/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3077 - acc: 0.0942 - val_loss: 2.3018 - val_acc: 0.0846\n",
            "Epoch 474/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3176 - acc: 0.0894 - val_loss: 2.3153 - val_acc: 0.0826\n",
            "Epoch 475/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3319 - acc: 0.0889 - val_loss: 2.3335 - val_acc: 0.0856\n",
            "Epoch 476/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3708 - acc: 0.0888 - val_loss: 2.3065 - val_acc: 0.0989\n",
            "Epoch 477/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4130 - acc: 0.0954 - val_loss: 2.3564 - val_acc: 0.0912\n",
            "Epoch 478/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.4001 - acc: 0.0923 - val_loss: 2.3624 - val_acc: 0.0988\n",
            "Epoch 479/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.4024 - acc: 0.0981 - val_loss: 2.3648 - val_acc: 0.0957\n",
            "Epoch 480/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3816 - acc: 0.1012 - val_loss: 2.3658 - val_acc: 0.1014\n",
            "Epoch 481/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3501 - acc: 0.1016 - val_loss: 2.3629 - val_acc: 0.0935\n",
            "Epoch 482/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3442 - acc: 0.0948 - val_loss: 2.3407 - val_acc: 0.0921\n",
            "Epoch 483/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3470 - acc: 0.0989 - val_loss: 2.3273 - val_acc: 0.0894\n",
            "Epoch 484/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3400 - acc: 0.0974 - val_loss: 2.3303 - val_acc: 0.0901\n",
            "Epoch 485/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3376 - acc: 0.0995 - val_loss: 2.3312 - val_acc: 0.0977\n",
            "Epoch 486/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3377 - acc: 0.1021 - val_loss: 2.3233 - val_acc: 0.0966\n",
            "Epoch 487/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3351 - acc: 0.0989 - val_loss: 2.3294 - val_acc: 0.0933\n",
            "Epoch 488/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3335 - acc: 0.0977 - val_loss: 2.3230 - val_acc: 0.0996\n",
            "Epoch 489/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3345 - acc: 0.1023 - val_loss: 2.3261 - val_acc: 0.0988\n",
            "Epoch 490/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3418 - acc: 0.1034 - val_loss: 2.3402 - val_acc: 0.0958\n",
            "Epoch 491/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3469 - acc: 0.1009 - val_loss: 2.3216 - val_acc: 0.0901\n",
            "Epoch 492/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3398 - acc: 0.1003 - val_loss: 2.3300 - val_acc: 0.0994\n",
            "Epoch 493/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3379 - acc: 0.1010 - val_loss: 2.3303 - val_acc: 0.1036\n",
            "Epoch 494/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3386 - acc: 0.1015 - val_loss: 2.3253 - val_acc: 0.1009\n",
            "Epoch 495/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3340 - acc: 0.1003 - val_loss: 2.3345 - val_acc: 0.1017\n",
            "Epoch 496/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3378 - acc: 0.1006 - val_loss: 2.3253 - val_acc: 0.1064\n",
            "Epoch 497/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.3320 - acc: 0.0990 - val_loss: 2.3260 - val_acc: 0.0987\n",
            "Epoch 498/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3288 - acc: 0.0980 - val_loss: 2.3268 - val_acc: 0.1019\n",
            "Epoch 499/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 2.3323 - acc: 0.0989 - val_loss: 2.3202 - val_acc: 0.0914\n",
            "Epoch 500/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 2.3266 - acc: 0.0993 - val_loss: 2.3167 - val_acc: 0.0952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f107ee43f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfkBzXyYXnTx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58da979d-dbfa-4258-d0e8-afc1d7a77cfc"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(1024, input_shape = (1024,), activation = 'tanh'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1024, activation = 'tanh'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(10, activation = 'tanh'))\n",
        "sgd = optimizers.Adam(lr = 0.3)\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()\n",
        "model2.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,113,546\n",
            "Trainable params: 2,111,498\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 1s 32us/step - loss: 0.3784 - acc: 0.0993 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 1.1921e-07 - acc: 0.0997 - val_loss: 1.1921e-07 - val_acc: 0.1008\n",
            "Epoch 20/500\n",
            "19500/42000 [============>.................] - ETA: 0s - loss: 1.1921e-07 - acc: 0.0997"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-41be41b229ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C-seJhIX-sa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba5d6184-8ed0-44d9-bcc4-340cc9dbf2be"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(1024, input_shape = (1024,), activation = 'sigmoid'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(1024, activation = 'sigmoid'))\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(10, activation = 'sigmoid'))\n",
        "sgd = optimizers.Adam(lr = 0.3)\n",
        "model3.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.summary()\n",
        "model3.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,113,546\n",
            "Trainable params: 2,111,498\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 1s 35us/step - loss: 7.4822 - acc: 0.0998 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5452 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 7.5434 - acc: 0.0997 - val_loss: 7.4729 - val_acc: 0.1008\n",
            "Epoch 18/500\n",
            "30000/42000 [====================>.........] - ETA: 0s - loss: 7.5108 - acc: 0.0999"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c316ac6b8326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 192\u001b[0;31m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0m\u001b[1;32m    193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuFUexTeYK7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d994e400-34c5-4f8b-cc40-f7e78c8ed25d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model4 = Sequential()\n",
        "model4.add(Dense(1024, input_shape = (1024,), activation = 'sigmoid'))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(Dense(1024, activation = 'sigmoid'))\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(Dense(10, activation = 'sigmoid'))\n",
        "sgd = optimizers.Adam(lr = 0.3)\n",
        "model4.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model4.summary()\n",
        "model4.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,113,546\n",
            "Trainable params: 2,111,498\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 2s 38us/step - loss: 6.1449 - acc: 0.0999 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 18us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 6.1814 - acc: 0.0997 - val_loss: 6.2354 - val_acc: 0.1008\n",
            "Epoch 14/500\n",
            "  500/42000 [..............................] - ETA: 1s - loss: 6.3393 - acc: 0.1180"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a11d75f04f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 192\u001b[0;31m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0m\u001b[1;32m    193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA2khPuxYV8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7646b21-3872-40b1-b269-7f24f8fc1a49"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "from keras.layers import LeakyReLU\n",
        "model5 = Sequential()\n",
        "model5.add(Dense(1024, input_shape = (1024,)))\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(LeakyReLU(alpha=0.01))\n",
        "model5.add(Dense(10))\n",
        "sgd = optimizers.Adam(lr = 0.01)\n",
        "model5.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model5.summary()\n",
        "model5.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 500, epochs = 500, verbose = 1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 1,063,946\n",
            "Trainable params: 1,061,898\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 2s 41us/step - loss: 6.9022 - acc: 0.1012 - val_loss: 6.1623 - val_acc: 0.0981\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 6.9902 - acc: 0.1007 - val_loss: 4.9663 - val_acc: 0.0953\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 6.0180 - acc: 0.1028 - val_loss: 5.6328 - val_acc: 0.0953\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 6.3154 - acc: 0.1037 - val_loss: 10.3422 - val_acc: 0.1067\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 6.9678 - acc: 0.1037 - val_loss: 5.2277 - val_acc: 0.1043\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 6.3511 - acc: 0.0995 - val_loss: 6.2676 - val_acc: 0.1007\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.5997 - acc: 0.0995 - val_loss: 8.7742 - val_acc: 0.1007\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 9.1281 - acc: 0.1032 - val_loss: 7.8447 - val_acc: 0.0989\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.9534 - acc: 0.1022 - val_loss: 7.9803 - val_acc: 0.1018\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.9853 - acc: 0.1024 - val_loss: 7.9543 - val_acc: 0.1027\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 7.9842 - acc: 0.1024 - val_loss: 7.8663 - val_acc: 0.1024\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.8218 - acc: 0.1020 - val_loss: 8.0574 - val_acc: 0.1029\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 7.6781 - acc: 0.1032 - val_loss: 8.0423 - val_acc: 0.1029\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 7.8528 - acc: 0.1029 - val_loss: 8.3575 - val_acc: 0.1031\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.2018 - acc: 0.1049 - val_loss: 8.0501 - val_acc: 0.0993\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.0387 - acc: 0.1054 - val_loss: 8.0635 - val_acc: 0.1011\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 8.0433 - acc: 0.1052 - val_loss: 7.9283 - val_acc: 0.1036\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.0564 - acc: 0.1058 - val_loss: 7.9856 - val_acc: 0.1059\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.0464 - acc: 0.1058 - val_loss: 7.9883 - val_acc: 0.1067\n",
            "Epoch 20/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 8.0602 - acc: 0.1056 - val_loss: 8.0179 - val_acc: 0.1069\n",
            "Epoch 21/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 8.0429 - acc: 0.1057 - val_loss: 8.0008 - val_acc: 0.1071\n",
            "Epoch 22/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.0472 - acc: 0.1059 - val_loss: 8.0053 - val_acc: 0.1071\n",
            "Epoch 23/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.0541 - acc: 0.1055 - val_loss: 8.0080 - val_acc: 0.1071\n",
            "Epoch 24/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.0537 - acc: 0.1051 - val_loss: 8.0214 - val_acc: 0.1072\n",
            "Epoch 25/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 8.0468 - acc: 0.1053 - val_loss: 8.0232 - val_acc: 0.1072\n",
            "Epoch 26/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 8.0690 - acc: 0.1060 - val_loss: 8.0223 - val_acc: 0.1072\n",
            "Epoch 27/500\n",
            "12500/42000 [=======>......................] - ETA: 0s - loss: 8.1300 - acc: 0.1062"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9a4252547797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wHh7QDubvy7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f64961d-7bcf-4024-db9a-f135f5311136"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model_final = Sequential()\n",
        "model_final.add(Dense(units = 1024, input_shape = (1024,), activation = 'relu'))\n",
        "model_final.add(BatchNormalization())\n",
        "model_final.add(Dropout(0.5))\n",
        "model_final.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final.add(Dense(units = 10, activation = 'relu'))\n",
        "sgd = optimizers.SGD(lr = 0.1, momentum=0.9, nesterov=True)\n",
        "model_final.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model_final.summary()\n",
        "model_final.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 1000, epochs = 100, verbose = 1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_35 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,113,546\n",
            "Trainable params: 2,111,498\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 44us/step - loss: 2.5903 - acc: 0.0992 - val_loss: 2.3446 - val_acc: 0.1016\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.3308 - acc: 0.0996 - val_loss: 2.3024 - val_acc: 0.1040\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2967 - acc: 0.1117 - val_loss: 2.2699 - val_acc: 0.1577\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2722 - acc: 0.1419 - val_loss: 2.2451 - val_acc: 0.1750\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2488 - acc: 0.1687 - val_loss: 2.2226 - val_acc: 0.1966\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2262 - acc: 0.1913 - val_loss: 2.1995 - val_acc: 0.2278\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2011 - acc: 0.2122 - val_loss: 2.1828 - val_acc: 0.2177\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1771 - acc: 0.2334 - val_loss: 2.1953 - val_acc: 0.1991\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1576 - acc: 0.2440 - val_loss: 2.1435 - val_acc: 0.2704\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1383 - acc: 0.2506 - val_loss: 2.3960 - val_acc: 0.1436\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1512 - acc: 0.2381 - val_loss: 2.3852 - val_acc: 0.1396\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1396 - acc: 0.2447 - val_loss: 2.3125 - val_acc: 0.1697\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1201 - acc: 0.2528 - val_loss: 2.3820 - val_acc: 0.1119\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1084 - acc: 0.2454 - val_loss: 2.2702 - val_acc: 0.1562\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2205 - acc: 0.1644 - val_loss: 2.3501 - val_acc: 0.1208\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2205 - acc: 0.1590 - val_loss: 2.3386 - val_acc: 0.1577\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1504 - acc: 0.2204 - val_loss: 2.2718 - val_acc: 0.1689\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.3422 - acc: 0.1429 - val_loss: 2.3364 - val_acc: 0.1369\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2970 - acc: 0.1513 - val_loss: 2.2825 - val_acc: 0.1610\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2553 - acc: 0.1560 - val_loss: 2.3545 - val_acc: 0.1121\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2517 - acc: 0.1494 - val_loss: 2.3666 - val_acc: 0.1046\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2213 - acc: 0.1663 - val_loss: 2.2650 - val_acc: 0.1306\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2107 - acc: 0.1681 - val_loss: 2.9875 - val_acc: 0.1192\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1910 - acc: 0.1726 - val_loss: 2.3547 - val_acc: 0.0946\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2900 - acc: 0.1132 - val_loss: 2.2922 - val_acc: 0.1084\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2383 - acc: 0.1270 - val_loss: 2.2507 - val_acc: 0.1234\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1936 - acc: 0.1508 - val_loss: 2.2102 - val_acc: 0.1557\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1645 - acc: 0.1800 - val_loss: 2.2408 - val_acc: 0.1308\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1473 - acc: 0.2072 - val_loss: 2.2205 - val_acc: 0.1192\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1310 - acc: 0.2228 - val_loss: 2.1950 - val_acc: 0.1708\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1184 - acc: 0.2377 - val_loss: 2.2324 - val_acc: 0.1672\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2594 - acc: 0.1636 - val_loss: 2.3253 - val_acc: 0.1303\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2312 - acc: 0.1525 - val_loss: 2.2717 - val_acc: 0.1317\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1822 - acc: 0.1682 - val_loss: 2.2022 - val_acc: 0.1737\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1397 - acc: 0.1920 - val_loss: 2.2344 - val_acc: 0.1426\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1396 - acc: 0.2004 - val_loss: 2.5978 - val_acc: 0.1139\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1057 - acc: 0.2342 - val_loss: 2.0712 - val_acc: 0.2508\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1637 - acc: 0.2110 - val_loss: 2.4546 - val_acc: 0.1080\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.3474 - acc: 0.1087 - val_loss: 2.3313 - val_acc: 0.1116\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2531 - acc: 0.1443 - val_loss: 2.2185 - val_acc: 0.1676\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2112 - acc: 0.1632 - val_loss: 5.1614 - val_acc: 0.1008\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2204 - acc: 0.1562 - val_loss: 2.8391 - val_acc: 0.1049\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2169 - acc: 0.1592 - val_loss: 2.2262 - val_acc: 0.1707\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1837 - acc: 0.1768 - val_loss: 2.1730 - val_acc: 0.1948\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1868 - acc: 0.1742 - val_loss: 2.2608 - val_acc: 0.1307\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2186 - acc: 0.1399 - val_loss: 2.2314 - val_acc: 0.1553\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1748 - acc: 0.1750 - val_loss: 2.1955 - val_acc: 0.1803\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1987 - acc: 0.1685 - val_loss: 2.2584 - val_acc: 0.1587\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2292 - acc: 0.1425 - val_loss: 2.3381 - val_acc: 0.1063\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2210 - acc: 0.1343 - val_loss: 2.2214 - val_acc: 0.1416\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1630 - acc: 0.1885 - val_loss: 5.1372 - val_acc: 0.1149\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1468 - acc: 0.2061 - val_loss: 2.2335 - val_acc: 0.1510\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1203 - acc: 0.2044 - val_loss: 2.2312 - val_acc: 0.1718\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1000 - acc: 0.2300 - val_loss: 2.4826 - val_acc: 0.1861\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.0837 - acc: 0.2391 - val_loss: 2.1507 - val_acc: 0.1870\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1098 - acc: 0.2296 - val_loss: 2.3078 - val_acc: 0.1367\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.0988 - acc: 0.2375 - val_loss: 2.1083 - val_acc: 0.2276\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.0718 - acc: 0.2508 - val_loss: 2.1120 - val_acc: 0.2498\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1257 - acc: 0.2055 - val_loss: 2.2273 - val_acc: 0.1416\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.0873 - acc: 0.2127 - val_loss: 2.0574 - val_acc: 0.2594\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.0563 - acc: 0.2574 - val_loss: 2.8463 - val_acc: 0.1359\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1369 - acc: 0.1920 - val_loss: 2.1578 - val_acc: 0.1891\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.0888 - acc: 0.2357 - val_loss: 2.9738 - val_acc: 0.1053\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.0551 - acc: 0.2499 - val_loss: 2.2540 - val_acc: 0.1807\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.0383 - acc: 0.2668 - val_loss: 2.2325 - val_acc: 0.1877\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.0786 - acc: 0.2572 - val_loss: 2.7142 - val_acc: 0.1005\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.5478 - acc: 0.1003 - val_loss: 2.5293 - val_acc: 0.1005\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.4109 - acc: 0.1065 - val_loss: 2.4100 - val_acc: 0.1051\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.3391 - acc: 0.1184 - val_loss: 2.3486 - val_acc: 0.1181\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.3000 - acc: 0.1325 - val_loss: 2.3075 - val_acc: 0.1278\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2748 - acc: 0.1463 - val_loss: 2.3106 - val_acc: 0.1193\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2692 - acc: 0.1453 - val_loss: 2.2763 - val_acc: 0.1407\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2445 - acc: 0.1630 - val_loss: 2.2384 - val_acc: 0.1752\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2224 - acc: 0.1799 - val_loss: 2.2164 - val_acc: 0.1954\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1991 - acc: 0.1891 - val_loss: 2.1704 - val_acc: 0.2394\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1709 - acc: 0.2053 - val_loss: 2.1283 - val_acc: 0.2671\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1417 - acc: 0.2182 - val_loss: 2.1147 - val_acc: 0.2340\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1163 - acc: 0.2279 - val_loss: 2.0630 - val_acc: 0.3013\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1066 - acc: 0.2389 - val_loss: 2.1126 - val_acc: 0.2914\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1011 - acc: 0.2365 - val_loss: 3.3078 - val_acc: 0.1876\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.4068 - acc: 0.1165 - val_loss: 2.4076 - val_acc: 0.1090\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.3619 - acc: 0.1099 - val_loss: 2.3483 - val_acc: 0.1153\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.3199 - acc: 0.1137 - val_loss: 2.3179 - val_acc: 0.1203\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2954 - acc: 0.1189 - val_loss: 2.2954 - val_acc: 0.1250\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2761 - acc: 0.1232 - val_loss: 2.2751 - val_acc: 0.1332\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2583 - acc: 0.1289 - val_loss: 2.2552 - val_acc: 0.1412\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2421 - acc: 0.1375 - val_loss: 2.2334 - val_acc: 0.1481\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2279 - acc: 0.1416 - val_loss: 2.2102 - val_acc: 0.1561\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2140 - acc: 0.1502 - val_loss: 2.1795 - val_acc: 0.1702\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1998 - acc: 0.1554 - val_loss: 2.1612 - val_acc: 0.1885\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1851 - acc: 0.1671 - val_loss: 2.1443 - val_acc: 0.1983\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1726 - acc: 0.1784 - val_loss: 2.1373 - val_acc: 0.2161\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1587 - acc: 0.1854 - val_loss: 2.1125 - val_acc: 0.2283\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.2036 - acc: 0.1657 - val_loss: 2.3874 - val_acc: 0.1318\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.2146 - acc: 0.1559 - val_loss: 2.2150 - val_acc: 0.1741\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1993 - acc: 0.1633 - val_loss: 2.1650 - val_acc: 0.1828\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1875 - acc: 0.1682 - val_loss: 2.1564 - val_acc: 0.1890\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1750 - acc: 0.1740 - val_loss: 2.1308 - val_acc: 0.1999\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 0s 10us/step - loss: 2.1709 - acc: 0.1795 - val_loss: 2.1965 - val_acc: 0.1904\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 0s 11us/step - loss: 2.1562 - acc: 0.1853 - val_loss: 2.1019 - val_acc: 0.2151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f57bb7cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60J2XNo8cwUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9412e8e6-7e32-4bac-f4df-0b6669a38db0"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 2s 43us/step - loss: 2.4554 - acc: 0.1199 - val_loss: 2.2753 - val_acc: 0.1660\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.2764 - acc: 0.1620 - val_loss: 2.2370 - val_acc: 0.2227\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.2448 - acc: 0.2093 - val_loss: 2.1757 - val_acc: 0.3187\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.1880 - acc: 0.2587 - val_loss: 2.0611 - val_acc: 0.3826\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.1400 - acc: 0.2453 - val_loss: 2.1352 - val_acc: 0.2539\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 2.1502 - acc: 0.2421 - val_loss: 2.2073 - val_acc: 0.1512\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.6359 - acc: 0.1892 - val_loss: 5.1269 - val_acc: 0.1008\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 4.8424 - acc: 0.0997 - val_loss: 4.4992 - val_acc: 0.1008\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 4.3917 - acc: 0.0997 - val_loss: 4.2870 - val_acc: 0.1008\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 4.2311 - acc: 0.0997 - val_loss: 4.1525 - val_acc: 0.1008\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 4.1166 - acc: 0.0997 - val_loss: 4.0529 - val_acc: 0.1008\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 4.0279 - acc: 0.0997 - val_loss: 3.9732 - val_acc: 0.1008\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.9555 - acc: 0.0997 - val_loss: 3.9076 - val_acc: 0.1008\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 1s 15us/step - loss: 3.8954 - acc: 0.0997 - val_loss: 3.8506 - val_acc: 0.1008\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 3.8410 - acc: 0.0997 - val_loss: 3.8015 - val_acc: 0.1008\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.7952 - acc: 0.0997 - val_loss: 3.7571 - val_acc: 0.1008\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.7531 - acc: 0.0997 - val_loss: 3.7177 - val_acc: 0.1008\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.7145 - acc: 0.0997 - val_loss: 3.6810 - val_acc: 0.1008\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.6783 - acc: 0.0997 - val_loss: 3.6478 - val_acc: 0.1008\n",
            "Epoch 20/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.6458 - acc: 0.0997 - val_loss: 3.6168 - val_acc: 0.1008\n",
            "Epoch 21/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.6174 - acc: 0.0997 - val_loss: 3.5881 - val_acc: 0.1008\n",
            "Epoch 22/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.5906 - acc: 0.0997 - val_loss: 3.5612 - val_acc: 0.1008\n",
            "Epoch 23/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.5657 - acc: 0.0997 - val_loss: 3.5362 - val_acc: 0.1008\n",
            "Epoch 24/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.5411 - acc: 0.0997 - val_loss: 3.5126 - val_acc: 0.1008\n",
            "Epoch 25/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.5184 - acc: 0.0997 - val_loss: 3.4910 - val_acc: 0.1008\n",
            "Epoch 26/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.4977 - acc: 0.0997 - val_loss: 3.4690 - val_acc: 0.1008\n",
            "Epoch 27/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.4752 - acc: 0.0997 - val_loss: 3.4492 - val_acc: 0.1008\n",
            "Epoch 28/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.4558 - acc: 0.0997 - val_loss: 3.4294 - val_acc: 0.1008\n",
            "Epoch 29/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.4372 - acc: 0.0997 - val_loss: 3.4112 - val_acc: 0.1008\n",
            "Epoch 30/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 3.4197 - acc: 0.0997 - val_loss: 3.3936 - val_acc: 0.1008\n",
            "Epoch 31/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.4031 - acc: 0.0997 - val_loss: 3.3761 - val_acc: 0.1008\n",
            "Epoch 32/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.3855 - acc: 0.0997 - val_loss: 3.3609 - val_acc: 0.1008\n",
            "Epoch 33/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 3.3694 - acc: 0.0997 - val_loss: 3.3441 - val_acc: 0.1008\n",
            "Epoch 34/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.3534 - acc: 0.0997 - val_loss: 3.3291 - val_acc: 0.1008\n",
            "Epoch 35/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.3425 - acc: 0.0997 - val_loss: 3.3149 - val_acc: 0.1008\n",
            "Epoch 36/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.3248 - acc: 0.0997 - val_loss: 3.2998 - val_acc: 0.1008\n",
            "Epoch 37/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.3118 - acc: 0.0997 - val_loss: 3.2862 - val_acc: 0.1008\n",
            "Epoch 38/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2976 - acc: 0.0997 - val_loss: 3.2730 - val_acc: 0.1008\n",
            "Epoch 39/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2848 - acc: 0.0997 - val_loss: 3.2596 - val_acc: 0.1008\n",
            "Epoch 40/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2718 - acc: 0.0997 - val_loss: 3.2475 - val_acc: 0.1008\n",
            "Epoch 41/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2617 - acc: 0.0997 - val_loss: 3.2348 - val_acc: 0.1008\n",
            "Epoch 42/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2493 - acc: 0.0997 - val_loss: 3.2231 - val_acc: 0.1008\n",
            "Epoch 43/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2380 - acc: 0.0997 - val_loss: 3.2113 - val_acc: 0.1008\n",
            "Epoch 44/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2264 - acc: 0.0997 - val_loss: 3.1998 - val_acc: 0.1008\n",
            "Epoch 45/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.2156 - acc: 0.0997 - val_loss: 3.1889 - val_acc: 0.1008\n",
            "Epoch 46/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 3.2049 - acc: 0.0997 - val_loss: 3.1771 - val_acc: 0.1008\n",
            "Epoch 47/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 3.1935 - acc: 0.0997 - val_loss: 3.1661 - val_acc: 0.1008\n",
            "Epoch 48/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.1825 - acc: 0.0997 - val_loss: 3.1554 - val_acc: 0.1008\n",
            "Epoch 49/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.1741 - acc: 0.0997 - val_loss: 3.1458 - val_acc: 0.1008\n",
            "Epoch 50/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.1635 - acc: 0.0997 - val_loss: 3.1351 - val_acc: 0.1008\n",
            "Epoch 51/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.1532 - acc: 0.0997 - val_loss: 3.1263 - val_acc: 0.1008\n",
            "Epoch 52/500\n",
            "42000/42000 [==============================] - 1s 17us/step - loss: 3.1450 - acc: 0.0997 - val_loss: 3.1168 - val_acc: 0.1008\n",
            "Epoch 53/500\n",
            "42000/42000 [==============================] - 1s 16us/step - loss: 3.1361 - acc: 0.0997 - val_loss: 3.1065 - val_acc: 0.1008\n",
            "Epoch 54/500\n",
            "15000/42000 [=========>....................] - ETA: 0s - loss: 3.1372 - acc: 0.0981"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-f6c76739b3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_kacYrMAKxh",
        "colab_type": "text"
      },
      "source": [
        "# Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions) (2.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IpfZhnAXwc",
        "colab_type": "text"
      },
      "source": [
        "# Implement batch normalization for training the neural network (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifJLIfwd-zp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c054c6e3-1baf-47da-8624-402c813a752f"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "model_final1 = Sequential()\n",
        "model_final1.add(Dense(units = 1024, input_shape = (1024,), activation = 'relu'))\n",
        "model_final1.add(BatchNormalization())\n",
        "model_final1.add(Dropout(0.5))\n",
        "model_final1.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final1.add(Dropout(0.4))\n",
        "model_final1.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final1.add(Dropout(0.3))\n",
        "model_final1.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final1.add(Dropout(0.2))\n",
        "model_final1.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final1.add(Dropout(0.1))\n",
        "model_final1.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final1.add(Dropout(0.05))\n",
        "model_final1.add(Dense(units = 10, activation = 'relu'))\n",
        "model_final1.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model_final1.summary()\n",
        "model_final1.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 2500, epochs = 1000, verbose = 1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_109 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 6,311,946\n",
            "Trainable params: 6,309,898\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/1000\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 2.7801 - acc: 0.1008 - val_loss: 2.3003 - val_acc: 0.1100\n",
            "Epoch 2/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 2.2996 - acc: 0.1128 - val_loss: 2.2974 - val_acc: 0.1303\n",
            "Epoch 3/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 2.2822 - acc: 0.1425 - val_loss: 2.1781 - val_acc: 0.2533\n",
            "Epoch 4/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 2.1918 - acc: 0.2021 - val_loss: 3.1285 - val_acc: 0.1130\n",
            "Epoch 5/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 2.1246 - acc: 0.2354 - val_loss: 2.3792 - val_acc: 0.1160\n",
            "Epoch 6/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 2.0167 - acc: 0.2608 - val_loss: 2.4704 - val_acc: 0.1526\n",
            "Epoch 7/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.9384 - acc: 0.3028 - val_loss: 2.3857 - val_acc: 0.1550\n",
            "Epoch 8/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.8950 - acc: 0.3419 - val_loss: 2.3339 - val_acc: 0.1764\n",
            "Epoch 9/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.8231 - acc: 0.3653 - val_loss: 2.2683 - val_acc: 0.1799\n",
            "Epoch 10/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.7800 - acc: 0.3847 - val_loss: 3.1404 - val_acc: 0.1722\n",
            "Epoch 11/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.7402 - acc: 0.4056 - val_loss: 2.4899 - val_acc: 0.2412\n",
            "Epoch 12/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.7397 - acc: 0.4131 - val_loss: 3.1771 - val_acc: 0.2067\n",
            "Epoch 13/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.7030 - acc: 0.4207 - val_loss: 3.6467 - val_acc: 0.2036\n",
            "Epoch 14/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.6882 - acc: 0.4379 - val_loss: 2.2683 - val_acc: 0.1726\n",
            "Epoch 15/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.6311 - acc: 0.4423 - val_loss: 2.8056 - val_acc: 0.1429\n",
            "Epoch 16/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.6238 - acc: 0.4512 - val_loss: 4.5706 - val_acc: 0.2102\n",
            "Epoch 17/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.6065 - acc: 0.4625 - val_loss: 3.2255 - val_acc: 0.1882\n",
            "Epoch 18/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.5771 - acc: 0.4742 - val_loss: 3.0306 - val_acc: 0.2076\n",
            "Epoch 19/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.5983 - acc: 0.4655 - val_loss: 3.1253 - val_acc: 0.2830\n",
            "Epoch 20/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.5406 - acc: 0.4784 - val_loss: 2.6047 - val_acc: 0.2768\n",
            "Epoch 21/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.5179 - acc: 0.4844 - val_loss: 2.5507 - val_acc: 0.2531\n",
            "Epoch 22/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.5024 - acc: 0.4966 - val_loss: 2.0354 - val_acc: 0.3215\n",
            "Epoch 23/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4855 - acc: 0.5008 - val_loss: 2.1187 - val_acc: 0.3559\n",
            "Epoch 24/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4845 - acc: 0.5028 - val_loss: 3.6447 - val_acc: 0.1648\n",
            "Epoch 25/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4785 - acc: 0.5081 - val_loss: 2.4646 - val_acc: 0.3393\n",
            "Epoch 26/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4594 - acc: 0.5187 - val_loss: 2.0048 - val_acc: 0.3739\n",
            "Epoch 27/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4628 - acc: 0.5088 - val_loss: 2.4269 - val_acc: 0.2325\n",
            "Epoch 28/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4373 - acc: 0.5176 - val_loss: 2.1577 - val_acc: 0.3236\n",
            "Epoch 29/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4237 - acc: 0.5292 - val_loss: 2.5023 - val_acc: 0.3037\n",
            "Epoch 30/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4367 - acc: 0.5232 - val_loss: 2.7283 - val_acc: 0.3247\n",
            "Epoch 31/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3956 - acc: 0.5380 - val_loss: 3.7142 - val_acc: 0.2660\n",
            "Epoch 32/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3755 - acc: 0.5466 - val_loss: 4.1139 - val_acc: 0.3185\n",
            "Epoch 33/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3947 - acc: 0.5381 - val_loss: 2.3486 - val_acc: 0.3418\n",
            "Epoch 34/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4315 - acc: 0.5216 - val_loss: 1.7670 - val_acc: 0.4718\n",
            "Epoch 35/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.4000 - acc: 0.5340 - val_loss: 1.9908 - val_acc: 0.3859\n",
            "Epoch 36/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3767 - acc: 0.5440 - val_loss: 1.8137 - val_acc: 0.4509\n",
            "Epoch 37/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3663 - acc: 0.5439 - val_loss: 1.5105 - val_acc: 0.5456\n",
            "Epoch 38/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3565 - acc: 0.5494 - val_loss: 1.9275 - val_acc: 0.4953\n",
            "Epoch 39/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3683 - acc: 0.5487 - val_loss: 1.5233 - val_acc: 0.4836\n",
            "Epoch 40/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3400 - acc: 0.5554 - val_loss: 1.7057 - val_acc: 0.4461\n",
            "Epoch 41/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3352 - acc: 0.5549 - val_loss: 2.9613 - val_acc: 0.3433\n",
            "Epoch 42/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3426 - acc: 0.5526 - val_loss: 1.6485 - val_acc: 0.5033\n",
            "Epoch 43/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3024 - acc: 0.5745 - val_loss: 1.3854 - val_acc: 0.5523\n",
            "Epoch 44/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3435 - acc: 0.5578 - val_loss: 1.6429 - val_acc: 0.4870\n",
            "Epoch 45/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2895 - acc: 0.5791 - val_loss: 1.7564 - val_acc: 0.4517\n",
            "Epoch 46/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3443 - acc: 0.5504 - val_loss: 1.9770 - val_acc: 0.4774\n",
            "Epoch 47/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2933 - acc: 0.5738 - val_loss: 1.3925 - val_acc: 0.5657\n",
            "Epoch 48/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3133 - acc: 0.5672 - val_loss: 1.8086 - val_acc: 0.4728\n",
            "Epoch 49/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2972 - acc: 0.5694 - val_loss: 1.9591 - val_acc: 0.3897\n",
            "Epoch 50/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2789 - acc: 0.5776 - val_loss: 1.3024 - val_acc: 0.5752\n",
            "Epoch 51/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.3006 - acc: 0.5716 - val_loss: 1.3132 - val_acc: 0.5665\n",
            "Epoch 52/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2871 - acc: 0.5814 - val_loss: 1.3406 - val_acc: 0.5471\n",
            "Epoch 53/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2980 - acc: 0.5799 - val_loss: 1.6602 - val_acc: 0.5403\n",
            "Epoch 54/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2660 - acc: 0.5875 - val_loss: 1.2829 - val_acc: 0.5761\n",
            "Epoch 55/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2594 - acc: 0.5909 - val_loss: 1.1662 - val_acc: 0.6186\n",
            "Epoch 56/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2537 - acc: 0.5942 - val_loss: 1.1320 - val_acc: 0.6276\n",
            "Epoch 57/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2689 - acc: 0.5900 - val_loss: 1.2211 - val_acc: 0.6178\n",
            "Epoch 58/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2273 - acc: 0.6053 - val_loss: 1.2590 - val_acc: 0.6251\n",
            "Epoch 59/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2290 - acc: 0.6055 - val_loss: 2.2285 - val_acc: 0.4696\n",
            "Epoch 60/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2176 - acc: 0.6117 - val_loss: 1.2194 - val_acc: 0.6110\n",
            "Epoch 61/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2370 - acc: 0.6058 - val_loss: 1.1808 - val_acc: 0.6123\n",
            "Epoch 62/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1928 - acc: 0.6139 - val_loss: 1.1130 - val_acc: 0.6382\n",
            "Epoch 63/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1898 - acc: 0.6223 - val_loss: 1.3712 - val_acc: 0.6091\n",
            "Epoch 64/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2090 - acc: 0.6147 - val_loss: 1.0640 - val_acc: 0.6596\n",
            "Epoch 65/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1691 - acc: 0.6268 - val_loss: 1.2530 - val_acc: 0.6417\n",
            "Epoch 66/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1861 - acc: 0.6303 - val_loss: 1.7759 - val_acc: 0.4677\n",
            "Epoch 67/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2577 - acc: 0.6000 - val_loss: 1.4115 - val_acc: 0.5711\n",
            "Epoch 68/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2024 - acc: 0.6155 - val_loss: 1.3749 - val_acc: 0.5793\n",
            "Epoch 69/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.2100 - acc: 0.6149 - val_loss: 1.1019 - val_acc: 0.6474\n",
            "Epoch 70/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1934 - acc: 0.6196 - val_loss: 1.2261 - val_acc: 0.6246\n",
            "Epoch 71/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1832 - acc: 0.6250 - val_loss: 1.1407 - val_acc: 0.6319\n",
            "Epoch 72/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1618 - acc: 0.6347 - val_loss: 1.0914 - val_acc: 0.6446\n",
            "Epoch 73/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1488 - acc: 0.6394 - val_loss: 1.3785 - val_acc: 0.5997\n",
            "Epoch 74/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1633 - acc: 0.6343 - val_loss: 1.2736 - val_acc: 0.6140\n",
            "Epoch 75/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1610 - acc: 0.6373 - val_loss: 1.0412 - val_acc: 0.6720\n",
            "Epoch 76/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1392 - acc: 0.6462 - val_loss: 1.5549 - val_acc: 0.5287\n",
            "Epoch 77/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1588 - acc: 0.6398 - val_loss: 1.5057 - val_acc: 0.5759\n",
            "Epoch 78/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1373 - acc: 0.6503 - val_loss: 1.0391 - val_acc: 0.7072\n",
            "Epoch 79/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1307 - acc: 0.6465 - val_loss: 1.2693 - val_acc: 0.5882\n",
            "Epoch 80/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1549 - acc: 0.6426 - val_loss: 1.7863 - val_acc: 0.4658\n",
            "Epoch 81/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1297 - acc: 0.6508 - val_loss: 1.3587 - val_acc: 0.5490\n",
            "Epoch 82/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1655 - acc: 0.6393 - val_loss: 1.1641 - val_acc: 0.6242\n",
            "Epoch 83/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1671 - acc: 0.6409 - val_loss: 1.1142 - val_acc: 0.6676\n",
            "Epoch 84/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1885 - acc: 0.6305 - val_loss: 1.0202 - val_acc: 0.6891\n",
            "Epoch 85/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1311 - acc: 0.6543 - val_loss: 1.0807 - val_acc: 0.6656\n",
            "Epoch 86/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1586 - acc: 0.6494 - val_loss: 1.1554 - val_acc: 0.6497\n",
            "Epoch 87/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1624 - acc: 0.6444 - val_loss: 1.0106 - val_acc: 0.6790\n",
            "Epoch 88/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1260 - acc: 0.6562 - val_loss: 0.9263 - val_acc: 0.7268\n",
            "Epoch 89/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1017 - acc: 0.6663 - val_loss: 1.5740 - val_acc: 0.5632\n",
            "Epoch 90/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1235 - acc: 0.6581 - val_loss: 1.1148 - val_acc: 0.6794\n",
            "Epoch 91/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1357 - acc: 0.6540 - val_loss: 1.3678 - val_acc: 0.5768\n",
            "Epoch 92/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1219 - acc: 0.6591 - val_loss: 1.2098 - val_acc: 0.6237\n",
            "Epoch 93/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0979 - acc: 0.6674 - val_loss: 1.4020 - val_acc: 0.5856\n",
            "Epoch 94/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1414 - acc: 0.6494 - val_loss: 1.4254 - val_acc: 0.5593\n",
            "Epoch 95/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1157 - acc: 0.6620 - val_loss: 1.2534 - val_acc: 0.6397\n",
            "Epoch 96/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0976 - acc: 0.6668 - val_loss: 1.0390 - val_acc: 0.6829\n",
            "Epoch 97/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0884 - acc: 0.6688 - val_loss: 1.0300 - val_acc: 0.6978\n",
            "Epoch 98/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0881 - acc: 0.6706 - val_loss: 1.6765 - val_acc: 0.5448\n",
            "Epoch 99/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0996 - acc: 0.6682 - val_loss: 1.1020 - val_acc: 0.6619\n",
            "Epoch 100/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1007 - acc: 0.6655 - val_loss: 1.6680 - val_acc: 0.4783\n",
            "Epoch 101/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1235 - acc: 0.6587 - val_loss: 0.9610 - val_acc: 0.7146\n",
            "Epoch 102/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0589 - acc: 0.6787 - val_loss: 1.0150 - val_acc: 0.6947\n",
            "Epoch 103/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0632 - acc: 0.6821 - val_loss: 1.0905 - val_acc: 0.6721\n",
            "Epoch 104/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0615 - acc: 0.6795 - val_loss: 1.0860 - val_acc: 0.6722\n",
            "Epoch 105/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0505 - acc: 0.6820 - val_loss: 1.0488 - val_acc: 0.6874\n",
            "Epoch 106/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0636 - acc: 0.6787 - val_loss: 1.6195 - val_acc: 0.5689\n",
            "Epoch 107/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0677 - acc: 0.6770 - val_loss: 1.1319 - val_acc: 0.6507\n",
            "Epoch 108/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0775 - acc: 0.6801 - val_loss: 0.9789 - val_acc: 0.6918\n",
            "Epoch 109/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0965 - acc: 0.6670 - val_loss: 0.9424 - val_acc: 0.7189\n",
            "Epoch 110/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0421 - acc: 0.6844 - val_loss: 1.0797 - val_acc: 0.7056\n",
            "Epoch 111/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0959 - acc: 0.6682 - val_loss: 1.0888 - val_acc: 0.6541\n",
            "Epoch 112/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0495 - acc: 0.6834 - val_loss: 0.9247 - val_acc: 0.7284\n",
            "Epoch 113/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0864 - acc: 0.6721 - val_loss: 1.0099 - val_acc: 0.6944\n",
            "Epoch 114/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0527 - acc: 0.6816 - val_loss: 1.4829 - val_acc: 0.5923\n",
            "Epoch 115/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0626 - acc: 0.6774 - val_loss: 1.1913 - val_acc: 0.6297\n",
            "Epoch 116/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0488 - acc: 0.6841 - val_loss: 1.0639 - val_acc: 0.6723\n",
            "Epoch 117/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0481 - acc: 0.6845 - val_loss: 1.1834 - val_acc: 0.6241\n",
            "Epoch 118/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0615 - acc: 0.6822 - val_loss: 0.9976 - val_acc: 0.7106\n",
            "Epoch 119/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1009 - acc: 0.6626 - val_loss: 0.9113 - val_acc: 0.7282\n",
            "Epoch 120/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0383 - acc: 0.6879 - val_loss: 0.8509 - val_acc: 0.7476\n",
            "Epoch 121/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0485 - acc: 0.6797 - val_loss: 1.2585 - val_acc: 0.5840\n",
            "Epoch 122/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1320 - acc: 0.6545 - val_loss: 1.1404 - val_acc: 0.6511\n",
            "Epoch 123/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0629 - acc: 0.6807 - val_loss: 0.8679 - val_acc: 0.7434\n",
            "Epoch 124/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0487 - acc: 0.6829 - val_loss: 1.5185 - val_acc: 0.6234\n",
            "Epoch 125/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0622 - acc: 0.6784 - val_loss: 0.8954 - val_acc: 0.7309\n",
            "Epoch 126/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0981 - acc: 0.6660 - val_loss: 1.2902 - val_acc: 0.5964\n",
            "Epoch 127/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1206 - acc: 0.6549 - val_loss: 1.3272 - val_acc: 0.5763\n",
            "Epoch 128/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1049 - acc: 0.6604 - val_loss: 1.1291 - val_acc: 0.6396\n",
            "Epoch 129/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0728 - acc: 0.6731 - val_loss: 1.0393 - val_acc: 0.6804\n",
            "Epoch 130/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0561 - acc: 0.6795 - val_loss: 1.0193 - val_acc: 0.6812\n",
            "Epoch 131/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0491 - acc: 0.6831 - val_loss: 1.0201 - val_acc: 0.6805\n",
            "Epoch 132/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.1104 - acc: 0.6605 - val_loss: 1.0378 - val_acc: 0.6742\n",
            "Epoch 133/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0868 - acc: 0.6697 - val_loss: 1.4965 - val_acc: 0.5962\n",
            "Epoch 134/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0780 - acc: 0.6771 - val_loss: 0.8848 - val_acc: 0.7405\n",
            "Epoch 135/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0885 - acc: 0.6707 - val_loss: 0.9287 - val_acc: 0.7246\n",
            "Epoch 136/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0532 - acc: 0.6851 - val_loss: 1.0877 - val_acc: 0.6906\n",
            "Epoch 137/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0451 - acc: 0.6853 - val_loss: 0.9180 - val_acc: 0.7399\n",
            "Epoch 138/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0355 - acc: 0.6930 - val_loss: 0.9762 - val_acc: 0.7047\n",
            "Epoch 139/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0403 - acc: 0.6855 - val_loss: 0.9863 - val_acc: 0.7272\n",
            "Epoch 140/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0290 - acc: 0.6928 - val_loss: 0.8960 - val_acc: 0.7411\n",
            "Epoch 141/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0912 - acc: 0.6684 - val_loss: 1.0125 - val_acc: 0.6796\n",
            "Epoch 142/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0711 - acc: 0.6734 - val_loss: 1.0252 - val_acc: 0.7153\n",
            "Epoch 143/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0238 - acc: 0.6926 - val_loss: 1.1350 - val_acc: 0.6868\n",
            "Epoch 144/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0033 - acc: 0.7014 - val_loss: 1.1129 - val_acc: 0.6503\n",
            "Epoch 145/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9992 - acc: 0.7008 - val_loss: 1.2926 - val_acc: 0.6352\n",
            "Epoch 146/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0122 - acc: 0.6945 - val_loss: 0.9899 - val_acc: 0.7007\n",
            "Epoch 147/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0048 - acc: 0.6968 - val_loss: 0.9850 - val_acc: 0.6976\n",
            "Epoch 148/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0221 - acc: 0.6944 - val_loss: 0.9100 - val_acc: 0.7155\n",
            "Epoch 149/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0163 - acc: 0.6943 - val_loss: 0.9011 - val_acc: 0.7386\n",
            "Epoch 150/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0129 - acc: 0.6951 - val_loss: 0.8556 - val_acc: 0.7414\n",
            "Epoch 151/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0040 - acc: 0.6998 - val_loss: 1.0426 - val_acc: 0.7128\n",
            "Epoch 152/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9901 - acc: 0.7053 - val_loss: 0.8594 - val_acc: 0.7558\n",
            "Epoch 153/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0091 - acc: 0.7028 - val_loss: 1.2336 - val_acc: 0.6653\n",
            "Epoch 154/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0232 - acc: 0.6946 - val_loss: 1.9526 - val_acc: 0.4288\n",
            "Epoch 155/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0428 - acc: 0.6891 - val_loss: 0.8365 - val_acc: 0.7597\n",
            "Epoch 156/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0119 - acc: 0.7001 - val_loss: 0.8503 - val_acc: 0.7608\n",
            "Epoch 157/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0044 - acc: 0.7015 - val_loss: 0.8331 - val_acc: 0.7566\n",
            "Epoch 158/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0130 - acc: 0.6943 - val_loss: 0.8434 - val_acc: 0.7487\n",
            "Epoch 159/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0080 - acc: 0.6987 - val_loss: 1.1174 - val_acc: 0.6744\n",
            "Epoch 160/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9832 - acc: 0.7099 - val_loss: 1.1570 - val_acc: 0.6367\n",
            "Epoch 161/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0012 - acc: 0.7006 - val_loss: 1.4208 - val_acc: 0.5654\n",
            "Epoch 162/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0151 - acc: 0.6950 - val_loss: 1.0209 - val_acc: 0.6953\n",
            "Epoch 163/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0336 - acc: 0.6853 - val_loss: 1.4815 - val_acc: 0.5581\n",
            "Epoch 164/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0297 - acc: 0.6899 - val_loss: 0.8096 - val_acc: 0.7629\n",
            "Epoch 165/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9770 - acc: 0.7085 - val_loss: 0.9038 - val_acc: 0.7485\n",
            "Epoch 166/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9697 - acc: 0.7088 - val_loss: 1.0926 - val_acc: 0.6597\n",
            "Epoch 167/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9958 - acc: 0.7033 - val_loss: 0.9013 - val_acc: 0.7122\n",
            "Epoch 168/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9553 - acc: 0.7158 - val_loss: 0.9117 - val_acc: 0.7263\n",
            "Epoch 169/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9890 - acc: 0.7045 - val_loss: 0.8207 - val_acc: 0.7591\n",
            "Epoch 170/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9587 - acc: 0.7170 - val_loss: 0.9254 - val_acc: 0.7322\n",
            "Epoch 171/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9550 - acc: 0.7160 - val_loss: 0.9759 - val_acc: 0.7200\n",
            "Epoch 172/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9667 - acc: 0.7135 - val_loss: 0.9873 - val_acc: 0.7021\n",
            "Epoch 173/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9731 - acc: 0.7115 - val_loss: 0.8716 - val_acc: 0.7420\n",
            "Epoch 174/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9765 - acc: 0.7090 - val_loss: 0.9774 - val_acc: 0.7398\n",
            "Epoch 175/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9813 - acc: 0.7043 - val_loss: 1.0732 - val_acc: 0.6692\n",
            "Epoch 176/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9990 - acc: 0.6984 - val_loss: 1.1137 - val_acc: 0.6596\n",
            "Epoch 177/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9689 - acc: 0.7102 - val_loss: 1.1044 - val_acc: 0.6817\n",
            "Epoch 178/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9680 - acc: 0.7099 - val_loss: 0.8933 - val_acc: 0.7412\n",
            "Epoch 179/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9745 - acc: 0.7122 - val_loss: 1.0896 - val_acc: 0.6970\n",
            "Epoch 180/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9769 - acc: 0.7078 - val_loss: 0.8955 - val_acc: 0.7249\n",
            "Epoch 181/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9566 - acc: 0.7133 - val_loss: 0.9228 - val_acc: 0.7180\n",
            "Epoch 182/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9989 - acc: 0.7024 - val_loss: 0.8376 - val_acc: 0.7567\n",
            "Epoch 183/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9440 - acc: 0.7205 - val_loss: 0.7950 - val_acc: 0.7791\n",
            "Epoch 184/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9448 - acc: 0.7190 - val_loss: 0.8395 - val_acc: 0.7501\n",
            "Epoch 185/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9850 - acc: 0.7059 - val_loss: 0.8609 - val_acc: 0.7381\n",
            "Epoch 186/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0018 - acc: 0.6953 - val_loss: 0.9444 - val_acc: 0.7396\n",
            "Epoch 187/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9773 - acc: 0.7069 - val_loss: 1.0436 - val_acc: 0.6672\n",
            "Epoch 188/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0244 - acc: 0.6891 - val_loss: 0.7955 - val_acc: 0.7679\n",
            "Epoch 189/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9577 - acc: 0.7149 - val_loss: 0.9290 - val_acc: 0.7133\n",
            "Epoch 190/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9266 - acc: 0.7225 - val_loss: 0.8515 - val_acc: 0.7606\n",
            "Epoch 191/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9653 - acc: 0.7179 - val_loss: 0.8131 - val_acc: 0.7596\n",
            "Epoch 192/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9757 - acc: 0.7075 - val_loss: 1.0130 - val_acc: 0.6856\n",
            "Epoch 193/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9780 - acc: 0.7094 - val_loss: 0.8235 - val_acc: 0.7546\n",
            "Epoch 194/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0083 - acc: 0.7012 - val_loss: 0.8315 - val_acc: 0.7568\n",
            "Epoch 195/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9930 - acc: 0.7041 - val_loss: 0.9538 - val_acc: 0.7158\n",
            "Epoch 196/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9304 - acc: 0.7230 - val_loss: 0.8681 - val_acc: 0.7410\n",
            "Epoch 197/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9382 - acc: 0.7245 - val_loss: 0.9806 - val_acc: 0.6908\n",
            "Epoch 198/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9639 - acc: 0.7158 - val_loss: 1.1906 - val_acc: 0.6331\n",
            "Epoch 199/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9390 - acc: 0.7207 - val_loss: 0.9504 - val_acc: 0.7222\n",
            "Epoch 200/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0246 - acc: 0.6913 - val_loss: 1.0309 - val_acc: 0.6812\n",
            "Epoch 201/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0227 - acc: 0.6909 - val_loss: 1.0419 - val_acc: 0.6668\n",
            "Epoch 202/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0543 - acc: 0.6784 - val_loss: 0.9122 - val_acc: 0.7289\n",
            "Epoch 203/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9900 - acc: 0.7013 - val_loss: 0.8321 - val_acc: 0.7506\n",
            "Epoch 204/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9678 - acc: 0.7114 - val_loss: 1.4288 - val_acc: 0.5774\n",
            "Epoch 205/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0015 - acc: 0.7000 - val_loss: 0.8783 - val_acc: 0.7518\n",
            "Epoch 206/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9940 - acc: 0.7010 - val_loss: 0.7652 - val_acc: 0.7722\n",
            "Epoch 207/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9359 - acc: 0.7196 - val_loss: 1.0190 - val_acc: 0.6876\n",
            "Epoch 208/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0116 - acc: 0.6940 - val_loss: 0.9637 - val_acc: 0.7083\n",
            "Epoch 209/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9900 - acc: 0.7075 - val_loss: 0.8005 - val_acc: 0.7777\n",
            "Epoch 210/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9366 - acc: 0.7267 - val_loss: 0.8265 - val_acc: 0.7657\n",
            "Epoch 211/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9968 - acc: 0.7029 - val_loss: 0.7930 - val_acc: 0.7715\n",
            "Epoch 212/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0107 - acc: 0.7015 - val_loss: 0.9116 - val_acc: 0.7171\n",
            "Epoch 213/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9875 - acc: 0.7018 - val_loss: 0.7935 - val_acc: 0.7669\n",
            "Epoch 214/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9954 - acc: 0.7023 - val_loss: 0.9401 - val_acc: 0.7353\n",
            "Epoch 215/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9530 - acc: 0.7167 - val_loss: 0.7979 - val_acc: 0.7739\n",
            "Epoch 216/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9550 - acc: 0.7174 - val_loss: 0.9674 - val_acc: 0.7284\n",
            "Epoch 217/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0075 - acc: 0.7005 - val_loss: 0.8972 - val_acc: 0.7286\n",
            "Epoch 218/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9745 - acc: 0.7068 - val_loss: 0.9801 - val_acc: 0.6929\n",
            "Epoch 219/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9795 - acc: 0.7027 - val_loss: 0.8048 - val_acc: 0.7644\n",
            "Epoch 220/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9671 - acc: 0.7149 - val_loss: 0.9381 - val_acc: 0.7237\n",
            "Epoch 221/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9379 - acc: 0.7210 - val_loss: 1.0715 - val_acc: 0.6832\n",
            "Epoch 222/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9315 - acc: 0.7222 - val_loss: 0.9431 - val_acc: 0.7072\n",
            "Epoch 223/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9560 - acc: 0.7128 - val_loss: 0.8812 - val_acc: 0.7501\n",
            "Epoch 224/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9282 - acc: 0.7260 - val_loss: 0.8966 - val_acc: 0.7276\n",
            "Epoch 225/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9522 - acc: 0.7167 - val_loss: 0.9269 - val_acc: 0.7186\n",
            "Epoch 226/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0285 - acc: 0.6885 - val_loss: 0.9404 - val_acc: 0.7118\n",
            "Epoch 227/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9595 - acc: 0.7154 - val_loss: 0.7616 - val_acc: 0.7794\n",
            "Epoch 228/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9301 - acc: 0.7232 - val_loss: 0.8731 - val_acc: 0.7470\n",
            "Epoch 229/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9702 - acc: 0.7084 - val_loss: 0.8482 - val_acc: 0.7532\n",
            "Epoch 230/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9512 - acc: 0.7192 - val_loss: 0.7847 - val_acc: 0.7732\n",
            "Epoch 231/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9326 - acc: 0.7219 - val_loss: 0.8124 - val_acc: 0.7561\n",
            "Epoch 232/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9728 - acc: 0.7101 - val_loss: 0.7826 - val_acc: 0.7660\n",
            "Epoch 233/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9567 - acc: 0.7161 - val_loss: 0.7386 - val_acc: 0.7881\n",
            "Epoch 234/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8930 - acc: 0.7340 - val_loss: 0.8650 - val_acc: 0.7571\n",
            "Epoch 235/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9417 - acc: 0.7225 - val_loss: 0.8122 - val_acc: 0.7587\n",
            "Epoch 236/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9414 - acc: 0.7204 - val_loss: 0.7506 - val_acc: 0.7847\n",
            "Epoch 237/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9520 - acc: 0.7202 - val_loss: 1.0605 - val_acc: 0.6821\n",
            "Epoch 238/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9285 - acc: 0.7251 - val_loss: 0.9078 - val_acc: 0.7386\n",
            "Epoch 239/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9541 - acc: 0.7198 - val_loss: 0.8128 - val_acc: 0.7630\n",
            "Epoch 240/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9304 - acc: 0.7236 - val_loss: 0.8901 - val_acc: 0.7358\n",
            "Epoch 241/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9299 - acc: 0.7259 - val_loss: 0.7879 - val_acc: 0.7708\n",
            "Epoch 242/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9404 - acc: 0.7209 - val_loss: 0.9462 - val_acc: 0.7238\n",
            "Epoch 243/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9123 - acc: 0.7314 - val_loss: 0.9303 - val_acc: 0.7205\n",
            "Epoch 244/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9345 - acc: 0.7255 - val_loss: 1.1099 - val_acc: 0.6591\n",
            "Epoch 245/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9211 - acc: 0.7277 - val_loss: 0.9266 - val_acc: 0.7318\n",
            "Epoch 246/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9018 - acc: 0.7346 - val_loss: 1.0074 - val_acc: 0.7005\n",
            "Epoch 247/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9193 - acc: 0.7244 - val_loss: 1.1255 - val_acc: 0.6423\n",
            "Epoch 248/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9119 - acc: 0.7308 - val_loss: 0.8026 - val_acc: 0.7701\n",
            "Epoch 249/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9103 - acc: 0.7343 - val_loss: 0.9989 - val_acc: 0.7403\n",
            "Epoch 250/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9221 - acc: 0.7290 - val_loss: 0.9565 - val_acc: 0.7065\n",
            "Epoch 251/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9503 - acc: 0.7147 - val_loss: 0.8571 - val_acc: 0.7438\n",
            "Epoch 252/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9155 - acc: 0.7298 - val_loss: 1.0732 - val_acc: 0.6884\n",
            "Epoch 253/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9809 - acc: 0.7065 - val_loss: 0.9158 - val_acc: 0.7178\n",
            "Epoch 254/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9838 - acc: 0.7026 - val_loss: 0.8113 - val_acc: 0.7584\n",
            "Epoch 255/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9408 - acc: 0.7230 - val_loss: 0.7622 - val_acc: 0.7803\n",
            "Epoch 256/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9125 - acc: 0.7315 - val_loss: 1.0162 - val_acc: 0.7018\n",
            "Epoch 257/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9372 - acc: 0.7225 - val_loss: 0.9347 - val_acc: 0.7400\n",
            "Epoch 258/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9202 - acc: 0.7278 - val_loss: 0.7717 - val_acc: 0.7757\n",
            "Epoch 259/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8953 - acc: 0.7309 - val_loss: 0.8214 - val_acc: 0.7476\n",
            "Epoch 260/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0025 - acc: 0.6947 - val_loss: 1.1108 - val_acc: 0.6549\n",
            "Epoch 261/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9675 - acc: 0.7092 - val_loss: 0.8109 - val_acc: 0.7567\n",
            "Epoch 262/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9056 - acc: 0.7311 - val_loss: 0.7464 - val_acc: 0.7883\n",
            "Epoch 263/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8912 - acc: 0.7370 - val_loss: 1.0015 - val_acc: 0.7054\n",
            "Epoch 264/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9395 - acc: 0.7209 - val_loss: 0.8300 - val_acc: 0.7417\n",
            "Epoch 265/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9305 - acc: 0.7251 - val_loss: 0.8225 - val_acc: 0.7607\n",
            "Epoch 266/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9342 - acc: 0.7217 - val_loss: 0.8565 - val_acc: 0.7322\n",
            "Epoch 267/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9244 - acc: 0.7257 - val_loss: 0.7753 - val_acc: 0.7744\n",
            "Epoch 268/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9170 - acc: 0.7299 - val_loss: 1.0949 - val_acc: 0.6835\n",
            "Epoch 269/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9601 - acc: 0.7159 - val_loss: 1.0086 - val_acc: 0.6880\n",
            "Epoch 270/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9430 - acc: 0.7194 - val_loss: 0.8127 - val_acc: 0.7581\n",
            "Epoch 271/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9095 - acc: 0.7353 - val_loss: 0.9812 - val_acc: 0.7346\n",
            "Epoch 272/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8886 - acc: 0.7368 - val_loss: 0.8945 - val_acc: 0.7337\n",
            "Epoch 273/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9013 - acc: 0.7308 - val_loss: 0.8594 - val_acc: 0.7676\n",
            "Epoch 274/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9675 - acc: 0.7135 - val_loss: 0.9478 - val_acc: 0.7144\n",
            "Epoch 275/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 1.0349 - acc: 0.6885 - val_loss: 1.3139 - val_acc: 0.6107\n",
            "Epoch 276/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9706 - acc: 0.7104 - val_loss: 0.8380 - val_acc: 0.7547\n",
            "Epoch 277/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9239 - acc: 0.7283 - val_loss: 0.7725 - val_acc: 0.7777\n",
            "Epoch 278/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9688 - acc: 0.7121 - val_loss: 0.7822 - val_acc: 0.7754\n",
            "Epoch 279/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9509 - acc: 0.7166 - val_loss: 0.7621 - val_acc: 0.7824\n",
            "Epoch 280/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8845 - acc: 0.7417 - val_loss: 1.2774 - val_acc: 0.6464\n",
            "Epoch 281/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8984 - acc: 0.7386 - val_loss: 1.4162 - val_acc: 0.6219\n",
            "Epoch 282/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9397 - acc: 0.7213 - val_loss: 0.7711 - val_acc: 0.7786\n",
            "Epoch 283/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9302 - acc: 0.7270 - val_loss: 0.7838 - val_acc: 0.7688\n",
            "Epoch 284/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9259 - acc: 0.7254 - val_loss: 0.7671 - val_acc: 0.7767\n",
            "Epoch 285/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9374 - acc: 0.7196 - val_loss: 0.7769 - val_acc: 0.7822\n",
            "Epoch 286/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9379 - acc: 0.7234 - val_loss: 0.7001 - val_acc: 0.7946\n",
            "Epoch 287/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9203 - acc: 0.7285 - val_loss: 0.8129 - val_acc: 0.7681\n",
            "Epoch 288/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9040 - acc: 0.7365 - val_loss: 0.8293 - val_acc: 0.7675\n",
            "Epoch 289/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9251 - acc: 0.7267 - val_loss: 0.7660 - val_acc: 0.7854\n",
            "Epoch 290/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9064 - acc: 0.7340 - val_loss: 0.8964 - val_acc: 0.7237\n",
            "Epoch 291/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9196 - acc: 0.7288 - val_loss: 0.8761 - val_acc: 0.7387\n",
            "Epoch 292/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8832 - acc: 0.7404 - val_loss: 0.8921 - val_acc: 0.7429\n",
            "Epoch 293/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8887 - acc: 0.7364 - val_loss: 0.8131 - val_acc: 0.7592\n",
            "Epoch 294/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9021 - acc: 0.7339 - val_loss: 0.9489 - val_acc: 0.7012\n",
            "Epoch 295/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9407 - acc: 0.7190 - val_loss: 0.7156 - val_acc: 0.7933\n",
            "Epoch 296/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8879 - acc: 0.7407 - val_loss: 1.2267 - val_acc: 0.6339\n",
            "Epoch 297/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9420 - acc: 0.7209 - val_loss: 1.0440 - val_acc: 0.7057\n",
            "Epoch 298/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9583 - acc: 0.7153 - val_loss: 0.8231 - val_acc: 0.7582\n",
            "Epoch 299/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9739 - acc: 0.7094 - val_loss: 0.7754 - val_acc: 0.7729\n",
            "Epoch 300/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9115 - acc: 0.7282 - val_loss: 0.7949 - val_acc: 0.7626\n",
            "Epoch 301/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9649 - acc: 0.7132 - val_loss: 0.8145 - val_acc: 0.7681\n",
            "Epoch 302/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8986 - acc: 0.7364 - val_loss: 0.9127 - val_acc: 0.7229\n",
            "Epoch 303/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8911 - acc: 0.7401 - val_loss: 0.8240 - val_acc: 0.7587\n",
            "Epoch 304/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8755 - acc: 0.7427 - val_loss: 0.8188 - val_acc: 0.7671\n",
            "Epoch 305/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9196 - acc: 0.7290 - val_loss: 0.7377 - val_acc: 0.7788\n",
            "Epoch 306/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8767 - acc: 0.7429 - val_loss: 0.7529 - val_acc: 0.7902\n",
            "Epoch 307/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8910 - acc: 0.7414 - val_loss: 0.7827 - val_acc: 0.7718\n",
            "Epoch 308/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8949 - acc: 0.7415 - val_loss: 0.9861 - val_acc: 0.7017\n",
            "Epoch 309/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9018 - acc: 0.7365 - val_loss: 0.7790 - val_acc: 0.7782\n",
            "Epoch 310/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8856 - acc: 0.7424 - val_loss: 0.9431 - val_acc: 0.7239\n",
            "Epoch 311/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9155 - acc: 0.7288 - val_loss: 0.9075 - val_acc: 0.7322\n",
            "Epoch 312/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9408 - acc: 0.7237 - val_loss: 0.7819 - val_acc: 0.7670\n",
            "Epoch 313/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9039 - acc: 0.7387 - val_loss: 0.7786 - val_acc: 0.7780\n",
            "Epoch 314/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9030 - acc: 0.7347 - val_loss: 0.8109 - val_acc: 0.7648\n",
            "Epoch 315/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8696 - acc: 0.7465 - val_loss: 0.7472 - val_acc: 0.7904\n",
            "Epoch 316/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9231 - acc: 0.7300 - val_loss: 0.7951 - val_acc: 0.7636\n",
            "Epoch 317/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9475 - acc: 0.7172 - val_loss: 0.9130 - val_acc: 0.7230\n",
            "Epoch 318/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9549 - acc: 0.7170 - val_loss: 0.8307 - val_acc: 0.7575\n",
            "Epoch 319/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9008 - acc: 0.7335 - val_loss: 0.7380 - val_acc: 0.7909\n",
            "Epoch 320/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8727 - acc: 0.7482 - val_loss: 0.7415 - val_acc: 0.7913\n",
            "Epoch 321/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8642 - acc: 0.7507 - val_loss: 0.6914 - val_acc: 0.8064\n",
            "Epoch 322/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9200 - acc: 0.7284 - val_loss: 0.7707 - val_acc: 0.7756\n",
            "Epoch 323/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9134 - acc: 0.7310 - val_loss: 0.6945 - val_acc: 0.7973\n",
            "Epoch 324/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8534 - acc: 0.7506 - val_loss: 0.7418 - val_acc: 0.7878\n",
            "Epoch 325/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8890 - acc: 0.7380 - val_loss: 0.8588 - val_acc: 0.7507\n",
            "Epoch 326/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8832 - acc: 0.7429 - val_loss: 0.9816 - val_acc: 0.7112\n",
            "Epoch 327/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8920 - acc: 0.7402 - val_loss: 0.7258 - val_acc: 0.7873\n",
            "Epoch 328/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8883 - acc: 0.7412 - val_loss: 0.8008 - val_acc: 0.7704\n",
            "Epoch 329/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8717 - acc: 0.7467 - val_loss: 0.7269 - val_acc: 0.7911\n",
            "Epoch 330/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9148 - acc: 0.7324 - val_loss: 0.8734 - val_acc: 0.7301\n",
            "Epoch 331/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8936 - acc: 0.7347 - val_loss: 0.7255 - val_acc: 0.7918\n",
            "Epoch 332/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8954 - acc: 0.7400 - val_loss: 0.7513 - val_acc: 0.7824\n",
            "Epoch 333/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8716 - acc: 0.7448 - val_loss: 0.7800 - val_acc: 0.7629\n",
            "Epoch 334/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8799 - acc: 0.7389 - val_loss: 1.3346 - val_acc: 0.5604\n",
            "Epoch 335/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9651 - acc: 0.7122 - val_loss: 0.7943 - val_acc: 0.7661\n",
            "Epoch 336/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9137 - acc: 0.7297 - val_loss: 0.8778 - val_acc: 0.7401\n",
            "Epoch 337/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9011 - acc: 0.7365 - val_loss: 0.7207 - val_acc: 0.7959\n",
            "Epoch 338/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9188 - acc: 0.7245 - val_loss: 0.7951 - val_acc: 0.7706\n",
            "Epoch 339/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9077 - acc: 0.7296 - val_loss: 0.7819 - val_acc: 0.7818\n",
            "Epoch 340/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8828 - acc: 0.7421 - val_loss: 0.7886 - val_acc: 0.7676\n",
            "Epoch 341/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8747 - acc: 0.7400 - val_loss: 0.8490 - val_acc: 0.7393\n",
            "Epoch 342/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9101 - acc: 0.7309 - val_loss: 0.7691 - val_acc: 0.7713\n",
            "Epoch 343/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9594 - acc: 0.7165 - val_loss: 0.7818 - val_acc: 0.7646\n",
            "Epoch 344/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8995 - acc: 0.7328 - val_loss: 0.7506 - val_acc: 0.7872\n",
            "Epoch 345/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8678 - acc: 0.7452 - val_loss: 0.7224 - val_acc: 0.7984\n",
            "Epoch 346/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8697 - acc: 0.7451 - val_loss: 0.8055 - val_acc: 0.7604\n",
            "Epoch 347/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8726 - acc: 0.7458 - val_loss: 0.7937 - val_acc: 0.7756\n",
            "Epoch 348/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8523 - acc: 0.7492 - val_loss: 0.7850 - val_acc: 0.7832\n",
            "Epoch 349/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9628 - acc: 0.7156 - val_loss: 0.7412 - val_acc: 0.7891\n",
            "Epoch 350/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9050 - acc: 0.7350 - val_loss: 0.8756 - val_acc: 0.7441\n",
            "Epoch 351/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9255 - acc: 0.7265 - val_loss: 0.7078 - val_acc: 0.7975\n",
            "Epoch 352/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9165 - acc: 0.7324 - val_loss: 0.7986 - val_acc: 0.7549\n",
            "Epoch 353/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9072 - acc: 0.7317 - val_loss: 0.7545 - val_acc: 0.7873\n",
            "Epoch 354/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9300 - acc: 0.7260 - val_loss: 0.7386 - val_acc: 0.7931\n",
            "Epoch 355/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8835 - acc: 0.7428 - val_loss: 0.7465 - val_acc: 0.7914\n",
            "Epoch 356/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8875 - acc: 0.7410 - val_loss: 0.8534 - val_acc: 0.7459\n",
            "Epoch 357/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8644 - acc: 0.7487 - val_loss: 0.7972 - val_acc: 0.7696\n",
            "Epoch 358/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8763 - acc: 0.7462 - val_loss: 0.7321 - val_acc: 0.7946\n",
            "Epoch 359/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8603 - acc: 0.7488 - val_loss: 0.8430 - val_acc: 0.7390\n",
            "Epoch 360/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8609 - acc: 0.7467 - val_loss: 0.6942 - val_acc: 0.7962\n",
            "Epoch 361/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8559 - acc: 0.7475 - val_loss: 0.7374 - val_acc: 0.7963\n",
            "Epoch 362/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8606 - acc: 0.7511 - val_loss: 0.9711 - val_acc: 0.7214\n",
            "Epoch 363/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8931 - acc: 0.7408 - val_loss: 0.8553 - val_acc: 0.7591\n",
            "Epoch 364/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9287 - acc: 0.7249 - val_loss: 0.9253 - val_acc: 0.7369\n",
            "Epoch 365/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8652 - acc: 0.7457 - val_loss: 0.7601 - val_acc: 0.7751\n",
            "Epoch 366/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8708 - acc: 0.7464 - val_loss: 0.8114 - val_acc: 0.7604\n",
            "Epoch 367/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8669 - acc: 0.7488 - val_loss: 0.6601 - val_acc: 0.8087\n",
            "Epoch 368/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8501 - acc: 0.7519 - val_loss: 0.9456 - val_acc: 0.7099\n",
            "Epoch 369/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8986 - acc: 0.7352 - val_loss: 0.7876 - val_acc: 0.7627\n",
            "Epoch 370/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8968 - acc: 0.7339 - val_loss: 0.7390 - val_acc: 0.7939\n",
            "Epoch 371/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8570 - acc: 0.7514 - val_loss: 1.0064 - val_acc: 0.6972\n",
            "Epoch 372/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8674 - acc: 0.7451 - val_loss: 0.7343 - val_acc: 0.7812\n",
            "Epoch 373/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8599 - acc: 0.7487 - val_loss: 0.6856 - val_acc: 0.8038\n",
            "Epoch 374/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9227 - acc: 0.7320 - val_loss: 0.7416 - val_acc: 0.7842\n",
            "Epoch 375/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8825 - acc: 0.7446 - val_loss: 0.7652 - val_acc: 0.7657\n",
            "Epoch 376/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8719 - acc: 0.7448 - val_loss: 0.7679 - val_acc: 0.7874\n",
            "Epoch 377/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8825 - acc: 0.7446 - val_loss: 0.7914 - val_acc: 0.7572\n",
            "Epoch 378/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8688 - acc: 0.7497 - val_loss: 0.7110 - val_acc: 0.8028\n",
            "Epoch 379/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8513 - acc: 0.7566 - val_loss: 0.9363 - val_acc: 0.7154\n",
            "Epoch 380/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9258 - acc: 0.7277 - val_loss: 0.8645 - val_acc: 0.7488\n",
            "Epoch 381/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9143 - acc: 0.7290 - val_loss: 0.6673 - val_acc: 0.8096\n",
            "Epoch 382/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8597 - acc: 0.7490 - val_loss: 1.0284 - val_acc: 0.6855\n",
            "Epoch 383/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8968 - acc: 0.7415 - val_loss: 0.7064 - val_acc: 0.7982\n",
            "Epoch 384/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8473 - acc: 0.7512 - val_loss: 0.7092 - val_acc: 0.8061\n",
            "Epoch 385/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8644 - acc: 0.7471 - val_loss: 0.6885 - val_acc: 0.7992\n",
            "Epoch 386/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8673 - acc: 0.7467 - val_loss: 0.7586 - val_acc: 0.7711\n",
            "Epoch 387/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8594 - acc: 0.7531 - val_loss: 0.7223 - val_acc: 0.7987\n",
            "Epoch 388/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8709 - acc: 0.7442 - val_loss: 0.9448 - val_acc: 0.7181\n",
            "Epoch 389/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9930 - acc: 0.7005 - val_loss: 1.0516 - val_acc: 0.6717\n",
            "Epoch 390/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9891 - acc: 0.7046 - val_loss: 1.0973 - val_acc: 0.6499\n",
            "Epoch 391/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9603 - acc: 0.7168 - val_loss: 0.7171 - val_acc: 0.7907\n",
            "Epoch 392/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8985 - acc: 0.7336 - val_loss: 0.7758 - val_acc: 0.7736\n",
            "Epoch 393/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8777 - acc: 0.7437 - val_loss: 0.7039 - val_acc: 0.7937\n",
            "Epoch 394/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8580 - acc: 0.7504 - val_loss: 0.7713 - val_acc: 0.7877\n",
            "Epoch 395/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8866 - acc: 0.7405 - val_loss: 0.8371 - val_acc: 0.7613\n",
            "Epoch 396/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9227 - acc: 0.7313 - val_loss: 0.7405 - val_acc: 0.7880\n",
            "Epoch 397/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8951 - acc: 0.7395 - val_loss: 0.6937 - val_acc: 0.8041\n",
            "Epoch 398/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8652 - acc: 0.7489 - val_loss: 0.7393 - val_acc: 0.7814\n",
            "Epoch 399/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8659 - acc: 0.7532 - val_loss: 0.7985 - val_acc: 0.7767\n",
            "Epoch 400/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8620 - acc: 0.7513 - val_loss: 0.9689 - val_acc: 0.7154\n",
            "Epoch 401/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8559 - acc: 0.7519 - val_loss: 0.6827 - val_acc: 0.8073\n",
            "Epoch 402/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9315 - acc: 0.7279 - val_loss: 0.7153 - val_acc: 0.7932\n",
            "Epoch 403/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8563 - acc: 0.7517 - val_loss: 0.7030 - val_acc: 0.8072\n",
            "Epoch 404/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8420 - acc: 0.7562 - val_loss: 1.4217 - val_acc: 0.5353\n",
            "Epoch 405/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8953 - acc: 0.7343 - val_loss: 1.0505 - val_acc: 0.6713\n",
            "Epoch 406/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8817 - acc: 0.7413 - val_loss: 0.7192 - val_acc: 0.7903\n",
            "Epoch 407/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8726 - acc: 0.7465 - val_loss: 0.6737 - val_acc: 0.8068\n",
            "Epoch 408/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8655 - acc: 0.7443 - val_loss: 0.6989 - val_acc: 0.7995\n",
            "Epoch 409/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8638 - acc: 0.7444 - val_loss: 0.7567 - val_acc: 0.7721\n",
            "Epoch 410/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8475 - acc: 0.7533 - val_loss: 0.6986 - val_acc: 0.7997\n",
            "Epoch 411/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8647 - acc: 0.7452 - val_loss: 0.7236 - val_acc: 0.7889\n",
            "Epoch 412/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8547 - acc: 0.7485 - val_loss: 0.7907 - val_acc: 0.7694\n",
            "Epoch 413/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8811 - acc: 0.7395 - val_loss: 0.6915 - val_acc: 0.7967\n",
            "Epoch 414/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9221 - acc: 0.7268 - val_loss: 0.7990 - val_acc: 0.7752\n",
            "Epoch 415/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8773 - acc: 0.7428 - val_loss: 0.8307 - val_acc: 0.7562\n",
            "Epoch 416/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9552 - acc: 0.7159 - val_loss: 0.8568 - val_acc: 0.7408\n",
            "Epoch 417/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9137 - acc: 0.7283 - val_loss: 0.7585 - val_acc: 0.7826\n",
            "Epoch 418/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8535 - acc: 0.7486 - val_loss: 0.6728 - val_acc: 0.8056\n",
            "Epoch 419/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8655 - acc: 0.7495 - val_loss: 0.7078 - val_acc: 0.7914\n",
            "Epoch 420/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8660 - acc: 0.7490 - val_loss: 0.7344 - val_acc: 0.7878\n",
            "Epoch 421/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8775 - acc: 0.7433 - val_loss: 0.7427 - val_acc: 0.7874\n",
            "Epoch 422/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8825 - acc: 0.7424 - val_loss: 0.6867 - val_acc: 0.8021\n",
            "Epoch 423/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8460 - acc: 0.7570 - val_loss: 0.8065 - val_acc: 0.7792\n",
            "Epoch 424/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8765 - acc: 0.7456 - val_loss: 0.7141 - val_acc: 0.7992\n",
            "Epoch 425/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8599 - acc: 0.7488 - val_loss: 0.7039 - val_acc: 0.7983\n",
            "Epoch 426/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8458 - acc: 0.7511 - val_loss: 0.6918 - val_acc: 0.7965\n",
            "Epoch 427/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8464 - acc: 0.7553 - val_loss: 0.7908 - val_acc: 0.7750\n",
            "Epoch 428/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8400 - acc: 0.7542 - val_loss: 0.6875 - val_acc: 0.8041\n",
            "Epoch 429/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8343 - acc: 0.7529 - val_loss: 0.7418 - val_acc: 0.7907\n",
            "Epoch 430/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8419 - acc: 0.7549 - val_loss: 0.7504 - val_acc: 0.7909\n",
            "Epoch 431/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8616 - acc: 0.7488 - val_loss: 0.7707 - val_acc: 0.7708\n",
            "Epoch 432/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8562 - acc: 0.7513 - val_loss: 0.7504 - val_acc: 0.7797\n",
            "Epoch 433/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8386 - acc: 0.7561 - val_loss: 0.7047 - val_acc: 0.7979\n",
            "Epoch 434/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8649 - acc: 0.7495 - val_loss: 0.7145 - val_acc: 0.8018\n",
            "Epoch 435/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8364 - acc: 0.7562 - val_loss: 0.6883 - val_acc: 0.8076\n",
            "Epoch 436/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8583 - acc: 0.7480 - val_loss: 0.6758 - val_acc: 0.8048\n",
            "Epoch 437/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8605 - acc: 0.7469 - val_loss: 0.7069 - val_acc: 0.7941\n",
            "Epoch 438/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8366 - acc: 0.7605 - val_loss: 0.6870 - val_acc: 0.8016\n",
            "Epoch 439/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8583 - acc: 0.7493 - val_loss: 0.8754 - val_acc: 0.7359\n",
            "Epoch 440/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8561 - acc: 0.7528 - val_loss: 0.7097 - val_acc: 0.7924\n",
            "Epoch 441/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8557 - acc: 0.7501 - val_loss: 0.8860 - val_acc: 0.7529\n",
            "Epoch 442/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8286 - acc: 0.7582 - val_loss: 0.6990 - val_acc: 0.7945\n",
            "Epoch 443/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8951 - acc: 0.7330 - val_loss: 0.8514 - val_acc: 0.7466\n",
            "Epoch 444/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8628 - acc: 0.7505 - val_loss: 0.6522 - val_acc: 0.8149\n",
            "Epoch 445/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8896 - acc: 0.7396 - val_loss: 0.7273 - val_acc: 0.7887\n",
            "Epoch 446/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8780 - acc: 0.7410 - val_loss: 1.0471 - val_acc: 0.6805\n",
            "Epoch 447/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9062 - acc: 0.7312 - val_loss: 0.7140 - val_acc: 0.7866\n",
            "Epoch 448/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8705 - acc: 0.7433 - val_loss: 0.7058 - val_acc: 0.7985\n",
            "Epoch 449/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8587 - acc: 0.7518 - val_loss: 0.7319 - val_acc: 0.7953\n",
            "Epoch 450/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8195 - acc: 0.7628 - val_loss: 0.7310 - val_acc: 0.7934\n",
            "Epoch 451/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8180 - acc: 0.7609 - val_loss: 0.8471 - val_acc: 0.7455\n",
            "Epoch 452/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8611 - acc: 0.7505 - val_loss: 0.7376 - val_acc: 0.7894\n",
            "Epoch 453/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8433 - acc: 0.7573 - val_loss: 0.6968 - val_acc: 0.8022\n",
            "Epoch 454/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8469 - acc: 0.7534 - val_loss: 1.0805 - val_acc: 0.6718\n",
            "Epoch 455/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8382 - acc: 0.7549 - val_loss: 0.7271 - val_acc: 0.7984\n",
            "Epoch 456/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8273 - acc: 0.7601 - val_loss: 0.7099 - val_acc: 0.8019\n",
            "Epoch 457/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8531 - acc: 0.7535 - val_loss: 0.8168 - val_acc: 0.7457\n",
            "Epoch 458/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8550 - acc: 0.7484 - val_loss: 0.7036 - val_acc: 0.7931\n",
            "Epoch 459/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8494 - acc: 0.7552 - val_loss: 0.8106 - val_acc: 0.7569\n",
            "Epoch 460/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8750 - acc: 0.7475 - val_loss: 0.7178 - val_acc: 0.7960\n",
            "Epoch 461/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8549 - acc: 0.7522 - val_loss: 0.7221 - val_acc: 0.7928\n",
            "Epoch 462/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8677 - acc: 0.7470 - val_loss: 0.6779 - val_acc: 0.8099\n",
            "Epoch 463/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8468 - acc: 0.7530 - val_loss: 0.7096 - val_acc: 0.7953\n",
            "Epoch 464/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8142 - acc: 0.7628 - val_loss: 0.7152 - val_acc: 0.7872\n",
            "Epoch 465/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8240 - acc: 0.7640 - val_loss: 0.7295 - val_acc: 0.7947\n",
            "Epoch 466/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8303 - acc: 0.7621 - val_loss: 0.8357 - val_acc: 0.7598\n",
            "Epoch 467/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8306 - acc: 0.7620 - val_loss: 1.4514 - val_acc: 0.6612\n",
            "Epoch 468/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8572 - acc: 0.7542 - val_loss: 0.8370 - val_acc: 0.7494\n",
            "Epoch 469/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8289 - acc: 0.7625 - val_loss: 0.8323 - val_acc: 0.7687\n",
            "Epoch 470/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8268 - acc: 0.7607 - val_loss: 0.6694 - val_acc: 0.8003\n",
            "Epoch 471/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8161 - acc: 0.7638 - val_loss: 0.6834 - val_acc: 0.8159\n",
            "Epoch 472/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8426 - acc: 0.7555 - val_loss: 0.7336 - val_acc: 0.7893\n",
            "Epoch 473/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8207 - acc: 0.7617 - val_loss: 0.7887 - val_acc: 0.7692\n",
            "Epoch 474/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8428 - acc: 0.7517 - val_loss: 0.7069 - val_acc: 0.7967\n",
            "Epoch 475/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8394 - acc: 0.7591 - val_loss: 0.6876 - val_acc: 0.8044\n",
            "Epoch 476/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8577 - acc: 0.7527 - val_loss: 0.7066 - val_acc: 0.7942\n",
            "Epoch 477/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8224 - acc: 0.7615 - val_loss: 0.6499 - val_acc: 0.8166\n",
            "Epoch 478/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8165 - acc: 0.7627 - val_loss: 0.7286 - val_acc: 0.8053\n",
            "Epoch 479/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8207 - acc: 0.7661 - val_loss: 0.8131 - val_acc: 0.7697\n",
            "Epoch 480/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8055 - acc: 0.7630 - val_loss: 0.7326 - val_acc: 0.8021\n",
            "Epoch 481/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8414 - acc: 0.7558 - val_loss: 0.7105 - val_acc: 0.7867\n",
            "Epoch 482/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8335 - acc: 0.7576 - val_loss: 0.7053 - val_acc: 0.7998\n",
            "Epoch 483/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8197 - acc: 0.7641 - val_loss: 0.6710 - val_acc: 0.8088\n",
            "Epoch 484/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8499 - acc: 0.7515 - val_loss: 0.8355 - val_acc: 0.7603\n",
            "Epoch 485/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8770 - acc: 0.7428 - val_loss: 0.7322 - val_acc: 0.7946\n",
            "Epoch 486/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8469 - acc: 0.7547 - val_loss: 0.6886 - val_acc: 0.7937\n",
            "Epoch 487/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8462 - acc: 0.7525 - val_loss: 0.6729 - val_acc: 0.8075\n",
            "Epoch 488/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8198 - acc: 0.7640 - val_loss: 0.6241 - val_acc: 0.8227\n",
            "Epoch 489/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8579 - acc: 0.7506 - val_loss: 0.6353 - val_acc: 0.8159\n",
            "Epoch 490/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8565 - acc: 0.7499 - val_loss: 0.9111 - val_acc: 0.7168\n",
            "Epoch 491/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8416 - acc: 0.7599 - val_loss: 0.7500 - val_acc: 0.7849\n",
            "Epoch 492/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8096 - acc: 0.7675 - val_loss: 0.7421 - val_acc: 0.7958\n",
            "Epoch 493/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8342 - acc: 0.7544 - val_loss: 0.7294 - val_acc: 0.7982\n",
            "Epoch 494/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7999 - acc: 0.7683 - val_loss: 0.6328 - val_acc: 0.8202\n",
            "Epoch 495/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8692 - acc: 0.7486 - val_loss: 0.7143 - val_acc: 0.7919\n",
            "Epoch 496/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8542 - acc: 0.7505 - val_loss: 0.7606 - val_acc: 0.7732\n",
            "Epoch 497/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8235 - acc: 0.7604 - val_loss: 0.6303 - val_acc: 0.8249\n",
            "Epoch 498/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8138 - acc: 0.7685 - val_loss: 0.8423 - val_acc: 0.7627\n",
            "Epoch 499/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8244 - acc: 0.7680 - val_loss: 0.7443 - val_acc: 0.7819\n",
            "Epoch 500/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8323 - acc: 0.7617 - val_loss: 1.0480 - val_acc: 0.6883\n",
            "Epoch 501/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8287 - acc: 0.7611 - val_loss: 0.6768 - val_acc: 0.8047\n",
            "Epoch 502/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8242 - acc: 0.7570 - val_loss: 0.7432 - val_acc: 0.7804\n",
            "Epoch 503/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8386 - acc: 0.7578 - val_loss: 0.7262 - val_acc: 0.7859\n",
            "Epoch 504/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8132 - acc: 0.7687 - val_loss: 0.7049 - val_acc: 0.7992\n",
            "Epoch 505/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8813 - acc: 0.7398 - val_loss: 0.8850 - val_acc: 0.7362\n",
            "Epoch 506/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8616 - acc: 0.7466 - val_loss: 0.6974 - val_acc: 0.8092\n",
            "Epoch 507/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8686 - acc: 0.7467 - val_loss: 0.8583 - val_acc: 0.7572\n",
            "Epoch 508/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8527 - acc: 0.7502 - val_loss: 0.6874 - val_acc: 0.8012\n",
            "Epoch 509/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7979 - acc: 0.7687 - val_loss: 0.8383 - val_acc: 0.7518\n",
            "Epoch 510/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8781 - acc: 0.7432 - val_loss: 0.7959 - val_acc: 0.7624\n",
            "Epoch 511/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8315 - acc: 0.7562 - val_loss: 0.6732 - val_acc: 0.8047\n",
            "Epoch 512/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8979 - acc: 0.7354 - val_loss: 0.9077 - val_acc: 0.7448\n",
            "Epoch 513/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8642 - acc: 0.7522 - val_loss: 0.6918 - val_acc: 0.7974\n",
            "Epoch 514/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8314 - acc: 0.7621 - val_loss: 0.6287 - val_acc: 0.8202\n",
            "Epoch 515/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8135 - acc: 0.7677 - val_loss: 0.7974 - val_acc: 0.7563\n",
            "Epoch 516/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8294 - acc: 0.7593 - val_loss: 1.0616 - val_acc: 0.6781\n",
            "Epoch 517/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8383 - acc: 0.7589 - val_loss: 0.7083 - val_acc: 0.7957\n",
            "Epoch 518/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8243 - acc: 0.7639 - val_loss: 0.8253 - val_acc: 0.7620\n",
            "Epoch 519/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8066 - acc: 0.7649 - val_loss: 0.7397 - val_acc: 0.7897\n",
            "Epoch 520/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8921 - acc: 0.7388 - val_loss: 0.6903 - val_acc: 0.7993\n",
            "Epoch 521/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8852 - acc: 0.7427 - val_loss: 0.9714 - val_acc: 0.6989\n",
            "Epoch 522/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8534 - acc: 0.7558 - val_loss: 0.6672 - val_acc: 0.8056\n",
            "Epoch 523/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8367 - acc: 0.7547 - val_loss: 0.7414 - val_acc: 0.7858\n",
            "Epoch 524/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8400 - acc: 0.7552 - val_loss: 0.7879 - val_acc: 0.7611\n",
            "Epoch 525/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8313 - acc: 0.7565 - val_loss: 0.6451 - val_acc: 0.8159\n",
            "Epoch 526/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8376 - acc: 0.7590 - val_loss: 0.8621 - val_acc: 0.7433\n",
            "Epoch 527/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8601 - acc: 0.7529 - val_loss: 1.1162 - val_acc: 0.6461\n",
            "Epoch 528/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8525 - acc: 0.7525 - val_loss: 0.6536 - val_acc: 0.8165\n",
            "Epoch 529/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8316 - acc: 0.7607 - val_loss: 0.6575 - val_acc: 0.8142\n",
            "Epoch 530/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8304 - acc: 0.7628 - val_loss: 0.6768 - val_acc: 0.8073\n",
            "Epoch 531/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8082 - acc: 0.7660 - val_loss: 0.7142 - val_acc: 0.7952\n",
            "Epoch 532/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8331 - acc: 0.7578 - val_loss: 0.6842 - val_acc: 0.8065\n",
            "Epoch 533/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8111 - acc: 0.7618 - val_loss: 0.8293 - val_acc: 0.7568\n",
            "Epoch 534/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8238 - acc: 0.7603 - val_loss: 0.6656 - val_acc: 0.8143\n",
            "Epoch 535/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8283 - acc: 0.7629 - val_loss: 0.7012 - val_acc: 0.8014\n",
            "Epoch 536/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8376 - acc: 0.7609 - val_loss: 0.7864 - val_acc: 0.7663\n",
            "Epoch 537/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8004 - acc: 0.7662 - val_loss: 0.6761 - val_acc: 0.8039\n",
            "Epoch 538/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8217 - acc: 0.7590 - val_loss: 0.6448 - val_acc: 0.8129\n",
            "Epoch 539/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8867 - acc: 0.7395 - val_loss: 0.6986 - val_acc: 0.7941\n",
            "Epoch 540/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8681 - acc: 0.7455 - val_loss: 0.7999 - val_acc: 0.7607\n",
            "Epoch 541/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8597 - acc: 0.7463 - val_loss: 0.6765 - val_acc: 0.8025\n",
            "Epoch 542/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8662 - acc: 0.7470 - val_loss: 0.6758 - val_acc: 0.8006\n",
            "Epoch 543/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8278 - acc: 0.7658 - val_loss: 0.6731 - val_acc: 0.8100\n",
            "Epoch 544/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8014 - acc: 0.7721 - val_loss: 0.6531 - val_acc: 0.8179\n",
            "Epoch 545/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8762 - acc: 0.7444 - val_loss: 0.7281 - val_acc: 0.7942\n",
            "Epoch 546/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8608 - acc: 0.7469 - val_loss: 0.7966 - val_acc: 0.7566\n",
            "Epoch 547/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8354 - acc: 0.7590 - val_loss: 0.6826 - val_acc: 0.8016\n",
            "Epoch 548/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9013 - acc: 0.7344 - val_loss: 0.7319 - val_acc: 0.7852\n",
            "Epoch 549/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8215 - acc: 0.7636 - val_loss: 0.6561 - val_acc: 0.8217\n",
            "Epoch 550/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8229 - acc: 0.7656 - val_loss: 0.6892 - val_acc: 0.8001\n",
            "Epoch 551/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8455 - acc: 0.7519 - val_loss: 0.6514 - val_acc: 0.8140\n",
            "Epoch 552/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8156 - acc: 0.7672 - val_loss: 0.6754 - val_acc: 0.8089\n",
            "Epoch 553/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8248 - acc: 0.7625 - val_loss: 0.7044 - val_acc: 0.8029\n",
            "Epoch 554/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8597 - acc: 0.7498 - val_loss: 0.7444 - val_acc: 0.7871\n",
            "Epoch 555/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8146 - acc: 0.7658 - val_loss: 1.0644 - val_acc: 0.6877\n",
            "Epoch 556/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8370 - acc: 0.7581 - val_loss: 0.7436 - val_acc: 0.7887\n",
            "Epoch 557/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8488 - acc: 0.7520 - val_loss: 0.6914 - val_acc: 0.7957\n",
            "Epoch 558/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8244 - acc: 0.7606 - val_loss: 0.6573 - val_acc: 0.8113\n",
            "Epoch 559/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8021 - acc: 0.7676 - val_loss: 0.6401 - val_acc: 0.8187\n",
            "Epoch 560/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8224 - acc: 0.7603 - val_loss: 0.9160 - val_acc: 0.7203\n",
            "Epoch 561/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8128 - acc: 0.7614 - val_loss: 0.6880 - val_acc: 0.7994\n",
            "Epoch 562/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7937 - acc: 0.7722 - val_loss: 0.6931 - val_acc: 0.8044\n",
            "Epoch 563/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8098 - acc: 0.7678 - val_loss: 0.8059 - val_acc: 0.7831\n",
            "Epoch 564/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8266 - acc: 0.7615 - val_loss: 0.7162 - val_acc: 0.8049\n",
            "Epoch 565/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8176 - acc: 0.7628 - val_loss: 0.6929 - val_acc: 0.8152\n",
            "Epoch 566/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8182 - acc: 0.7643 - val_loss: 0.7662 - val_acc: 0.7842\n",
            "Epoch 567/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8527 - acc: 0.7492 - val_loss: 0.6539 - val_acc: 0.8214\n",
            "Epoch 568/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8101 - acc: 0.7696 - val_loss: 0.6936 - val_acc: 0.8033\n",
            "Epoch 569/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8371 - acc: 0.7563 - val_loss: 0.6521 - val_acc: 0.8173\n",
            "Epoch 570/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8051 - acc: 0.7706 - val_loss: 0.7144 - val_acc: 0.7999\n",
            "Epoch 571/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7941 - acc: 0.7711 - val_loss: 0.7273 - val_acc: 0.7982\n",
            "Epoch 572/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8137 - acc: 0.7658 - val_loss: 0.6784 - val_acc: 0.8094\n",
            "Epoch 573/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8002 - acc: 0.7686 - val_loss: 0.6791 - val_acc: 0.8127\n",
            "Epoch 574/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8085 - acc: 0.7693 - val_loss: 0.6329 - val_acc: 0.8212\n",
            "Epoch 575/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8362 - acc: 0.7552 - val_loss: 0.7243 - val_acc: 0.7894\n",
            "Epoch 576/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8721 - acc: 0.7464 - val_loss: 0.7738 - val_acc: 0.7771\n",
            "Epoch 577/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8183 - acc: 0.7615 - val_loss: 0.6986 - val_acc: 0.8006\n",
            "Epoch 578/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8326 - acc: 0.7550 - val_loss: 0.7219 - val_acc: 0.7882\n",
            "Epoch 579/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8124 - acc: 0.7643 - val_loss: 0.6237 - val_acc: 0.8234\n",
            "Epoch 580/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7871 - acc: 0.7725 - val_loss: 0.6610 - val_acc: 0.8179\n",
            "Epoch 581/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8496 - acc: 0.7576 - val_loss: 0.6686 - val_acc: 0.8148\n",
            "Epoch 582/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8331 - acc: 0.7595 - val_loss: 0.7026 - val_acc: 0.7950\n",
            "Epoch 583/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8027 - acc: 0.7685 - val_loss: 0.6528 - val_acc: 0.8211\n",
            "Epoch 584/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8715 - acc: 0.7412 - val_loss: 0.8771 - val_acc: 0.7285\n",
            "Epoch 585/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8485 - acc: 0.7490 - val_loss: 0.7317 - val_acc: 0.7798\n",
            "Epoch 586/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8053 - acc: 0.7697 - val_loss: 0.7622 - val_acc: 0.7780\n",
            "Epoch 587/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8011 - acc: 0.7698 - val_loss: 0.6435 - val_acc: 0.8178\n",
            "Epoch 588/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8059 - acc: 0.7679 - val_loss: 0.7835 - val_acc: 0.7722\n",
            "Epoch 589/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8635 - acc: 0.7461 - val_loss: 0.7380 - val_acc: 0.7843\n",
            "Epoch 590/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8285 - acc: 0.7620 - val_loss: 0.6810 - val_acc: 0.8079\n",
            "Epoch 591/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8274 - acc: 0.7620 - val_loss: 0.6704 - val_acc: 0.8173\n",
            "Epoch 592/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8229 - acc: 0.7664 - val_loss: 0.7130 - val_acc: 0.8041\n",
            "Epoch 593/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8439 - acc: 0.7595 - val_loss: 0.6320 - val_acc: 0.8236\n",
            "Epoch 594/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8124 - acc: 0.7625 - val_loss: 0.7216 - val_acc: 0.7946\n",
            "Epoch 595/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8213 - acc: 0.7652 - val_loss: 0.6922 - val_acc: 0.7960\n",
            "Epoch 596/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8100 - acc: 0.7652 - val_loss: 0.7047 - val_acc: 0.8023\n",
            "Epoch 597/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7927 - acc: 0.7754 - val_loss: 0.6938 - val_acc: 0.8136\n",
            "Epoch 598/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9450 - acc: 0.7215 - val_loss: 1.0921 - val_acc: 0.6537\n",
            "Epoch 599/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8894 - acc: 0.7374 - val_loss: 0.6598 - val_acc: 0.8118\n",
            "Epoch 600/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8254 - acc: 0.7637 - val_loss: 0.6566 - val_acc: 0.8118\n",
            "Epoch 601/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8409 - acc: 0.7555 - val_loss: 0.7007 - val_acc: 0.8008\n",
            "Epoch 602/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8296 - acc: 0.7575 - val_loss: 0.8311 - val_acc: 0.7572\n",
            "Epoch 603/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8478 - acc: 0.7549 - val_loss: 0.7573 - val_acc: 0.7729\n",
            "Epoch 604/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8398 - acc: 0.7549 - val_loss: 0.6355 - val_acc: 0.8189\n",
            "Epoch 605/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8033 - acc: 0.7669 - val_loss: 0.6630 - val_acc: 0.8113\n",
            "Epoch 606/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8026 - acc: 0.7688 - val_loss: 0.6761 - val_acc: 0.8059\n",
            "Epoch 607/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7939 - acc: 0.7730 - val_loss: 0.7376 - val_acc: 0.7893\n",
            "Epoch 608/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8004 - acc: 0.7682 - val_loss: 0.8233 - val_acc: 0.7594\n",
            "Epoch 609/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8057 - acc: 0.7675 - val_loss: 0.6491 - val_acc: 0.8203\n",
            "Epoch 610/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8789 - acc: 0.7442 - val_loss: 0.7491 - val_acc: 0.7726\n",
            "Epoch 611/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8421 - acc: 0.7592 - val_loss: 0.6551 - val_acc: 0.8186\n",
            "Epoch 612/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7801 - acc: 0.7739 - val_loss: 0.6240 - val_acc: 0.8252\n",
            "Epoch 613/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8449 - acc: 0.7532 - val_loss: 0.6788 - val_acc: 0.8108\n",
            "Epoch 614/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7943 - acc: 0.7685 - val_loss: 0.7001 - val_acc: 0.8052\n",
            "Epoch 615/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7894 - acc: 0.7726 - val_loss: 0.6516 - val_acc: 0.8158\n",
            "Epoch 616/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8011 - acc: 0.7686 - val_loss: 0.8226 - val_acc: 0.7626\n",
            "Epoch 617/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8179 - acc: 0.7635 - val_loss: 0.7530 - val_acc: 0.7892\n",
            "Epoch 618/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7798 - acc: 0.7732 - val_loss: 0.6125 - val_acc: 0.8270\n",
            "Epoch 619/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7862 - acc: 0.7733 - val_loss: 0.6784 - val_acc: 0.7993\n",
            "Epoch 620/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7913 - acc: 0.7690 - val_loss: 0.7241 - val_acc: 0.7936\n",
            "Epoch 621/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7929 - acc: 0.7713 - val_loss: 0.7273 - val_acc: 0.7937\n",
            "Epoch 622/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8694 - acc: 0.7476 - val_loss: 0.7185 - val_acc: 0.7925\n",
            "Epoch 623/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8119 - acc: 0.7636 - val_loss: 0.6651 - val_acc: 0.8141\n",
            "Epoch 624/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8094 - acc: 0.7674 - val_loss: 0.6644 - val_acc: 0.8218\n",
            "Epoch 625/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8406 - acc: 0.7529 - val_loss: 0.7140 - val_acc: 0.7865\n",
            "Epoch 626/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8748 - acc: 0.7453 - val_loss: 0.6813 - val_acc: 0.8084\n",
            "Epoch 627/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8097 - acc: 0.7682 - val_loss: 0.8619 - val_acc: 0.7646\n",
            "Epoch 628/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8527 - acc: 0.7515 - val_loss: 0.7177 - val_acc: 0.7853\n",
            "Epoch 629/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8260 - acc: 0.7605 - val_loss: 0.6744 - val_acc: 0.8057\n",
            "Epoch 630/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8373 - acc: 0.7567 - val_loss: 0.6555 - val_acc: 0.8241\n",
            "Epoch 631/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7969 - acc: 0.7721 - val_loss: 0.6657 - val_acc: 0.8098\n",
            "Epoch 632/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7830 - acc: 0.7716 - val_loss: 0.6477 - val_acc: 0.8136\n",
            "Epoch 633/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8020 - acc: 0.7681 - val_loss: 0.6343 - val_acc: 0.8238\n",
            "Epoch 634/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8191 - acc: 0.7635 - val_loss: 0.6497 - val_acc: 0.8145\n",
            "Epoch 635/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8336 - acc: 0.7612 - val_loss: 0.7258 - val_acc: 0.7872\n",
            "Epoch 636/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7922 - acc: 0.7709 - val_loss: 0.6209 - val_acc: 0.8263\n",
            "Epoch 637/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8181 - acc: 0.7650 - val_loss: 0.6771 - val_acc: 0.8028\n",
            "Epoch 638/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8269 - acc: 0.7620 - val_loss: 0.6542 - val_acc: 0.8138\n",
            "Epoch 639/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7915 - acc: 0.7709 - val_loss: 0.6559 - val_acc: 0.8178\n",
            "Epoch 640/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7947 - acc: 0.7766 - val_loss: 0.6363 - val_acc: 0.8196\n",
            "Epoch 641/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7900 - acc: 0.7722 - val_loss: 0.6488 - val_acc: 0.8199\n",
            "Epoch 642/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7999 - acc: 0.7695 - val_loss: 0.6306 - val_acc: 0.8223\n",
            "Epoch 643/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8144 - acc: 0.7654 - val_loss: 1.0530 - val_acc: 0.6712\n",
            "Epoch 644/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8335 - acc: 0.7614 - val_loss: 0.6581 - val_acc: 0.8175\n",
            "Epoch 645/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7984 - acc: 0.7707 - val_loss: 0.6142 - val_acc: 0.8273\n",
            "Epoch 646/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8304 - acc: 0.7582 - val_loss: 0.7212 - val_acc: 0.7844\n",
            "Epoch 647/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7918 - acc: 0.7724 - val_loss: 0.6886 - val_acc: 0.8091\n",
            "Epoch 648/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7914 - acc: 0.7717 - val_loss: 0.7351 - val_acc: 0.7998\n",
            "Epoch 649/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8096 - acc: 0.7658 - val_loss: 0.6415 - val_acc: 0.8145\n",
            "Epoch 650/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8072 - acc: 0.7640 - val_loss: 0.8386 - val_acc: 0.7587\n",
            "Epoch 651/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7880 - acc: 0.7723 - val_loss: 0.7514 - val_acc: 0.7868\n",
            "Epoch 652/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8121 - acc: 0.7686 - val_loss: 0.7465 - val_acc: 0.7789\n",
            "Epoch 653/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8430 - acc: 0.7555 - val_loss: 0.7375 - val_acc: 0.7885\n",
            "Epoch 654/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8154 - acc: 0.7656 - val_loss: 0.6490 - val_acc: 0.8174\n",
            "Epoch 655/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8037 - acc: 0.7695 - val_loss: 0.9568 - val_acc: 0.7170\n",
            "Epoch 656/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7904 - acc: 0.7705 - val_loss: 0.7106 - val_acc: 0.7886\n",
            "Epoch 657/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7861 - acc: 0.7749 - val_loss: 0.7539 - val_acc: 0.7904\n",
            "Epoch 658/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8535 - acc: 0.7545 - val_loss: 0.7049 - val_acc: 0.7937\n",
            "Epoch 659/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8304 - acc: 0.7582 - val_loss: 0.6312 - val_acc: 0.8206\n",
            "Epoch 660/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8326 - acc: 0.7593 - val_loss: 0.7566 - val_acc: 0.7737\n",
            "Epoch 661/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8454 - acc: 0.7546 - val_loss: 0.9429 - val_acc: 0.7091\n",
            "Epoch 662/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8491 - acc: 0.7556 - val_loss: 0.7012 - val_acc: 0.7970\n",
            "Epoch 663/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8273 - acc: 0.7574 - val_loss: 0.6423 - val_acc: 0.8163\n",
            "Epoch 664/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8083 - acc: 0.7655 - val_loss: 0.6393 - val_acc: 0.8209\n",
            "Epoch 665/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7795 - acc: 0.7761 - val_loss: 0.6858 - val_acc: 0.7973\n",
            "Epoch 666/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8088 - acc: 0.7644 - val_loss: 0.6811 - val_acc: 0.8081\n",
            "Epoch 667/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8394 - acc: 0.7550 - val_loss: 0.7374 - val_acc: 0.7963\n",
            "Epoch 668/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8056 - acc: 0.7675 - val_loss: 0.7038 - val_acc: 0.7941\n",
            "Epoch 669/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7865 - acc: 0.7739 - val_loss: 0.6822 - val_acc: 0.8161\n",
            "Epoch 670/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8065 - acc: 0.7708 - val_loss: 0.6311 - val_acc: 0.8226\n",
            "Epoch 671/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8042 - acc: 0.7655 - val_loss: 0.6702 - val_acc: 0.8019\n",
            "Epoch 672/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8470 - acc: 0.7536 - val_loss: 1.1641 - val_acc: 0.6491\n",
            "Epoch 673/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.9176 - acc: 0.7281 - val_loss: 0.7632 - val_acc: 0.7752\n",
            "Epoch 674/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8744 - acc: 0.7453 - val_loss: 0.7009 - val_acc: 0.8005\n",
            "Epoch 675/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8280 - acc: 0.7621 - val_loss: 0.6863 - val_acc: 0.8163\n",
            "Epoch 676/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8182 - acc: 0.7646 - val_loss: 0.6628 - val_acc: 0.8212\n",
            "Epoch 677/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8216 - acc: 0.7638 - val_loss: 0.6547 - val_acc: 0.8218\n",
            "Epoch 678/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8146 - acc: 0.7680 - val_loss: 0.6584 - val_acc: 0.8117\n",
            "Epoch 679/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8436 - acc: 0.7555 - val_loss: 0.8105 - val_acc: 0.7566\n",
            "Epoch 680/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8594 - acc: 0.7496 - val_loss: 0.6949 - val_acc: 0.8039\n",
            "Epoch 681/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7950 - acc: 0.7728 - val_loss: 0.6328 - val_acc: 0.8260\n",
            "Epoch 682/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8140 - acc: 0.7632 - val_loss: 0.6520 - val_acc: 0.8178\n",
            "Epoch 683/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8074 - acc: 0.7678 - val_loss: 0.6720 - val_acc: 0.8230\n",
            "Epoch 684/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8350 - acc: 0.7610 - val_loss: 0.6783 - val_acc: 0.8059\n",
            "Epoch 685/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7942 - acc: 0.7665 - val_loss: 0.8550 - val_acc: 0.7420\n",
            "Epoch 686/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7804 - acc: 0.7759 - val_loss: 0.6158 - val_acc: 0.8299\n",
            "Epoch 687/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8131 - acc: 0.7679 - val_loss: 0.6993 - val_acc: 0.8083\n",
            "Epoch 688/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8018 - acc: 0.7665 - val_loss: 0.6997 - val_acc: 0.8075\n",
            "Epoch 689/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7808 - acc: 0.7756 - val_loss: 0.7805 - val_acc: 0.7671\n",
            "Epoch 690/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8030 - acc: 0.7692 - val_loss: 0.7091 - val_acc: 0.8006\n",
            "Epoch 691/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7991 - acc: 0.7697 - val_loss: 0.7126 - val_acc: 0.7839\n",
            "Epoch 692/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8573 - acc: 0.7470 - val_loss: 1.1301 - val_acc: 0.6529\n",
            "Epoch 693/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8520 - acc: 0.7530 - val_loss: 0.6837 - val_acc: 0.7997\n",
            "Epoch 694/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8041 - acc: 0.7654 - val_loss: 0.6270 - val_acc: 0.8188\n",
            "Epoch 695/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8033 - acc: 0.7730 - val_loss: 0.6853 - val_acc: 0.8069\n",
            "Epoch 696/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8183 - acc: 0.7618 - val_loss: 0.6705 - val_acc: 0.8120\n",
            "Epoch 697/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7918 - acc: 0.7678 - val_loss: 0.6164 - val_acc: 0.8264\n",
            "Epoch 698/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7993 - acc: 0.7689 - val_loss: 0.6478 - val_acc: 0.8164\n",
            "Epoch 699/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8174 - acc: 0.7645 - val_loss: 0.9474 - val_acc: 0.7154\n",
            "Epoch 700/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8219 - acc: 0.7611 - val_loss: 0.8382 - val_acc: 0.7498\n",
            "Epoch 701/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8080 - acc: 0.7624 - val_loss: 0.8371 - val_acc: 0.7417\n",
            "Epoch 702/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7979 - acc: 0.7722 - val_loss: 0.6859 - val_acc: 0.8022\n",
            "Epoch 703/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7723 - acc: 0.7788 - val_loss: 0.7749 - val_acc: 0.7817\n",
            "Epoch 704/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8304 - acc: 0.7618 - val_loss: 0.6753 - val_acc: 0.8083\n",
            "Epoch 705/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7915 - acc: 0.7719 - val_loss: 0.8585 - val_acc: 0.7367\n",
            "Epoch 706/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8084 - acc: 0.7673 - val_loss: 0.6725 - val_acc: 0.8069\n",
            "Epoch 707/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7618 - acc: 0.7828 - val_loss: 0.6979 - val_acc: 0.8082\n",
            "Epoch 708/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7715 - acc: 0.7777 - val_loss: 0.6353 - val_acc: 0.8257\n",
            "Epoch 709/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8004 - acc: 0.7695 - val_loss: 0.6894 - val_acc: 0.8039\n",
            "Epoch 710/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7723 - acc: 0.7749 - val_loss: 0.6746 - val_acc: 0.8219\n",
            "Epoch 711/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8257 - acc: 0.7567 - val_loss: 0.6603 - val_acc: 0.8030\n",
            "Epoch 712/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8480 - acc: 0.7522 - val_loss: 0.8039 - val_acc: 0.7731\n",
            "Epoch 713/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8250 - acc: 0.7630 - val_loss: 0.6453 - val_acc: 0.8174\n",
            "Epoch 714/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8003 - acc: 0.7725 - val_loss: 0.6287 - val_acc: 0.8253\n",
            "Epoch 715/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8163 - acc: 0.7667 - val_loss: 0.6685 - val_acc: 0.8118\n",
            "Epoch 716/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8105 - acc: 0.7681 - val_loss: 0.7019 - val_acc: 0.7988\n",
            "Epoch 717/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7947 - acc: 0.7716 - val_loss: 0.6805 - val_acc: 0.8128\n",
            "Epoch 718/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7950 - acc: 0.7696 - val_loss: 0.6649 - val_acc: 0.8075\n",
            "Epoch 719/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8520 - acc: 0.7525 - val_loss: 0.6378 - val_acc: 0.8185\n",
            "Epoch 720/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8032 - acc: 0.7683 - val_loss: 0.6345 - val_acc: 0.8147\n",
            "Epoch 721/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7716 - acc: 0.7748 - val_loss: 0.6247 - val_acc: 0.8239\n",
            "Epoch 722/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7757 - acc: 0.7787 - val_loss: 0.6767 - val_acc: 0.8131\n",
            "Epoch 723/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8178 - acc: 0.7613 - val_loss: 0.6993 - val_acc: 0.7994\n",
            "Epoch 724/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7901 - acc: 0.7720 - val_loss: 0.6499 - val_acc: 0.8230\n",
            "Epoch 725/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7788 - acc: 0.7748 - val_loss: 0.6732 - val_acc: 0.8156\n",
            "Epoch 726/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7774 - acc: 0.7807 - val_loss: 1.0078 - val_acc: 0.7144\n",
            "Epoch 727/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7678 - acc: 0.7788 - val_loss: 0.6285 - val_acc: 0.8317\n",
            "Epoch 728/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8082 - acc: 0.7640 - val_loss: 0.6439 - val_acc: 0.8266\n",
            "Epoch 729/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8086 - acc: 0.7644 - val_loss: 0.8775 - val_acc: 0.7263\n",
            "Epoch 730/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8259 - acc: 0.7591 - val_loss: 0.6940 - val_acc: 0.7942\n",
            "Epoch 731/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8348 - acc: 0.7599 - val_loss: 0.7549 - val_acc: 0.7823\n",
            "Epoch 732/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8285 - acc: 0.7680 - val_loss: 1.3138 - val_acc: 0.6623\n",
            "Epoch 733/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8022 - acc: 0.7686 - val_loss: 0.7266 - val_acc: 0.7870\n",
            "Epoch 734/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8106 - acc: 0.7623 - val_loss: 0.6739 - val_acc: 0.8085\n",
            "Epoch 735/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7882 - acc: 0.7758 - val_loss: 0.6238 - val_acc: 0.8272\n",
            "Epoch 736/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7940 - acc: 0.7745 - val_loss: 0.6243 - val_acc: 0.8258\n",
            "Epoch 737/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7866 - acc: 0.7757 - val_loss: 0.7138 - val_acc: 0.8018\n",
            "Epoch 738/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8016 - acc: 0.7740 - val_loss: 0.6660 - val_acc: 0.8199\n",
            "Epoch 739/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8183 - acc: 0.7689 - val_loss: 0.6375 - val_acc: 0.8194\n",
            "Epoch 740/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7895 - acc: 0.7755 - val_loss: 0.6844 - val_acc: 0.8091\n",
            "Epoch 741/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7977 - acc: 0.7703 - val_loss: 0.8255 - val_acc: 0.7517\n",
            "Epoch 742/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8407 - acc: 0.7529 - val_loss: 0.7203 - val_acc: 0.7876\n",
            "Epoch 743/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8409 - acc: 0.7512 - val_loss: 0.9609 - val_acc: 0.6989\n",
            "Epoch 744/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8143 - acc: 0.7579 - val_loss: 0.6546 - val_acc: 0.8164\n",
            "Epoch 745/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8000 - acc: 0.7693 - val_loss: 0.7457 - val_acc: 0.7832\n",
            "Epoch 746/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7888 - acc: 0.7754 - val_loss: 0.6890 - val_acc: 0.7962\n",
            "Epoch 747/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7860 - acc: 0.7748 - val_loss: 0.8036 - val_acc: 0.7607\n",
            "Epoch 748/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8176 - acc: 0.7595 - val_loss: 0.6382 - val_acc: 0.8209\n",
            "Epoch 749/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8086 - acc: 0.7676 - val_loss: 0.7859 - val_acc: 0.7683\n",
            "Epoch 750/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7849 - acc: 0.7711 - val_loss: 0.6179 - val_acc: 0.8286\n",
            "Epoch 751/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7704 - acc: 0.7784 - val_loss: 0.6574 - val_acc: 0.8163\n",
            "Epoch 752/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7733 - acc: 0.7805 - val_loss: 0.6242 - val_acc: 0.8266\n",
            "Epoch 753/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8488 - acc: 0.7492 - val_loss: 0.7334 - val_acc: 0.7898\n",
            "Epoch 754/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8406 - acc: 0.7553 - val_loss: 0.6509 - val_acc: 0.8132\n",
            "Epoch 755/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7910 - acc: 0.7721 - val_loss: 0.6552 - val_acc: 0.8190\n",
            "Epoch 756/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8294 - acc: 0.7616 - val_loss: 0.6889 - val_acc: 0.8061\n",
            "Epoch 757/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7922 - acc: 0.7747 - val_loss: 0.7589 - val_acc: 0.7849\n",
            "Epoch 758/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7792 - acc: 0.7788 - val_loss: 0.6886 - val_acc: 0.8076\n",
            "Epoch 759/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8256 - acc: 0.7623 - val_loss: 0.7028 - val_acc: 0.7923\n",
            "Epoch 760/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7963 - acc: 0.7738 - val_loss: 0.6473 - val_acc: 0.8203\n",
            "Epoch 761/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7765 - acc: 0.7827 - val_loss: 0.6526 - val_acc: 0.8221\n",
            "Epoch 762/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7917 - acc: 0.7768 - val_loss: 0.7983 - val_acc: 0.7554\n",
            "Epoch 763/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8021 - acc: 0.7673 - val_loss: 0.8245 - val_acc: 0.7658\n",
            "Epoch 764/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8269 - acc: 0.7582 - val_loss: 0.6839 - val_acc: 0.7995\n",
            "Epoch 765/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7876 - acc: 0.7734 - val_loss: 0.6451 - val_acc: 0.8225\n",
            "Epoch 766/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7939 - acc: 0.7764 - val_loss: 0.6851 - val_acc: 0.8071\n",
            "Epoch 767/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8116 - acc: 0.7690 - val_loss: 0.6734 - val_acc: 0.8142\n",
            "Epoch 768/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7742 - acc: 0.7748 - val_loss: 0.6509 - val_acc: 0.8236\n",
            "Epoch 769/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7800 - acc: 0.7774 - val_loss: 0.6204 - val_acc: 0.8228\n",
            "Epoch 770/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7662 - acc: 0.7790 - val_loss: 0.6391 - val_acc: 0.8179\n",
            "Epoch 771/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7884 - acc: 0.7772 - val_loss: 0.6767 - val_acc: 0.8061\n",
            "Epoch 772/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8054 - acc: 0.7665 - val_loss: 0.7188 - val_acc: 0.7896\n",
            "Epoch 773/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7913 - acc: 0.7729 - val_loss: 0.7163 - val_acc: 0.8107\n",
            "Epoch 774/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8322 - acc: 0.7605 - val_loss: 0.7017 - val_acc: 0.7934\n",
            "Epoch 775/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7914 - acc: 0.7681 - val_loss: 0.6440 - val_acc: 0.8247\n",
            "Epoch 776/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7576 - acc: 0.7802 - val_loss: 0.6199 - val_acc: 0.8247\n",
            "Epoch 777/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7632 - acc: 0.7804 - val_loss: 0.6309 - val_acc: 0.8185\n",
            "Epoch 778/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7945 - acc: 0.7728 - val_loss: 0.7166 - val_acc: 0.7883\n",
            "Epoch 779/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7914 - acc: 0.7748 - val_loss: 0.7046 - val_acc: 0.7946\n",
            "Epoch 780/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8085 - acc: 0.7674 - val_loss: 0.8368 - val_acc: 0.7506\n",
            "Epoch 781/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7745 - acc: 0.7745 - val_loss: 0.6278 - val_acc: 0.8229\n",
            "Epoch 782/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7989 - acc: 0.7635 - val_loss: 0.6364 - val_acc: 0.8230\n",
            "Epoch 783/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7966 - acc: 0.7728 - val_loss: 0.6882 - val_acc: 0.8076\n",
            "Epoch 784/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8144 - acc: 0.7674 - val_loss: 0.7517 - val_acc: 0.7793\n",
            "Epoch 785/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7739 - acc: 0.7765 - val_loss: 0.6225 - val_acc: 0.8278\n",
            "Epoch 786/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7693 - acc: 0.7793 - val_loss: 0.6591 - val_acc: 0.8070\n",
            "Epoch 787/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8087 - acc: 0.7657 - val_loss: 0.6350 - val_acc: 0.8229\n",
            "Epoch 788/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7907 - acc: 0.7715 - val_loss: 0.6165 - val_acc: 0.8221\n",
            "Epoch 789/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7875 - acc: 0.7750 - val_loss: 0.6564 - val_acc: 0.8147\n",
            "Epoch 790/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8081 - acc: 0.7650 - val_loss: 0.7648 - val_acc: 0.7716\n",
            "Epoch 791/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8614 - acc: 0.7479 - val_loss: 0.6644 - val_acc: 0.8094\n",
            "Epoch 792/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8509 - acc: 0.7533 - val_loss: 0.6535 - val_acc: 0.8116\n",
            "Epoch 793/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8184 - acc: 0.7652 - val_loss: 0.6566 - val_acc: 0.8101\n",
            "Epoch 794/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7801 - acc: 0.7745 - val_loss: 0.6765 - val_acc: 0.8103\n",
            "Epoch 795/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7726 - acc: 0.7820 - val_loss: 0.6325 - val_acc: 0.8287\n",
            "Epoch 796/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7706 - acc: 0.7817 - val_loss: 0.6685 - val_acc: 0.8095\n",
            "Epoch 797/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7756 - acc: 0.7761 - val_loss: 0.8034 - val_acc: 0.7634\n",
            "Epoch 798/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8175 - acc: 0.7647 - val_loss: 0.6991 - val_acc: 0.7996\n",
            "Epoch 799/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7832 - acc: 0.7737 - val_loss: 0.6558 - val_acc: 0.8220\n",
            "Epoch 800/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8496 - acc: 0.7518 - val_loss: 0.6748 - val_acc: 0.8077\n",
            "Epoch 801/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8349 - acc: 0.7546 - val_loss: 0.7799 - val_acc: 0.7643\n",
            "Epoch 802/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8315 - acc: 0.7602 - val_loss: 0.6369 - val_acc: 0.8165\n",
            "Epoch 803/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7888 - acc: 0.7746 - val_loss: 0.6436 - val_acc: 0.8198\n",
            "Epoch 804/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7703 - acc: 0.7786 - val_loss: 0.6767 - val_acc: 0.8123\n",
            "Epoch 805/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7742 - acc: 0.7762 - val_loss: 0.6554 - val_acc: 0.8129\n",
            "Epoch 806/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7798 - acc: 0.7775 - val_loss: 0.6483 - val_acc: 0.8221\n",
            "Epoch 807/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7766 - acc: 0.7748 - val_loss: 0.6227 - val_acc: 0.8284\n",
            "Epoch 808/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7970 - acc: 0.7706 - val_loss: 0.7239 - val_acc: 0.7837\n",
            "Epoch 809/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8116 - acc: 0.7631 - val_loss: 0.7337 - val_acc: 0.7955\n",
            "Epoch 810/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7763 - acc: 0.7760 - val_loss: 0.6503 - val_acc: 0.8161\n",
            "Epoch 811/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7579 - acc: 0.7813 - val_loss: 0.6448 - val_acc: 0.8271\n",
            "Epoch 812/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8200 - acc: 0.7657 - val_loss: 0.6291 - val_acc: 0.8173\n",
            "Epoch 813/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7885 - acc: 0.7730 - val_loss: 0.6433 - val_acc: 0.8141\n",
            "Epoch 814/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7600 - acc: 0.7803 - val_loss: 0.6374 - val_acc: 0.8216\n",
            "Epoch 815/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7797 - acc: 0.7745 - val_loss: 0.6540 - val_acc: 0.8158\n",
            "Epoch 816/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8043 - acc: 0.7695 - val_loss: 0.6302 - val_acc: 0.8233\n",
            "Epoch 817/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7959 - acc: 0.7724 - val_loss: 0.6598 - val_acc: 0.8164\n",
            "Epoch 818/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7773 - acc: 0.7763 - val_loss: 0.6228 - val_acc: 0.8258\n",
            "Epoch 819/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8115 - acc: 0.7605 - val_loss: 0.6982 - val_acc: 0.8043\n",
            "Epoch 820/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8102 - acc: 0.7681 - val_loss: 0.6275 - val_acc: 0.8241\n",
            "Epoch 821/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8100 - acc: 0.7642 - val_loss: 0.7274 - val_acc: 0.7927\n",
            "Epoch 822/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7844 - acc: 0.7733 - val_loss: 0.6373 - val_acc: 0.8274\n",
            "Epoch 823/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7835 - acc: 0.7747 - val_loss: 0.7686 - val_acc: 0.7774\n",
            "Epoch 824/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8297 - acc: 0.7607 - val_loss: 0.6650 - val_acc: 0.8059\n",
            "Epoch 825/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7649 - acc: 0.7791 - val_loss: 0.6310 - val_acc: 0.8303\n",
            "Epoch 826/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7667 - acc: 0.7824 - val_loss: 0.8745 - val_acc: 0.7610\n",
            "Epoch 827/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7854 - acc: 0.7762 - val_loss: 0.7161 - val_acc: 0.7928\n",
            "Epoch 828/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8074 - acc: 0.7617 - val_loss: 0.6730 - val_acc: 0.8038\n",
            "Epoch 829/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8167 - acc: 0.7642 - val_loss: 0.8609 - val_acc: 0.7337\n",
            "Epoch 830/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8512 - acc: 0.7560 - val_loss: 0.6833 - val_acc: 0.8008\n",
            "Epoch 831/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7959 - acc: 0.7700 - val_loss: 0.6192 - val_acc: 0.8327\n",
            "Epoch 832/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7946 - acc: 0.7720 - val_loss: 0.8594 - val_acc: 0.7370\n",
            "Epoch 833/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8313 - acc: 0.7577 - val_loss: 0.6554 - val_acc: 0.8124\n",
            "Epoch 834/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7678 - acc: 0.7790 - val_loss: 0.6585 - val_acc: 0.8149\n",
            "Epoch 835/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7741 - acc: 0.7797 - val_loss: 0.7596 - val_acc: 0.7721\n",
            "Epoch 836/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8043 - acc: 0.7724 - val_loss: 0.6473 - val_acc: 0.8179\n",
            "Epoch 837/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7632 - acc: 0.7830 - val_loss: 0.6585 - val_acc: 0.8234\n",
            "Epoch 838/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7965 - acc: 0.7679 - val_loss: 0.6235 - val_acc: 0.8246\n",
            "Epoch 839/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7690 - acc: 0.7804 - val_loss: 0.8261 - val_acc: 0.7764\n",
            "Epoch 840/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8221 - acc: 0.7609 - val_loss: 0.6639 - val_acc: 0.8074\n",
            "Epoch 841/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8339 - acc: 0.7565 - val_loss: 0.6610 - val_acc: 0.8157\n",
            "Epoch 842/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7843 - acc: 0.7712 - val_loss: 0.6082 - val_acc: 0.8277\n",
            "Epoch 843/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7481 - acc: 0.7858 - val_loss: 0.6972 - val_acc: 0.7984\n",
            "Epoch 844/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8607 - acc: 0.7500 - val_loss: 0.7114 - val_acc: 0.7910\n",
            "Epoch 845/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8432 - acc: 0.7571 - val_loss: 0.6331 - val_acc: 0.8279\n",
            "Epoch 846/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7742 - acc: 0.7791 - val_loss: 0.6185 - val_acc: 0.8277\n",
            "Epoch 847/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7790 - acc: 0.7774 - val_loss: 0.6280 - val_acc: 0.8223\n",
            "Epoch 848/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7775 - acc: 0.7809 - val_loss: 0.6876 - val_acc: 0.8032\n",
            "Epoch 849/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7870 - acc: 0.7760 - val_loss: 0.6894 - val_acc: 0.8036\n",
            "Epoch 850/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7675 - acc: 0.7776 - val_loss: 0.6853 - val_acc: 0.8094\n",
            "Epoch 851/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7698 - acc: 0.7797 - val_loss: 0.6475 - val_acc: 0.8312\n",
            "Epoch 852/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7756 - acc: 0.7817 - val_loss: 0.8663 - val_acc: 0.7579\n",
            "Epoch 853/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7681 - acc: 0.7802 - val_loss: 0.5990 - val_acc: 0.8329\n",
            "Epoch 854/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7586 - acc: 0.7817 - val_loss: 0.6284 - val_acc: 0.8295\n",
            "Epoch 855/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8001 - acc: 0.7711 - val_loss: 0.7559 - val_acc: 0.7806\n",
            "Epoch 856/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8065 - acc: 0.7667 - val_loss: 0.7127 - val_acc: 0.8004\n",
            "Epoch 857/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7942 - acc: 0.7701 - val_loss: 0.6608 - val_acc: 0.8062\n",
            "Epoch 858/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7910 - acc: 0.7719 - val_loss: 0.7940 - val_acc: 0.7651\n",
            "Epoch 859/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8046 - acc: 0.7669 - val_loss: 0.6126 - val_acc: 0.8225\n",
            "Epoch 860/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7662 - acc: 0.7809 - val_loss: 0.6922 - val_acc: 0.8063\n",
            "Epoch 861/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7859 - acc: 0.7741 - val_loss: 0.6291 - val_acc: 0.8249\n",
            "Epoch 862/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7845 - acc: 0.7833 - val_loss: 0.7278 - val_acc: 0.7927\n",
            "Epoch 863/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7912 - acc: 0.7732 - val_loss: 0.6829 - val_acc: 0.8109\n",
            "Epoch 864/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7635 - acc: 0.7825 - val_loss: 0.6729 - val_acc: 0.8110\n",
            "Epoch 865/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7579 - acc: 0.7834 - val_loss: 0.6366 - val_acc: 0.8367\n",
            "Epoch 866/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7826 - acc: 0.7753 - val_loss: 0.6651 - val_acc: 0.8037\n",
            "Epoch 867/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8124 - acc: 0.7628 - val_loss: 0.6703 - val_acc: 0.8112\n",
            "Epoch 868/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7710 - acc: 0.7797 - val_loss: 0.7224 - val_acc: 0.7973\n",
            "Epoch 869/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7916 - acc: 0.7728 - val_loss: 0.6823 - val_acc: 0.8014\n",
            "Epoch 870/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7960 - acc: 0.7679 - val_loss: 0.6627 - val_acc: 0.8186\n",
            "Epoch 871/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8573 - acc: 0.7475 - val_loss: 0.6738 - val_acc: 0.8111\n",
            "Epoch 872/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7945 - acc: 0.7694 - val_loss: 0.6961 - val_acc: 0.7916\n",
            "Epoch 873/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7661 - acc: 0.7835 - val_loss: 0.6449 - val_acc: 0.8262\n",
            "Epoch 874/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7475 - acc: 0.7876 - val_loss: 0.6624 - val_acc: 0.8171\n",
            "Epoch 875/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7609 - acc: 0.7834 - val_loss: 0.6845 - val_acc: 0.8049\n",
            "Epoch 876/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7870 - acc: 0.7755 - val_loss: 0.6466 - val_acc: 0.8157\n",
            "Epoch 877/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7683 - acc: 0.7781 - val_loss: 0.6187 - val_acc: 0.8333\n",
            "Epoch 878/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7635 - acc: 0.7830 - val_loss: 0.6513 - val_acc: 0.8177\n",
            "Epoch 879/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7622 - acc: 0.7814 - val_loss: 1.1586 - val_acc: 0.6689\n",
            "Epoch 880/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8648 - acc: 0.7475 - val_loss: 0.8032 - val_acc: 0.7657\n",
            "Epoch 881/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8333 - acc: 0.7575 - val_loss: 0.6717 - val_acc: 0.8073\n",
            "Epoch 882/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7884 - acc: 0.7763 - val_loss: 0.6514 - val_acc: 0.8237\n",
            "Epoch 883/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7865 - acc: 0.7734 - val_loss: 0.7975 - val_acc: 0.7738\n",
            "Epoch 884/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7768 - acc: 0.7751 - val_loss: 0.6391 - val_acc: 0.8259\n",
            "Epoch 885/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8425 - acc: 0.7542 - val_loss: 0.6383 - val_acc: 0.8215\n",
            "Epoch 886/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8154 - acc: 0.7654 - val_loss: 0.7679 - val_acc: 0.7709\n",
            "Epoch 887/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7652 - acc: 0.7796 - val_loss: 0.6578 - val_acc: 0.8306\n",
            "Epoch 888/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8091 - acc: 0.7695 - val_loss: 0.6422 - val_acc: 0.8160\n",
            "Epoch 889/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8507 - acc: 0.7525 - val_loss: 0.7818 - val_acc: 0.7695\n",
            "Epoch 890/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8704 - acc: 0.7486 - val_loss: 0.6950 - val_acc: 0.7908\n",
            "Epoch 891/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8106 - acc: 0.7648 - val_loss: 0.6672 - val_acc: 0.8123\n",
            "Epoch 892/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7772 - acc: 0.7801 - val_loss: 0.8194 - val_acc: 0.7522\n",
            "Epoch 893/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8523 - acc: 0.7548 - val_loss: 0.6703 - val_acc: 0.8106\n",
            "Epoch 894/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8230 - acc: 0.7666 - val_loss: 0.6379 - val_acc: 0.8174\n",
            "Epoch 895/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7758 - acc: 0.7766 - val_loss: 0.6548 - val_acc: 0.8245\n",
            "Epoch 896/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7733 - acc: 0.7848 - val_loss: 0.6873 - val_acc: 0.8031\n",
            "Epoch 897/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7834 - acc: 0.7798 - val_loss: 0.7435 - val_acc: 0.7872\n",
            "Epoch 898/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7585 - acc: 0.7850 - val_loss: 0.6630 - val_acc: 0.8240\n",
            "Epoch 899/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7767 - acc: 0.7768 - val_loss: 0.6174 - val_acc: 0.8259\n",
            "Epoch 900/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7467 - acc: 0.7870 - val_loss: 0.6255 - val_acc: 0.8275\n",
            "Epoch 901/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7509 - acc: 0.7873 - val_loss: 0.6903 - val_acc: 0.8006\n",
            "Epoch 902/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7648 - acc: 0.7806 - val_loss: 0.6170 - val_acc: 0.8314\n",
            "Epoch 903/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7929 - acc: 0.7686 - val_loss: 0.6300 - val_acc: 0.8150\n",
            "Epoch 904/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7757 - acc: 0.7797 - val_loss: 0.6542 - val_acc: 0.8126\n",
            "Epoch 905/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7491 - acc: 0.7838 - val_loss: 0.7823 - val_acc: 0.7954\n",
            "Epoch 906/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7679 - acc: 0.7817 - val_loss: 0.7793 - val_acc: 0.7748\n",
            "Epoch 907/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7818 - acc: 0.7788 - val_loss: 0.7545 - val_acc: 0.7832\n",
            "Epoch 908/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7704 - acc: 0.7759 - val_loss: 0.6224 - val_acc: 0.8298\n",
            "Epoch 909/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7996 - acc: 0.7732 - val_loss: 0.6291 - val_acc: 0.8265\n",
            "Epoch 910/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8204 - acc: 0.7642 - val_loss: 0.6270 - val_acc: 0.8261\n",
            "Epoch 911/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8172 - acc: 0.7682 - val_loss: 0.6354 - val_acc: 0.8203\n",
            "Epoch 912/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7846 - acc: 0.7758 - val_loss: 0.6652 - val_acc: 0.8106\n",
            "Epoch 913/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7693 - acc: 0.7782 - val_loss: 0.6995 - val_acc: 0.7989\n",
            "Epoch 914/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8023 - acc: 0.7644 - val_loss: 0.6324 - val_acc: 0.8241\n",
            "Epoch 915/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7787 - acc: 0.7743 - val_loss: 0.6901 - val_acc: 0.8000\n",
            "Epoch 916/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7498 - acc: 0.7835 - val_loss: 0.6406 - val_acc: 0.8136\n",
            "Epoch 917/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8169 - acc: 0.7646 - val_loss: 0.6358 - val_acc: 0.8159\n",
            "Epoch 918/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7852 - acc: 0.7758 - val_loss: 0.6038 - val_acc: 0.8282\n",
            "Epoch 919/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8330 - acc: 0.7566 - val_loss: 0.6793 - val_acc: 0.8037\n",
            "Epoch 920/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8039 - acc: 0.7674 - val_loss: 0.9194 - val_acc: 0.7169\n",
            "Epoch 921/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8387 - acc: 0.7543 - val_loss: 0.6416 - val_acc: 0.8148\n",
            "Epoch 922/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8264 - acc: 0.7618 - val_loss: 0.6779 - val_acc: 0.8095\n",
            "Epoch 923/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7744 - acc: 0.7761 - val_loss: 0.7022 - val_acc: 0.8028\n",
            "Epoch 924/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7627 - acc: 0.7823 - val_loss: 0.6237 - val_acc: 0.8275\n",
            "Epoch 925/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7605 - acc: 0.7826 - val_loss: 0.6394 - val_acc: 0.8099\n",
            "Epoch 926/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8133 - acc: 0.7646 - val_loss: 0.6620 - val_acc: 0.8034\n",
            "Epoch 927/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7971 - acc: 0.7689 - val_loss: 0.7284 - val_acc: 0.7861\n",
            "Epoch 928/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7794 - acc: 0.7710 - val_loss: 0.7254 - val_acc: 0.7861\n",
            "Epoch 929/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8145 - acc: 0.7696 - val_loss: 0.6597 - val_acc: 0.8216\n",
            "Epoch 930/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7852 - acc: 0.7804 - val_loss: 0.6610 - val_acc: 0.8257\n",
            "Epoch 931/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8477 - acc: 0.7581 - val_loss: 0.6227 - val_acc: 0.8247\n",
            "Epoch 932/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7877 - acc: 0.7741 - val_loss: 0.6427 - val_acc: 0.8137\n",
            "Epoch 933/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7780 - acc: 0.7770 - val_loss: 0.6340 - val_acc: 0.8206\n",
            "Epoch 934/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7634 - acc: 0.7800 - val_loss: 0.7481 - val_acc: 0.7923\n",
            "Epoch 935/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7741 - acc: 0.7790 - val_loss: 0.7756 - val_acc: 0.7847\n",
            "Epoch 936/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7528 - acc: 0.7873 - val_loss: 0.6261 - val_acc: 0.8241\n",
            "Epoch 937/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8003 - acc: 0.7689 - val_loss: 0.6998 - val_acc: 0.7954\n",
            "Epoch 938/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8326 - acc: 0.7588 - val_loss: 0.6736 - val_acc: 0.7981\n",
            "Epoch 939/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7666 - acc: 0.7787 - val_loss: 0.6354 - val_acc: 0.8281\n",
            "Epoch 940/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7467 - acc: 0.7867 - val_loss: 0.6691 - val_acc: 0.8130\n",
            "Epoch 941/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7474 - acc: 0.7869 - val_loss: 0.6655 - val_acc: 0.8124\n",
            "Epoch 942/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7925 - acc: 0.7747 - val_loss: 0.6185 - val_acc: 0.8243\n",
            "Epoch 943/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7620 - acc: 0.7800 - val_loss: 0.6736 - val_acc: 0.8006\n",
            "Epoch 944/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7718 - acc: 0.7777 - val_loss: 0.6781 - val_acc: 0.8048\n",
            "Epoch 945/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7522 - acc: 0.7886 - val_loss: 0.8509 - val_acc: 0.7532\n",
            "Epoch 946/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7897 - acc: 0.7699 - val_loss: 0.8979 - val_acc: 0.7307\n",
            "Epoch 947/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8078 - acc: 0.7654 - val_loss: 0.8683 - val_acc: 0.7443\n",
            "Epoch 948/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8062 - acc: 0.7712 - val_loss: 0.6417 - val_acc: 0.8174\n",
            "Epoch 949/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7796 - acc: 0.7780 - val_loss: 0.6555 - val_acc: 0.8153\n",
            "Epoch 950/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7666 - acc: 0.7813 - val_loss: 0.6629 - val_acc: 0.8113\n",
            "Epoch 951/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7494 - acc: 0.7851 - val_loss: 0.6132 - val_acc: 0.8308\n",
            "Epoch 952/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8570 - acc: 0.7508 - val_loss: 0.6937 - val_acc: 0.7938\n",
            "Epoch 953/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7977 - acc: 0.7689 - val_loss: 0.6440 - val_acc: 0.8172\n",
            "Epoch 954/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7879 - acc: 0.7778 - val_loss: 0.6602 - val_acc: 0.8187\n",
            "Epoch 955/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8004 - acc: 0.7688 - val_loss: 0.8350 - val_acc: 0.7445\n",
            "Epoch 956/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7769 - acc: 0.7756 - val_loss: 0.6018 - val_acc: 0.8297\n",
            "Epoch 957/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7623 - acc: 0.7812 - val_loss: 0.6993 - val_acc: 0.8006\n",
            "Epoch 958/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7673 - acc: 0.7764 - val_loss: 0.6236 - val_acc: 0.8263\n",
            "Epoch 959/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7781 - acc: 0.7781 - val_loss: 0.6623 - val_acc: 0.8058\n",
            "Epoch 960/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7650 - acc: 0.7783 - val_loss: 0.6890 - val_acc: 0.8106\n",
            "Epoch 961/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7787 - acc: 0.7740 - val_loss: 0.8005 - val_acc: 0.7777\n",
            "Epoch 962/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7857 - acc: 0.7734 - val_loss: 0.6528 - val_acc: 0.8152\n",
            "Epoch 963/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7468 - acc: 0.7887 - val_loss: 0.6495 - val_acc: 0.8276\n",
            "Epoch 964/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7780 - acc: 0.7765 - val_loss: 0.7644 - val_acc: 0.7693\n",
            "Epoch 965/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7906 - acc: 0.7736 - val_loss: 0.6433 - val_acc: 0.8193\n",
            "Epoch 966/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7755 - acc: 0.7730 - val_loss: 0.6514 - val_acc: 0.8143\n",
            "Epoch 967/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7536 - acc: 0.7853 - val_loss: 0.6386 - val_acc: 0.8227\n",
            "Epoch 968/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7677 - acc: 0.7762 - val_loss: 0.6137 - val_acc: 0.8261\n",
            "Epoch 969/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7551 - acc: 0.7842 - val_loss: 0.7249 - val_acc: 0.8043\n",
            "Epoch 970/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7662 - acc: 0.7795 - val_loss: 0.6427 - val_acc: 0.8161\n",
            "Epoch 971/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7527 - acc: 0.7844 - val_loss: 0.6219 - val_acc: 0.8322\n",
            "Epoch 972/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7559 - acc: 0.7847 - val_loss: 0.7829 - val_acc: 0.7633\n",
            "Epoch 973/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7705 - acc: 0.7794 - val_loss: 0.6740 - val_acc: 0.8154\n",
            "Epoch 974/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7807 - acc: 0.7720 - val_loss: 0.6142 - val_acc: 0.8313\n",
            "Epoch 975/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7769 - acc: 0.7756 - val_loss: 0.8383 - val_acc: 0.7578\n",
            "Epoch 976/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7819 - acc: 0.7754 - val_loss: 0.6134 - val_acc: 0.8272\n",
            "Epoch 977/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7530 - acc: 0.7826 - val_loss: 0.5965 - val_acc: 0.8302\n",
            "Epoch 978/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7624 - acc: 0.7802 - val_loss: 0.6686 - val_acc: 0.8044\n",
            "Epoch 979/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8044 - acc: 0.7690 - val_loss: 0.6303 - val_acc: 0.8312\n",
            "Epoch 980/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7948 - acc: 0.7745 - val_loss: 0.6623 - val_acc: 0.8128\n",
            "Epoch 981/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8055 - acc: 0.7676 - val_loss: 0.6199 - val_acc: 0.8237\n",
            "Epoch 982/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7814 - acc: 0.7746 - val_loss: 0.6028 - val_acc: 0.8281\n",
            "Epoch 983/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7505 - acc: 0.7847 - val_loss: 0.6636 - val_acc: 0.8137\n",
            "Epoch 984/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7894 - acc: 0.7733 - val_loss: 0.7982 - val_acc: 0.7765\n",
            "Epoch 985/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7663 - acc: 0.7799 - val_loss: 0.6020 - val_acc: 0.8334\n",
            "Epoch 986/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7751 - acc: 0.7774 - val_loss: 0.7360 - val_acc: 0.7909\n",
            "Epoch 987/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7629 - acc: 0.7818 - val_loss: 0.6052 - val_acc: 0.8360\n",
            "Epoch 988/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7569 - acc: 0.7854 - val_loss: 0.5862 - val_acc: 0.8319\n",
            "Epoch 989/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7298 - acc: 0.7875 - val_loss: 0.8049 - val_acc: 0.7789\n",
            "Epoch 990/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7630 - acc: 0.7837 - val_loss: 0.6212 - val_acc: 0.8318\n",
            "Epoch 991/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7459 - acc: 0.7882 - val_loss: 0.6080 - val_acc: 0.8349\n",
            "Epoch 992/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7846 - acc: 0.7773 - val_loss: 0.6505 - val_acc: 0.8123\n",
            "Epoch 993/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7660 - acc: 0.7828 - val_loss: 0.6240 - val_acc: 0.8289\n",
            "Epoch 994/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7711 - acc: 0.7775 - val_loss: 0.8619 - val_acc: 0.7448\n",
            "Epoch 995/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7960 - acc: 0.7693 - val_loss: 0.6434 - val_acc: 0.8122\n",
            "Epoch 996/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7515 - acc: 0.7814 - val_loss: 0.6199 - val_acc: 0.8323\n",
            "Epoch 997/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.7492 - acc: 0.7888 - val_loss: 0.6979 - val_acc: 0.8184\n",
            "Epoch 998/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8499 - acc: 0.7545 - val_loss: 0.6886 - val_acc: 0.7928\n",
            "Epoch 999/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8229 - acc: 0.7600 - val_loss: 0.6483 - val_acc: 0.8074\n",
            "Epoch 1000/1000\n",
            "42000/42000 [==============================] - 1s 14us/step - loss: 0.8160 - acc: 0.7650 - val_loss: 0.6975 - val_acc: 0.8009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f35a47e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxo2EgG9ALFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e0634df-c04f-4b92-a64f-e148b79df2a2"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "model_final2 = Sequential()\n",
        "model_final2.add(Dense(units = 1024, input_shape = (1024,), activation = 'relu', kernel_initializer=initializers.he_normal(seed=10), bias_initializer=initializers.Zeros()))\n",
        "model_final2.add(BatchNormalization())\n",
        "model_final2.add(Dropout(0.1))\n",
        "model_final2.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final2.add(Dropout(0.1))\n",
        "model_final2.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final2.add(Dropout(0.1))\n",
        "model_final2.add(Dense(units = 10, activation = 'softmax'))\n",
        "model_final2.compile(optimizer='sgd', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model_final2.summary()\n",
        "model_final2.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 100, epochs = 25, verbose = 1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_201 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_144 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_202 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_145 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_203 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_146 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_204 (Dense)            (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,163,146\n",
            "Trainable params: 3,161,098\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/25\n",
            "42000/42000 [==============================] - 9s 203us/step - loss: 1.9198 - acc: 0.3527 - val_loss: 1.4701 - val_acc: 0.5574\n",
            "Epoch 2/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 1.2732 - acc: 0.6032 - val_loss: 1.0934 - val_acc: 0.6782\n",
            "Epoch 3/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 1.0420 - acc: 0.6771 - val_loss: 0.9565 - val_acc: 0.7158\n",
            "Epoch 4/25\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 0.9253 - acc: 0.7137 - val_loss: 0.8880 - val_acc: 0.7324\n",
            "Epoch 5/25\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 0.8445 - acc: 0.7390 - val_loss: 0.8255 - val_acc: 0.7578\n",
            "Epoch 6/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.7832 - acc: 0.7579 - val_loss: 0.8056 - val_acc: 0.7583\n",
            "Epoch 7/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.7369 - acc: 0.7710 - val_loss: 0.7752 - val_acc: 0.7720\n",
            "Epoch 8/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.6976 - acc: 0.7859 - val_loss: 0.7473 - val_acc: 0.7762\n",
            "Epoch 9/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.6666 - acc: 0.7957 - val_loss: 0.7705 - val_acc: 0.7714\n",
            "Epoch 10/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.6374 - acc: 0.8043 - val_loss: 0.7067 - val_acc: 0.7944\n",
            "Epoch 11/25\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 0.6218 - acc: 0.8093 - val_loss: 0.7174 - val_acc: 0.7924\n",
            "Epoch 12/25\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 0.6047 - acc: 0.8170 - val_loss: 0.6857 - val_acc: 0.8021\n",
            "Epoch 13/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.5767 - acc: 0.8224 - val_loss: 0.7149 - val_acc: 0.7923\n",
            "Epoch 14/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.5629 - acc: 0.8286 - val_loss: 0.6846 - val_acc: 0.8052\n",
            "Epoch 15/25\n",
            "42000/42000 [==============================] - 3s 69us/step - loss: 0.5485 - acc: 0.8307 - val_loss: 0.7057 - val_acc: 0.7939\n",
            "Epoch 16/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.5339 - acc: 0.8366 - val_loss: 0.6483 - val_acc: 0.8159\n",
            "Epoch 17/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.5260 - acc: 0.8387 - val_loss: 0.6543 - val_acc: 0.8135\n",
            "Epoch 18/25\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 0.5081 - acc: 0.8450 - val_loss: 0.6656 - val_acc: 0.8108\n",
            "Epoch 19/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.5019 - acc: 0.8477 - val_loss: 0.6782 - val_acc: 0.8046\n",
            "Epoch 20/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.4874 - acc: 0.8508 - val_loss: 0.6287 - val_acc: 0.8250\n",
            "Epoch 21/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.4813 - acc: 0.8526 - val_loss: 0.6318 - val_acc: 0.8251\n",
            "Epoch 22/25\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 0.4635 - acc: 0.8595 - val_loss: 0.7249 - val_acc: 0.7916\n",
            "Epoch 23/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.4592 - acc: 0.8601 - val_loss: 0.6299 - val_acc: 0.8226\n",
            "Epoch 24/25\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 0.4499 - acc: 0.8618 - val_loss: 0.6038 - val_acc: 0.8349\n",
            "Epoch 25/25\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 0.4449 - acc: 0.8635 - val_loss: 0.6238 - val_acc: 0.8272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f32d83828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgqg4eF7s_oP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "663f52fc-e410-4394-a678-b45d3719529e"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "model_final3 = Sequential()\n",
        "model_final3.add(Dense(units = 1024, input_shape = (1024,), activation = 'relu', kernel_initializer=initializers.he_normal(seed=10), bias_initializer=initializers.Zeros()))\n",
        "model_final3.add(BatchNormalization())\n",
        "model_final3.add(Dropout(0.1))\n",
        "model_final3.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final3.add(Dropout(0.1))\n",
        "model_final3.add(Dense(units = 1024, activation = 'relu'))\n",
        "model_final3.add(Dropout(0.1))\n",
        "model_final3.add(Dense(units = 10, activation = 'softmax'))\n",
        "model_final3.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model_final3.summary()\n",
        "model_final3.fit(X_train_flat, trainY, validation_data=(X_test_flat,testY), batch_size = 100, epochs = 25, verbose = 1)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_209 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_150 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_210 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_151 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_211 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_152 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_212 (Dense)            (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,163,146\n",
            "Trainable params: 3,161,098\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/25\n",
            "42000/42000 [==============================] - 9s 226us/step - loss: 1.3319 - acc: 0.5668 - val_loss: 2.3393 - val_acc: 0.3212\n",
            "Epoch 2/25\n",
            "42000/42000 [==============================] - 3s 81us/step - loss: 1.0037 - acc: 0.6818 - val_loss: 1.5976 - val_acc: 0.4692\n",
            "Epoch 3/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.8977 - acc: 0.7140 - val_loss: 1.3406 - val_acc: 0.5513\n",
            "Epoch 4/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.8240 - acc: 0.7411 - val_loss: 0.8573 - val_acc: 0.7379\n",
            "Epoch 5/25\n",
            "42000/42000 [==============================] - 3s 81us/step - loss: 0.7731 - acc: 0.7555 - val_loss: 0.8506 - val_acc: 0.7326\n",
            "Epoch 6/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.7279 - acc: 0.7684 - val_loss: 0.9279 - val_acc: 0.7134\n",
            "Epoch 7/25\n",
            "42000/42000 [==============================] - 3s 81us/step - loss: 0.6974 - acc: 0.7789 - val_loss: 0.9038 - val_acc: 0.7163\n",
            "Epoch 8/25\n",
            "42000/42000 [==============================] - 3s 80us/step - loss: 0.6629 - acc: 0.7901 - val_loss: 0.8074 - val_acc: 0.7472\n",
            "Epoch 9/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.6377 - acc: 0.7972 - val_loss: 0.8870 - val_acc: 0.7284\n",
            "Epoch 10/25\n",
            "42000/42000 [==============================] - 3s 80us/step - loss: 0.6187 - acc: 0.8037 - val_loss: 0.7313 - val_acc: 0.7751\n",
            "Epoch 11/25\n",
            "42000/42000 [==============================] - 3s 80us/step - loss: 0.6171 - acc: 0.8038 - val_loss: 0.7637 - val_acc: 0.7626\n",
            "Epoch 12/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.5860 - acc: 0.8127 - val_loss: 0.8040 - val_acc: 0.7525\n",
            "Epoch 13/25\n",
            "42000/42000 [==============================] - 3s 80us/step - loss: 0.5732 - acc: 0.8154 - val_loss: 0.6689 - val_acc: 0.7932\n",
            "Epoch 14/25\n",
            "42000/42000 [==============================] - 3s 80us/step - loss: 0.5609 - acc: 0.8204 - val_loss: 0.9103 - val_acc: 0.7174\n",
            "Epoch 15/25\n",
            "42000/42000 [==============================] - 3s 81us/step - loss: 0.5364 - acc: 0.8265 - val_loss: 0.7207 - val_acc: 0.7713\n",
            "Epoch 16/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.5280 - acc: 0.8286 - val_loss: 0.9072 - val_acc: 0.7253\n",
            "Epoch 17/25\n",
            "42000/42000 [==============================] - 3s 82us/step - loss: 0.5124 - acc: 0.8353 - val_loss: 0.6232 - val_acc: 0.8118\n",
            "Epoch 18/25\n",
            "42000/42000 [==============================] - 3s 82us/step - loss: 0.4956 - acc: 0.8390 - val_loss: 0.6577 - val_acc: 0.8038\n",
            "Epoch 19/25\n",
            "42000/42000 [==============================] - 3s 83us/step - loss: 0.4998 - acc: 0.8370 - val_loss: 0.6110 - val_acc: 0.8176\n",
            "Epoch 20/25\n",
            "42000/42000 [==============================] - 3s 78us/step - loss: 0.4794 - acc: 0.8432 - val_loss: 0.6415 - val_acc: 0.8056\n",
            "Epoch 21/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.4697 - acc: 0.8483 - val_loss: 0.6661 - val_acc: 0.7982\n",
            "Epoch 22/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.4545 - acc: 0.8513 - val_loss: 0.6186 - val_acc: 0.8183\n",
            "Epoch 23/25\n",
            "42000/42000 [==============================] - 3s 81us/step - loss: 0.4401 - acc: 0.8541 - val_loss: 0.7692 - val_acc: 0.7719\n",
            "Epoch 24/25\n",
            "42000/42000 [==============================] - 3s 79us/step - loss: 0.4328 - acc: 0.8591 - val_loss: 0.5604 - val_acc: 0.8324\n",
            "Epoch 25/25\n",
            "42000/42000 [==============================] - 3s 78us/step - loss: 0.4244 - acc: 0.8609 - val_loss: 0.7073 - val_acc: 0.7899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f32e631d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-zMiM_bneBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "591becdb-3a88-4943-9634-4b334e420ddf"
      },
      "source": [
        "# LEt us try a CNN architecture with Keras \n",
        "#Ref :: https://stats.stackexchange.com/questions/272607/cifar-10-cant-get-above-60-accuracy-keras-with-tensorflow-backend\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dense, Flatten\n",
        "\n",
        "n_classes = 10\n",
        "\n",
        "X_train_conv = X_train.reshape(42000,32,32,1)\n",
        "X_test_conv = X_test.reshape(18000,32,32,1)\n",
        "model_new = Sequential()\n",
        "model_new.add(Conv2D(input_shape=(32,32,1), filters=32, \n",
        "                 use_bias=True, kernel_size=(3,3)))\n",
        "model_new.add(Activation('relu'))\n",
        "model_new.add(Dropout(0.1))\n",
        "model_new.add(Conv2D(filters=64, use_bias=False, kernel_size=(5,5), strides=2))\n",
        "model_new.add(Activation('relu'))\n",
        "model_new.add(Dropout(0.2))\n",
        "model_new.add(Flatten())\n",
        "model_new.add(Dense(128))\n",
        "model_new.add(Activation('relu'))\n",
        "model_new.add(Dropout(0.3))\n",
        "model_new.add(Dense(n_classes, activation=\"softmax\"))\n",
        "model_new.compile(loss='categorical_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
        "model_new.summary()\n",
        "model_new.fit(X_train_conv, trainY, validation_data=(X_test_conv,testY), batch_size = 100, epochs = 50, verbose = 1)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 30, 30, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_179 (Dropout)        (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 13, 13, 64)        51200     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_180 (Dropout)        (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_229 (Dense)            (None, 128)               1384576   \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_181 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_230 (Dense)            (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,437,386\n",
            "Trainable params: 1,437,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/50\n",
            "42000/42000 [==============================] - 11s 258us/step - loss: 1.4590 - acc: 0.6195 - val_loss: 0.5841 - val_acc: 0.8336\n",
            "Epoch 2/50\n",
            "42000/42000 [==============================] - 4s 101us/step - loss: 0.6355 - acc: 0.8080 - val_loss: 0.4661 - val_acc: 0.8662\n",
            "Epoch 3/50\n",
            "42000/42000 [==============================] - 4s 100us/step - loss: 0.5138 - acc: 0.8443 - val_loss: 0.4285 - val_acc: 0.8776\n",
            "Epoch 4/50\n",
            "42000/42000 [==============================] - 4s 99us/step - loss: 0.4457 - acc: 0.8628 - val_loss: 0.4161 - val_acc: 0.8846\n",
            "Epoch 5/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.4221 - acc: 0.8712 - val_loss: 0.3993 - val_acc: 0.8929\n",
            "Epoch 6/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.3705 - acc: 0.8859 - val_loss: 0.4054 - val_acc: 0.8903\n",
            "Epoch 7/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.3435 - acc: 0.8925 - val_loss: 0.4028 - val_acc: 0.8912\n",
            "Epoch 8/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.3177 - acc: 0.9015 - val_loss: 0.4150 - val_acc: 0.8925\n",
            "Epoch 9/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.2987 - acc: 0.9077 - val_loss: 0.4004 - val_acc: 0.8950\n",
            "Epoch 10/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.2804 - acc: 0.9099 - val_loss: 0.4140 - val_acc: 0.8931\n",
            "Epoch 11/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.2650 - acc: 0.9179 - val_loss: 0.4239 - val_acc: 0.8921\n",
            "Epoch 12/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.2464 - acc: 0.9225 - val_loss: 0.4129 - val_acc: 0.8965\n",
            "Epoch 13/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.2354 - acc: 0.9266 - val_loss: 0.4407 - val_acc: 0.8938\n",
            "Epoch 14/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.2175 - acc: 0.9310 - val_loss: 0.4446 - val_acc: 0.8997\n",
            "Epoch 15/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.2120 - acc: 0.9338 - val_loss: 0.4285 - val_acc: 0.8989\n",
            "Epoch 16/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.2040 - acc: 0.9368 - val_loss: 0.4475 - val_acc: 0.8977\n",
            "Epoch 17/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.1942 - acc: 0.9396 - val_loss: 0.4425 - val_acc: 0.8937\n",
            "Epoch 18/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1868 - acc: 0.9428 - val_loss: 0.4530 - val_acc: 0.8987\n",
            "Epoch 19/50\n",
            "42000/42000 [==============================] - 4s 100us/step - loss: 0.1900 - acc: 0.9404 - val_loss: 0.4664 - val_acc: 0.8919\n",
            "Epoch 20/50\n",
            "42000/42000 [==============================] - 4s 98us/step - loss: 0.1846 - acc: 0.9431 - val_loss: 0.4349 - val_acc: 0.8998\n",
            "Epoch 21/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1676 - acc: 0.9490 - val_loss: 0.4617 - val_acc: 0.8969\n",
            "Epoch 22/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1715 - acc: 0.9477 - val_loss: 0.5086 - val_acc: 0.8931\n",
            "Epoch 23/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1697 - acc: 0.9484 - val_loss: 0.4733 - val_acc: 0.9001\n",
            "Epoch 24/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.1643 - acc: 0.9499 - val_loss: 0.4956 - val_acc: 0.8947\n",
            "Epoch 25/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.1540 - acc: 0.9541 - val_loss: 0.4944 - val_acc: 0.8968\n",
            "Epoch 26/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1508 - acc: 0.9544 - val_loss: 0.4993 - val_acc: 0.8937\n",
            "Epoch 27/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1494 - acc: 0.9540 - val_loss: 0.5084 - val_acc: 0.8949\n",
            "Epoch 28/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1447 - acc: 0.9561 - val_loss: 0.5149 - val_acc: 0.8985\n",
            "Epoch 29/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1420 - acc: 0.9560 - val_loss: 0.5149 - val_acc: 0.8937\n",
            "Epoch 30/50\n",
            "42000/42000 [==============================] - 4s 94us/step - loss: 0.1428 - acc: 0.9580 - val_loss: 0.5118 - val_acc: 0.8986\n",
            "Epoch 31/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1431 - acc: 0.9572 - val_loss: 0.5284 - val_acc: 0.8993\n",
            "Epoch 32/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1356 - acc: 0.9593 - val_loss: 0.5052 - val_acc: 0.9001\n",
            "Epoch 33/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.1317 - acc: 0.9602 - val_loss: 0.5231 - val_acc: 0.8984\n",
            "Epoch 34/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1324 - acc: 0.9604 - val_loss: 0.5122 - val_acc: 0.8996\n",
            "Epoch 35/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1198 - acc: 0.9645 - val_loss: 0.5408 - val_acc: 0.8993\n",
            "Epoch 36/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1282 - acc: 0.9616 - val_loss: 0.5606 - val_acc: 0.8952\n",
            "Epoch 37/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1251 - acc: 0.9630 - val_loss: 0.5189 - val_acc: 0.9001\n",
            "Epoch 38/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.1326 - acc: 0.9614 - val_loss: 0.5035 - val_acc: 0.9001\n",
            "Epoch 39/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1240 - acc: 0.9631 - val_loss: 0.5165 - val_acc: 0.9003\n",
            "Epoch 40/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1251 - acc: 0.9646 - val_loss: 0.5503 - val_acc: 0.8973\n",
            "Epoch 41/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1188 - acc: 0.9654 - val_loss: 0.5173 - val_acc: 0.9009\n",
            "Epoch 42/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1225 - acc: 0.9644 - val_loss: 0.5297 - val_acc: 0.8984\n",
            "Epoch 43/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1290 - acc: 0.9622 - val_loss: 0.5318 - val_acc: 0.8996\n",
            "Epoch 44/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1183 - acc: 0.9657 - val_loss: 0.5604 - val_acc: 0.9017\n",
            "Epoch 45/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1119 - acc: 0.9684 - val_loss: 0.5480 - val_acc: 0.9017\n",
            "Epoch 46/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.1093 - acc: 0.9675 - val_loss: 0.5721 - val_acc: 0.9022\n",
            "Epoch 47/50\n",
            "42000/42000 [==============================] - 4s 97us/step - loss: 0.1199 - acc: 0.9655 - val_loss: 0.5640 - val_acc: 0.8998\n",
            "Epoch 48/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.1111 - acc: 0.9677 - val_loss: 0.5628 - val_acc: 0.8964\n",
            "Epoch 49/50\n",
            "42000/42000 [==============================] - 4s 95us/step - loss: 0.1062 - acc: 0.9695 - val_loss: 0.5827 - val_acc: 0.8969\n",
            "Epoch 50/50\n",
            "42000/42000 [==============================] - 4s 96us/step - loss: 0.0991 - acc: 0.9719 - val_loss: 0.5959 - val_acc: 0.8964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ee15b8c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xV_RWpVxpop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predy_nn = model_new.predict(X_test_conv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7gg0tw70ARY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55c5394a-ce24-4c3a-95e7-8edeea39405c"
      },
      "source": [
        "predy_nn.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6CXj8kS0FXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce997c6e-3118-43af-9b55-cf7fb946cb92"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMy3iQrM0XCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"KNN Metrics = \\n\", metrics.classification_report(y_test, predy_nn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zEsE_KeAehc",
        "colab_type": "text"
      },
      "source": [
        "# Understand the differences and trade-offs between traditional and NN classifiers with the help of classification metrics (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "577BzLXPZ1a9",
        "colab_type": "text"
      },
      "source": [
        "Traditional KNearestNeighbor method :: \n",
        "1. We have to prerun and determine the value of K\n",
        "2. After that we have limited number of parameters to tweak to improve accuracy\n",
        "3. Able to achieve maximum accuracy of 64 %\n",
        "\n",
        "Neural network\n",
        "1. We have higher level of granularity on tweaking parameters\n",
        "2. Initially i had a tough time getting accuracy beyond 40- 50 %\n",
        "3. But there are various functions and options to add intermidiate layers with non-linear activation functions to tweak\n",
        "4. With normal Neural Networks i was able to achieve 80% + accuracy\n",
        "5. I also tried with Convolutional NN to experiment and achieved 89 % accuracy"
      ]
    }
  ]
}